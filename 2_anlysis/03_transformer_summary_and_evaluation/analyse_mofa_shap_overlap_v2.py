# -*- coding: utf-8 -*-
"""analyse_mofa_shap_overlap_v2.py

Purpose
-------
Compute overlap between:
  (i) variance-driving features (MOFA+ feature-importance exports), and
  (ii) prediction-driving features (SHAP importance exports from analyse_transformer_shap_v2.py).

This script is a Python replacement for the earlier UI mockup
`analyse_mofa_shap_overlap.py` and is designed to *directly* consume the real
outputs generated by your MOFA+ and SHAP steps.

Expected inputs
---------------
SHAP (from analyse_transformer_shap_v2.py):
  <SHAP_DATA_DIR>/shap_importance_<Tissue>_<Task>.csv
    - Must contain at least: Feature, MeanAbsoluteShap

MOFA feature-importance exports (from select_mofa_features.py or similar):
  <MOFA_DIR>/mofa_overall_feature_importance_<view>.csv
    - view is expected to include tokens like: leaf/root and spectral/metabolite
    - Columns vary by implementation; this script will infer:
        * feature column (Feature/feature/feature_name/Name/...) and
        * score column (Importance/Score/Weight/AbsLoading/...).

Outputs
-------
Writes to <OUT_DIR>:
  - mofa_shap_overlap_summary.csv (one row per tissue×task×threshold)
  - mofa_shap_overlap_details.json (intersection feature lists + wavelength summaries)

Notes on methodology
--------------------
MOFA importance is commonly exported per view (e.g., leaf_spectral, leaf_metabolite).
To create a tissue-level ranking that is robust to scale differences across views,
we compute a within-view rank percentile (1.0 = most important) and average these
percentiles across available views.

Run
---
python analyse_mofa_shap_overlap_v2.py \
  --base_dir "C:/Users/ms/Desktop/hyper" \
  --k 50 100 \
  --pct 0.01 0.05 0.10

"""

from __future__ import annotations

import argparse
import glob
import json
import os
import re
from typing import Dict, Iterable, List, Optional, Tuple

import numpy as np
import pandas as pd


# ---------------------------
# Defaults (match your project)
# ---------------------------
DEFAULT_BASE_DIR = r"C:/Users/ms/Desktop/hyper"
DEFAULT_MOFA_DIR = os.path.join(DEFAULT_BASE_DIR, "output", "mofa")
DEFAULT_SHAP_DATA_DIR = os.path.join(
    DEFAULT_BASE_DIR, "output", "transformer", "shap_analysis_ggl", "importance_data"
)
DEFAULT_OUT_DIR = os.path.join(
    DEFAULT_BASE_DIR, "output", "transformer", "mofa_shap_overlap"
)

TISSUES = ["Leaf", "Root"]
TASKS = ["Genotype", "Treatment", "Day"]

SPECTRAL_TOKENS = ("spectral", "spec", "wavelength", "wl")
METAB_TOKENS = ("metabolite", "metab", "molecular", "mol", "lcms")


# ---------------------------
# Helpers
# ---------------------------

def _safe_read_csv(path: str) -> pd.DataFrame:
    """Read CSV with pragmatic fallbacks for messy exports."""
    try:
        return pd.read_csv(path)
    except UnicodeDecodeError:
        return pd.read_csv(path, encoding="utf-8", errors="replace")


def _infer_feature_col(df: pd.DataFrame) -> str:
    preferred = [
        "Feature", "feature", "feature_name", "FeatureName", "name", "Name",
        "variable", "Variable", "id", "ID"
    ]
    for c in preferred:
        if c in df.columns:
            return c

    # fallback: first non-numeric column
    for c in df.columns:
        if not pd.api.types.is_numeric_dtype(df[c]):
            return c

    # ultimate fallback: first column
    return df.columns[0]


def _infer_score_col(df: pd.DataFrame, feature_col: str) -> Optional[str]:
    """Infer an importance/weight column. Return None if none found."""
    # Strong preference by name
    name_priority = [
        "MeanAbsLoading", "mean_abs_loading",
        "AbsLoading", "abs_loading",
        "Importance", "importance",
        "Score", "score",
        "Weight", "weight",
        "Loading", "loading",
        "Value", "value",
    ]
    for c in name_priority:
        if c in df.columns and pd.api.types.is_numeric_dtype(df[c]):
            return c

    # Otherwise: pick the numeric column with the most non-null values
    numeric_cols = [c for c in df.columns if c != feature_col and pd.api.types.is_numeric_dtype(df[c])]
    if not numeric_cols:
        return None

    numeric_cols_sorted = sorted(numeric_cols, key=lambda c: df[c].notna().sum(), reverse=True)
    return numeric_cols_sorted[0]


def _view_from_filename(path: str) -> str:
    base = os.path.basename(path).lower()
    # strip extension
    base = re.sub(r"\.[^.]+$", "", base)
    return base


def _is_spectral_feature(feature: str) -> bool:
    # Common in your project: W_553 etc.
    return bool(re.match(r"^(w|wl)[_\- ]?\d+(\.\d+)?$", feature.strip(), flags=re.IGNORECASE))


def _parse_wavelength_nm(feature: str) -> Optional[float]:
    m = re.match(r"^(w|wl)[_\- ]?(\d+(?:\.\d+)?)$", feature.strip(), flags=re.IGNORECASE)
    if not m:
        return None
    try:
        return float(m.group(2))
    except ValueError:
        return None


def _jaccard(a: Iterable[str], b: Iterable[str]) -> float:
    sa, sb = set(a), set(b)
    if not sa and not sb:
        return 0.0
    return len(sa & sb) / len(sa | sb)


# ---------------------------
# Loading: MOFA importance
# ---------------------------

def find_mofa_importance_files(mofa_dir: str, tissue: str) -> List[str]:
    """Find MOFA importance CSVs for a tissue (Leaf/Root)."""
    tissue_l = tissue.lower()
    candidates = glob.glob(os.path.join(mofa_dir, "*.csv"))

    def is_match(p: str) -> bool:
        b = os.path.basename(p).lower()
        if "mofa" not in b:
            return False
        if "importance" not in b and "feature" not in b:
            return False
        if tissue_l not in b:
            return False
        return True

    return sorted([p for p in candidates if is_match(p)])


def load_mofa_view_importance(path: str) -> pd.DataFrame:
    """Load a MOFA importance export and standardise columns."""
    df = _safe_read_csv(path)
    if df.empty:
        raise ValueError(f"MOFA importance file is empty: {path}")

    feature_col = _infer_feature_col(df)
    score_col = _infer_score_col(df, feature_col)

    out = pd.DataFrame({
        "Feature": df[feature_col].astype(str),
    })

    if score_col is None:
        # If there is no explicit numeric score, preserve row order as ranking
        out["Score"] = np.linspace(1.0, 0.0, num=len(out), endpoint=False)
    else:
        out["Score"] = pd.to_numeric(df[score_col], errors="coerce")
        # If scores are all NaN, fall back to row-order ranking
        if out["Score"].isna().all():
            out["Score"] = np.linspace(1.0, 0.0, num=len(out), endpoint=False)

    out["View"] = _view_from_filename(path)

    # rank-percentile within view (1 = best)
    out = out.dropna(subset=["Feature"]).copy()
    out["Rank"] = out["Score"].rank(method="dense", ascending=False)
    n = float(len(out))
    if n <= 1:
        out["RankPct"] = 1.0
    else:
        out["RankPct"] = 1.0 - (out["Rank"] - 1.0) / (n - 1.0)

    return out[["Feature", "Score", "Rank", "RankPct", "View"]]


def build_tissue_mofa_ranking(mofa_dir: str, tissue: str) -> pd.DataFrame:
    """Build tissue-level MOFA ranking by combining all tissue-matching view exports."""
    files = find_mofa_importance_files(mofa_dir, tissue)
    if not files:
        raise FileNotFoundError(
            f"No MOFA importance CSVs found for tissue '{tissue}' under: {mofa_dir}\n"
            "Expected something like: mofa_overall_feature_importance_leaf_spectral.csv"
        )

    view_dfs = []
    for p in files:
        try:
            view_dfs.append(load_mofa_view_importance(p))
        except Exception as e:
            raise RuntimeError(f"Failed to parse MOFA importance file {p}: {e}") from e

    all_views = pd.concat(view_dfs, ignore_index=True)

    # Average rank-percentile across views (missing views contribute 0)
    # This avoids comparing raw score scales across views.
    combined = (
        all_views
        .groupby("Feature", as_index=False)
        .agg(
            CombinedRankPct=("RankPct", "mean"),
            Views=("View", lambda x: ";".join(sorted(set(map(str, x)))))
        )
        .sort_values("CombinedRankPct", ascending=False)
    )

    combined["Tissue"] = tissue
    return combined[["Tissue", "Feature", "CombinedRankPct", "Views"]]


# ---------------------------
# Loading: SHAP importance
# ---------------------------

def load_shap_importance(shap_data_dir: str, tissue: str, task: str) -> pd.DataFrame:
    path = os.path.join(shap_data_dir, f"shap_importance_{tissue}_{task}.csv")
    if not os.path.exists(path):
        raise FileNotFoundError(f"Missing SHAP importance file: {path}")

    df = _safe_read_csv(path)
    if df.empty:
        raise ValueError(f"SHAP importance file is empty: {path}")

    if "Feature" not in df.columns:
        # try to infer
        feature_col = _infer_feature_col(df)
        df = df.rename(columns={feature_col: "Feature"})

    if "MeanAbsoluteShap" not in df.columns:
        # allow common alternatives
        candidates = [c for c in df.columns if c.lower() in ("meanabsoluteshap", "mean_abs_shap", "meanabs", "importance")]
        if candidates:
            df = df.rename(columns={candidates[0]: "MeanAbsoluteShap"})
        else:
            # infer any numeric column
            score_col = _infer_score_col(df, "Feature")
            if score_col is None:
                raise ValueError(f"Cannot infer SHAP importance column in: {path}")
            df = df.rename(columns={score_col: "MeanAbsoluteShap"})

    out = df[["Feature", "MeanAbsoluteShap"]].copy()
    out["Feature"] = out["Feature"].astype(str)
    out["MeanAbsoluteShap"] = pd.to_numeric(out["MeanAbsoluteShap"], errors="coerce")
    out = out.dropna(subset=["Feature", "MeanAbsoluteShap"]).sort_values("MeanAbsoluteShap", ascending=False)
    out["Tissue"] = tissue
    out["Task"] = task
    return out


# ---------------------------
# Overlap computation
# ---------------------------

def top_n_features(df: pd.DataFrame, feature_col: str, n: int) -> List[str]:
    n = int(max(1, n))
    return df[feature_col].head(n).astype(str).tolist()


def compute_overlap_details(
    mofa_features: List[str],
    shap_features: List[str],
    interface_nm: Tuple[float, float] = (546.0, 560.0),
) -> Dict:
    inter = sorted(set(mofa_features) & set(shap_features))
    union = sorted(set(mofa_features) | set(shap_features))

    wl = [w for w in (_parse_wavelength_nm(f) for f in inter) if w is not None]
    wl_sorted = sorted(wl)

    lo, hi = interface_nm
    wl_in_interface = [w for w in wl_sorted if lo <= w <= hi]

    return {
        "n_mofa": len(mofa_features),
        "n_shap": len(shap_features),
        "n_overlap": len(inter),
        "jaccard": (len(inter) / len(union)) if union else 0.0,
        "overlap_features": inter,
        "overlap_wavelengths_nm": wl_sorted,
        "overlap_wavelength_min": (min(wl_sorted) if wl_sorted else None),
        "overlap_wavelength_max": (max(wl_sorted) if wl_sorted else None),
        "n_overlap_wavelengths": len(wl_sorted),
        "n_overlap_wavelengths_in_interface": len(wl_in_interface),
        "overlap_wavelengths_in_interface": wl_in_interface,
    }


def run_overlap(
    mofa_ranking: pd.DataFrame,
    shap_df: pd.DataFrame,
    ks: List[int],
    pcts: List[float],
    interface_nm: Tuple[float, float],
) -> Tuple[pd.DataFrame, Dict]:
    """Return summary table + nested details dict."""

    tissue = str(shap_df["Tissue"].iloc[0])
    task = str(shap_df["Task"].iloc[0])

    details: Dict[str, Dict] = {}
    rows = []

    # Ensure rankings are sorted
    mofa_sorted = mofa_ranking.sort_values("CombinedRankPct", ascending=False).reset_index(drop=True)
    shap_sorted = shap_df.sort_values("MeanAbsoluteShap", ascending=False).reset_index(drop=True)

    # Top-K thresholds
    for k in ks:
        mofa_top = top_n_features(mofa_sorted, "Feature", k)
        shap_top = top_n_features(shap_sorted, "Feature", k)
        d = compute_overlap_details(mofa_top, shap_top, interface_nm=interface_nm)

        key = f"topk_{k}"
        details[key] = d
        rows.append({
            "Tissue": tissue,
            "Task": task,
            "ThresholdType": "topk",
            "Threshold": k,
            "MOFA_n": d["n_mofa"],
            "SHAP_n": d["n_shap"],
            "Overlap_n": d["n_overlap"],
            "Jaccard": d["jaccard"],
            "OverlapWavelengths_n": d["n_overlap_wavelengths"],
            "OverlapWavelengthsInInterface_n": d["n_overlap_wavelengths_in_interface"],
            "OverlapWavelengthMin_nm": d["overlap_wavelength_min"],
            "OverlapWavelengthMax_nm": d["overlap_wavelength_max"],
        })

    # Percent thresholds (per-list lengths)
    for pct in pcts:
        if pct <= 0 or pct > 1:
            continue
        mofa_n = int(np.ceil(pct * len(mofa_sorted)))
        shap_n = int(np.ceil(pct * len(shap_sorted)))
        mofa_top = top_n_features(mofa_sorted, "Feature", mofa_n)
        shap_top = top_n_features(shap_sorted, "Feature", shap_n)
        d = compute_overlap_details(mofa_top, shap_top, interface_nm=interface_nm)

        key = f"pct_{pct}"
        details[key] = d
        rows.append({
            "Tissue": tissue,
            "Task": task,
            "ThresholdType": "pct",
            "Threshold": pct,
            "MOFA_n": d["n_mofa"],
            "SHAP_n": d["n_shap"],
            "Overlap_n": d["n_overlap"],
            "Jaccard": d["jaccard"],
            "OverlapWavelengths_n": d["n_overlap_wavelengths"],
            "OverlapWavelengthsInInterface_n": d["n_overlap_wavelengths_in_interface"],
            "OverlapWavelengthMin_nm": d["overlap_wavelength_min"],
            "OverlapWavelengthMax_nm": d["overlap_wavelength_max"],
        })

    summary_df = pd.DataFrame(rows)
    return summary_df, {"tissue": tissue, "task": task, "details": details}


# ---------------------------
# CLI
# ---------------------------

def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(
        description="Compute MOFA-vs-SHAP feature overlap using analyse_transformer_shap_v2.py outputs."
    )
    p.add_argument("--base_dir", default=DEFAULT_BASE_DIR, help="Project base dir (default matches pipeline).")
    p.add_argument("--mofa_dir", default=None, help="Directory containing MOFA importance CSV exports.")
    p.add_argument("--shap_data_dir", default=None, help="Directory containing SHAP importance CSV exports.")
    p.add_argument("--out_dir", default=None, help="Output directory for overlap tables.")

    p.add_argument("--k", nargs="+", type=int, default=[50], help="Top-K thresholds to compute.")
    p.add_argument("--pct", nargs="+", type=float, default=[0.01, 0.05, 0.10], help="Top-percent thresholds (0-1).")

    p.add_argument("--interface_nm", nargs=2, type=float, default=[546.0, 560.0],
                   help="Spectral interface wavelength range (nm) to count overlap within.")

    p.add_argument("--tissues", nargs="+", default=TISSUES, choices=TISSUES,
                   help="Tissues to process (Leaf/Root).")
    p.add_argument("--tasks", nargs="+", default=TASKS, choices=TASKS,
                   help="Tasks to process (Genotype/Treatment/Day).")

    return p.parse_args()


def main() -> int:
    args = parse_args()

    base_dir = args.base_dir
    mofa_dir = args.mofa_dir or os.path.join(base_dir, "output", "mofa")
    shap_data_dir = args.shap_data_dir or os.path.join(
        base_dir, "output", "transformer", "shap_analysis_ggl", "importance_data"
    )
    out_dir = args.out_dir or os.path.join(base_dir, "output", "transformer", "mofa_shap_overlap")

    os.makedirs(out_dir, exist_ok=True)

    interface_nm = (float(args.interface_nm[0]), float(args.interface_nm[1]))

    all_summary = []
    all_details: Dict[str, Dict] = {"interface_nm": list(interface_nm), "results": {}}

    # Prebuild MOFA rankings per tissue
    mofa_rankings: Dict[str, pd.DataFrame] = {}
    for tissue in args.tissues:
        mofa_rankings[tissue] = build_tissue_mofa_ranking(mofa_dir, tissue)

    for tissue in args.tissues:
        for task in args.tasks:
            shap_df = load_shap_importance(shap_data_dir, tissue, task)
            summary_df, detail = run_overlap(
                mofa_rankings[tissue],
                shap_df,
                ks=list(args.k),
                pcts=list(args.pct),
                interface_nm=interface_nm,
            )
            all_summary.append(summary_df)

            all_details["results"].setdefault(tissue, {})[task] = detail["details"]

    summary = pd.concat(all_summary, ignore_index=True) if all_summary else pd.DataFrame()

    summary_path = os.path.join(out_dir, "mofa_shap_overlap_summary.csv")
    details_path = os.path.join(out_dir, "mofa_shap_overlap_details.json")

    summary.to_csv(summary_path, index=False)
    with open(details_path, "w", encoding="utf-8") as f:
        json.dump(all_details, f, indent=2)

    print(f"Wrote: {summary_path}")
    print(f"Wrote: {details_path}")

    # Small console summary (topk default)
    try:
        topk_rows = summary[summary["ThresholdType"] == "topk"].copy()
        if not topk_rows.empty:
            # Print the smallest K in the list
            min_k = min(args.k)
            sel = topk_rows[topk_rows["Threshold"] == min_k]
            if not sel.empty:
                print("\nTop-K overlap summary (K=%d):" % min_k)
                cols = ["Tissue", "Task", "Overlap_n", "Jaccard", "OverlapWavelengthsInInterface_n"]
                print(sel[cols].to_string(index=False))
    except Exception:
        pass

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
