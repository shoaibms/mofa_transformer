analyse_transformer_shap.py
# -*- coding: utf-8 -*-
"""
SHAP Analysis for Multi-Omic Transformer with Feature Attention

This script performs SHAP (SHapley Additive exPlanations) analysis on a trained
multi-omic transformer model that uses feature-level attention. It evaluates
feature importance across different omics types (spectral and metabolite) and
prediction tasks (genotype, treatment, day).

The script uses GradientExplainer to calculate SHAP values and generates:
- SHAP importance CSV files per task/tissue pairing
- Basic SHAP summary bar plots
- Advanced visualizations:
  - SHAP clustermaps showing top features across tasks
  - Omics contribution stacked bar plots
  - Faceted top features bar plots showing importance by task

This implementation supports separate metadata and feature files and
applies concatenated input strategy for SHAP calculation.

Outputs:
- SHAP importance CSV files per task/pairing (in SHAP_DATA_DIR).
- Basic SHAP summary bar plots per task/pairing (in SHAP_PLOT_DIR).
- Advanced SHAP clustermaps per pairing (in SHAP_PLOT_DIR).
- Advanced Omics Contribution stacked bar plots per pairing (in SHAP_PLOT_DIR).
- Advanced Faceted Top Features bar plots per pairing (in SHAP_PLOT_DIR) - like Figure 4.
"""

# Standard library imports
import os
import sys
import time
import logging
import traceback
from datetime import datetime

# Third-party imports
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns

# Check for required packages
try:
    import shap
except ImportError:
    print("ERROR: SHAP library not found. Please install using 'pip install shap'")
    sys.exit(1)

try:
    import pyarrow
except ImportError:
    pyarrow = None
    print("WARNING: `pyarrow` library not found. Will not be able to read .feather metadata files. Will attempt .csv fallback.")

# ===== CONFIGURATION =====

# --- Script Info ---
SCRIPT_NAME = "run_shap_gradient_v3_model_integrated_plots_v2"
VERSION = "1.2.0"

# --- Paths ---
BASE_DIR = r"C:/Users/ms/Desktop/hyper"
MOFA_OUTPUT_DIR = os.path.join(BASE_DIR, "output", "mofa")
TRANSFORMER_V3_OUTPUT_DIR = os.path.join(
    BASE_DIR, "output", "transformer", "v3_feature_attention"
)
SHAP_OUTPUT_DIR = os.path.join(BASE_DIR, "output", "transformer", "shap_analysis_ggl")

# Specific subdirs within SHAP_OUTPUT_DIR
SHAP_DATA_DIR = os.path.join(SHAP_OUTPUT_DIR, "importance_data")
SHAP_PLOT_DIR = os.path.join(SHAP_OUTPUT_DIR, "plots")
LOG_DIR = os.path.join(SHAP_OUTPUT_DIR, "logs")

# Ensure output directories exist
os.makedirs(SHAP_OUTPUT_DIR, exist_ok=True)
os.makedirs(SHAP_DATA_DIR, exist_ok=True)
os.makedirs(SHAP_PLOT_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

# --- Model Checkpoint Paths (v3 model) ---
MODEL_PATHS = {
    "Leaf": os.path.join(
        TRANSFORMER_V3_OUTPUT_DIR,
        r"leaf/checkpoints/Leaf/best_model_transformer_multi_omic_v3_attention_Leaf.pth"
    ),
    "Root": os.path.join(
        TRANSFORMER_V3_OUTPUT_DIR,
        r"root/checkpoints/Root/best_model_transformer_multi_omic_v3_attention_Root.pth"
    )
}

# --- Input Feature Data Paths ---
FEATURE_INPUT_FILES = {
    "Leaf": {
        "spectral": os.path.join(MOFA_OUTPUT_DIR, "transformer_input_leaf_spectral.csv"),
        "metabolite": os.path.join(MOFA_OUTPUT_DIR, "transformer_input_leaf_metabolite.csv")
    },
    "Root": {
        "spectral": os.path.join(MOFA_OUTPUT_DIR, "transformer_input_root_spectral.csv"),
        "metabolite": os.path.join(MOFA_OUTPUT_DIR, "transformer_input_root_metabolite.csv")
    }
}

# --- Input Metadata File Paths ---
METADATA_INPUT_FILES = {
    "Leaf": os.path.join(
        TRANSFORMER_V3_OUTPUT_DIR, 
        r"leaf/results/raw_attention_metadata_Leaf.feather"
    ),
    "Root": os.path.join(
        TRANSFORMER_V3_OUTPUT_DIR,
        r"root/results/raw_attention_metadata_Root.feather"
    )
}
METADATA_CSV_FALLBACK_FILES = {
    "Leaf": os.path.join(
        TRANSFORMER_V3_OUTPUT_DIR,
        r"leaf/results/raw_attention_metadata_Leaf.csv"
    ),
    "Root": os.path.join(
        TRANSFORMER_V3_OUTPUT_DIR,
        r"root/results/raw_attention_metadata_Root.csv"
    )
}

# --- Data & Columns ---
KNOWN_METADATA_COLS_IN_FEATURES = [
    'Vac_id', 'Genotype', 'Entry', 'Tissue.type', 
    'Batch', 'Treatment', 'Replication', 'Day'
]
TARGET_COLS = ['Genotype', 'Treatment', 'Day']
METADATA_INDEX_COL = 'Row_names'

# --- Model Hyperparameters ---
HIDDEN_DIM = 64
NUM_HEADS = 4
NUM_LAYERS = 2
DROPOUT = 0.1
NUM_CLASSES = {'Genotype': 2, 'Treatment': 2, 'Day': 3}

# --- Preprocessing Parameters ---
VAL_SIZE = 0.15
TEST_SIZE = 0.15
RANDOM_SEED = 42
ENCODING_MAPS = {
    'Genotype': {'G1': 0, 'G2': 1},
    'Treatment': {0: 0, 1: 1},
    'Day': {1: 0, 2: 1, 3: 2}
}

# --- SHAP Parameters ---
SHAP_EXPLAINER = shap.GradientExplainer
SHAP_BACKGROUND_SAMPLES = 100
SHAP_INSTANCE_SAMPLES = 200
SHAP_MAX_DISPLAY = 20  # For basic bar plot

# --- Visualization Parameters ---
FIG_DPI = 300
TOP_N_FEATURES_HEATMAP = 50  # For clustermap
TOP_M_FEATURES_PER_TASK = 15  # For faceted bar plot
FIG_SIZE_WIDE = (12, 8)
FIG_SIZE_TALL = (10, 12)
FIG_SIZE_SQUARE = (10, 10)
FONT_SIZE_TITLE = 16
FONT_SIZE_LABEL = 12
FONT_SIZE_TICK = 10
FEATURE_NAME_MAX_LENGTH = 40  # Truncate feature names in plots
OMICS_PALETTE = {"Spectral": "skyblue", "Metabolite": "lightcoral"}
CLUSTERMAP_CMAP = 'viridis'  # Colormap for clustermap

# --- Device ---
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ===== LOGGING =====
def setup_logging(log_dir: str, script_name: str, version: str) -> logging.Logger:
    """Set up logging for the script execution."""
    os.makedirs(log_dir, exist_ok=True)
    log_filename = f"{script_name}_{version}_{datetime.now():%Y%m%d_%H%M%S}.log"
    log_filepath = os.path.join(log_dir, log_filename)
    log_format = '%(asctime)s - %(levelname)s - [%(module)s:%(lineno)d] - %(message)s'
    date_format = '%Y-%m-%d %H:%M:%S'
    formatter = logging.Formatter(log_format, datefmt=date_format)
    
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    if logger.hasHandlers():
        logger.handlers.clear()
    
    file_handler = logging.FileHandler(log_filepath)
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    def handle_exception(exc_type, exc_value, exc_traceback):
        if issubclass(exc_type, KeyboardInterrupt):
            sys.__excepthook__(exc_type, exc_value, exc_traceback)
            return
        logger.error("Uncaught exception", exc_info=(exc_type, exc_value, exc_traceback))
    
    sys.excepthook = handle_exception
    logger.info(f"Logging setup complete. Log file: {log_filepath}")
    return logger


logger = setup_logging(LOG_DIR, SCRIPT_NAME, VERSION)

logger.info("="*60)
logger.info(f"Starting SHAP Analysis Script: {SCRIPT_NAME} v{VERSION}")
logger.info(f"Analyzing v3 Feature Attention Model using GradientExplainer")
logger.info(f"Includes Integrated Advanced Visualizations")
logger.info(f"Output Directory: {SHAP_OUTPUT_DIR}")
logger.info(f"Using Device: {DEVICE}")
logger.info(f"SHAP Params: Background Samples={SHAP_BACKGROUND_SAMPLES}, "
           f"Instance Samples={SHAP_INSTANCE_SAMPLES}")
logger.info(f"Plot Params: Top N (Clustermap)={TOP_N_FEATURES_HEATMAP}, "
           f"Top M (Faceted)={TOP_M_FEATURES_PER_TASK}")
logger.info("="*60)


# ===== MODEL DEFINITION =====
class CrossAttentionLayer(nn.Module):
    """Cross attention layer for interaction between different omics types."""
    
    def __init__(self, hidden_dim, num_heads, dropout=0.1):
        super().__init__()
        self.cross_attn_1_to_2 = nn.MultiheadAttention(
            hidden_dim, num_heads, dropout=dropout, batch_first=True
        )
        self.cross_attn_2_to_1 = nn.MultiheadAttention(
            hidden_dim, num_heads, dropout=dropout, batch_first=True
        )
        self.norm1 = nn.LayerNorm(hidden_dim)
        self.norm2 = nn.LayerNorm(hidden_dim)
        self.dropout = nn.Dropout(dropout)
        self.ffn = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 2, hidden_dim)
        )
        self.norm3 = nn.LayerNorm(hidden_dim)
        
    def forward(self, x1, x2):
        attn_output_1, attn_weights_1_to_2 = self.cross_attn_1_to_2(
            query=x1, key=x2, value=x2, average_attn_weights=False
        )
        x1 = self.norm1(x1 + self.dropout(attn_output_1))
        
        attn_output_2, attn_weights_2_to_1 = self.cross_attn_2_to_1(
            query=x2, key=x1, value=x1, average_attn_weights=False
        )
        x2 = self.norm2(x2 + self.dropout(attn_output_2))
        
        ffn_output1 = self.ffn(x1)
        x1 = self.norm3(x1 + self.dropout(ffn_output1))
        ffn_output2 = self.ffn(x2)
        x2 = self.norm3(x2 + self.dropout(ffn_output2))
        
        return x1, x2, attn_weights_1_to_2, attn_weights_2_to_1


class SimplifiedTransformer(nn.Module):
    """Multi-omic transformer model with feature-level attention."""
    
    def __init__(self, spectral_dim, metabolite_dim, hidden_dim, num_heads, 
                num_layers, num_classes, dropout=0.1):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_classes = num_classes
        
        # Feature embeddings
        self.spectral_feature_embedding = nn.Linear(1, hidden_dim)
        self.metabolite_feature_embedding = nn.Linear(1, hidden_dim)
        
        # Positional encodings
        self.pos_encoding_spec = nn.Parameter(
            torch.randn(1, spectral_dim, hidden_dim) * 0.02
        )
        self.pos_encoding_metab = nn.Parameter(
            torch.randn(1, metabolite_dim, hidden_dim) * 0.02
        )
        
        # Normalization and dropout
        self.embedding_norm_spec = nn.LayerNorm(hidden_dim)
        self.embedding_norm_metab = nn.LayerNorm(hidden_dim)
        self.embedding_dropout = nn.Dropout(dropout)
        
        # Cross-attention layers
        self.cross_attention_layers = nn.ModuleList(
            [CrossAttentionLayer(hidden_dim, num_heads, dropout) 
             for _ in range(num_layers)]
        )
        
        # Output heads for each task
        self.output_heads = nn.ModuleDict()
        total_pooled_dim = hidden_dim * 2
        for task_name, n_class in num_classes.items():
            self.output_heads[task_name] = nn.Sequential(
                nn.LayerNorm(total_pooled_dim),
                nn.Linear(total_pooled_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(hidden_dim, n_class)
            )
        
        self.attention_weights = {}
        
    def forward(self, spectral, metabolite):
        # Reshape inputs and apply feature embeddings
        spec_reshaped = spectral.unsqueeze(-1)
        metab_reshaped = metabolite.unsqueeze(-1)
        spec_emb = self.spectral_feature_embedding(spec_reshaped)
        metab_emb = self.metabolite_feature_embedding(metab_reshaped)
        
        # Add positional encodings
        spec_emb = spec_emb + self.pos_encoding_spec
        metab_emb = metab_emb + self.pos_encoding_metab
        
        # Apply normalization and dropout
        spec_emb = self.embedding_dropout(self.embedding_norm_spec(spec_emb))
        metab_emb = self.embedding_dropout(self.embedding_norm_metab(metab_emb))
        
        # Process through cross-attention layers
        self.attention_weights = {}
        last_attn_1_to_2, last_attn_2_to_1 = None, None
        
        for i, layer in enumerate(self.cross_attention_layers):
            spec_emb, metab_emb, attn_1_to_2, attn_2_to_1 = layer(spec_emb, metab_emb)
            if i == len(self.cross_attention_layers) - 1:
                self.attention_weights = {'1_to_2': attn_1_to_2, '2_to_1': attn_2_to_1}
        
        # Global pooling and task-specific predictions
        spec_pooled = spec_emb.mean(dim=1)
        metab_pooled = metab_emb.mean(dim=1)
        combined_pooled = torch.cat([spec_pooled, metab_pooled], dim=1)
        
        outputs = {}
        for task_name, head in self.output_heads.items():
            outputs[task_name] = head(combined_pooled)
            
        return outputs


# ===== DATA LOADING & PREPROCESSING =====
def load_metadata_file(feather_path: str, csv_fallback_path: str, 
                       index_col: str, target_cols: list) -> pd.DataFrame or None:
    """Load metadata from feather file with CSV fallback option."""
    metadata_df = None
    
    # Try loading from feather first
    if pyarrow and os.path.exists(feather_path):
        logger.info(f"Attempting to load metadata from Feather: {feather_path}")
        try:
            metadata_df = pd.read_feather(feather_path)
            if index_col in metadata_df.columns:
                metadata_df.set_index(index_col, inplace=True)
                logger.info(f"  Loaded metadata from Feather, shape: {metadata_df.shape}. "
                           f"Index set to '{index_col}'.")
            else:
                logger.error(f"Index column '{index_col}' not found in Feather file.")
                raise ValueError(f"Index column '{index_col}' not found in Feather file.")
                
            missing_targets = [col for col in target_cols if col not in metadata_df.columns]
            if missing_targets:
                logger.error(f"Metadata file {feather_path} missing targets: {missing_targets}")
                raise ValueError(f"Missing target columns: {missing_targets}")
                
            return metadata_df
        except Exception as e_feather:
            logger.warning(f"  Failed load from Feather {feather_path}: {e_feather}. Trying CSV.")
            metadata_df = None
    
    # Try loading from CSV as fallback
    if metadata_df is None and os.path.exists(csv_fallback_path):
        logger.info(f"Attempting to load metadata from CSV: {csv_fallback_path}")
        try:
            metadata_df = pd.read_csv(csv_fallback_path, index_col=index_col)
            if metadata_df.index.name == index_col:
                logger.info(f"  Loaded metadata from CSV, shape: {metadata_df.shape}. "
                           f"Index set to '{index_col}'.")
            else:
                logger.error(f"Failed to set index to '{index_col}' from CSV.")
                raise ValueError(f"Failed index set CSV '{index_col}'.")
                
            missing_targets = [col for col in target_cols if col not in metadata_df.columns]
            if missing_targets:
                logger.error(f"Metadata file {csv_fallback_path} missing targets: {missing_targets}")
                raise ValueError(f"Missing target columns: {missing_targets}")
                
            return metadata_df
        except Exception as e_csv:
            logger.error(f"  Failed to load metadata from CSV {csv_fallback_path}: {e_csv}")
            return None
    
    if metadata_df is None:
        logger.error(f"Metadata file not found or failed load. Checked Feather: "
                    f"{feather_path}, CSV: {csv_fallback_path}")
        
    return None


def load_and_preprocess_for_shap(config: dict, pairing: str) -> tuple:
    """Load and preprocess data for SHAP analysis."""
    logger.info(f"--- Starting Data Loading & Preprocessing for SHAP ({pairing}) ---")
    
    spectral_path = config['FEATURE_INPUT_FILES'][pairing]['spectral']
    metabolite_path = config['FEATURE_INPUT_FILES'][pairing]['metabolite']
    metadata_path = config['METADATA_INPUT_FILES'][pairing]
    metadata_csv_fallback = config['METADATA_CSV_FALLBACK_FILES'][pairing]
    index_col = config['METADATA_INDEX_COL']
    target_cols = config['TARGET_COLS']
    known_meta_in_features = config['KNOWN_METADATA_COLS_IN_FEATURES']

    try:
        # Load raw data files
        logger.info(f"Loading spectral data (may include metadata): {spectral_path}")
        spectral_df_raw = pd.read_csv(spectral_path, index_col=index_col, na_values='NA')
        
        logger.info(f"Loading metabolite data (may include metadata): {metabolite_path}")
        metabolite_df_raw = pd.read_csv(metabolite_path, index_col=index_col, na_values='NA')

        # Extract feature names
        spectral_feature_names = spectral_df_raw.columns.difference(
            known_meta_in_features, sort=False).tolist()
        metabolite_feature_names = metabolite_df_raw.columns.difference(
            known_meta_in_features, sort=False).tolist()
        
        if not spectral_feature_names:
            raise ValueError("No spectral features identified after excluding metadata.")
        if not metabolite_feature_names:
            raise ValueError("No metabolite features identified after excluding metadata.")

        # Extract features
        features_spectral_df = spectral_df_raw[spectral_feature_names]
        features_metabolite_df = metabolite_df_raw[metabolite_feature_names]
        
        logger.info(f"Identified {len(spectral_feature_names)} spectral features "
                   f"(e.g., '{spectral_feature_names[0]}').")
        logger.info(f"Identified {len(metabolite_feature_names)} metabolite features "
                   f"(e.g., '{metabolite_feature_names[0]}').")

        # Load metadata
        metadata_df = load_metadata_file(
            metadata_path, metadata_csv_fallback, index_col, target_cols
        )
        if metadata_df is None:
            raise FileNotFoundError(f"Metadata load failed for {pairing}.")
            
        logger.info(f"Feature DFs and Metadata loaded. Spec feats: "
                   f"{len(spectral_feature_names)}, Metab feats: "
                   f"{len(metabolite_feature_names)}, Meta rows: {len(metadata_df)}")

        # Align indices
        common_indices = features_spectral_df.index.intersection(
            features_metabolite_df.index).intersection(metadata_df.index)
        
        n_spectral = len(features_spectral_df)
        n_metab = len(features_metabolite_df)
        n_meta = len(metadata_df)
        n_common = len(common_indices)
        
        if n_common == 0:
            raise ValueError("Alignment failed: No common Row_names.")
        elif n_common < n_spectral or n_common < n_metab or n_common < n_meta:
            logger.warning(f"Found {n_common} common samples (Spec:{n_spectral}, "
                          f"Metab:{n_metab}, Meta:{n_meta}). Aligning.")
            
        features_spectral_df = features_spectral_df.loc[common_indices]
        features_metabolite_df = features_metabolite_df.loc[common_indices]
        metadata_df = metadata_df.loc[common_indices]
        
        logger.info(f"Data successfully aligned by index '{index_col}'. Samples: {n_common}")

        # Encode targets
        targets_encoded = pd.DataFrame(index=metadata_df.index)
        label_encoders = {}
        
        for col in target_cols:
            if col in config['ENCODING_MAPS']:
                targets_encoded[col] = metadata_df[col].map(config['ENCODING_MAPS'][col])
                if targets_encoded[col].isnull().any():
                    missing_vals = metadata_df.loc[targets_encoded[col].isnull(), col].unique()
                    raise ValueError(f"Encoding NaN for '{col}'. Unmapped: {missing_vals}")
            else:
                logger.warning(f"Using LabelEncoder for '{col}'.")
                le = LabelEncoder()
                targets_encoded[col] = le.fit_transform(metadata_df[col])
                label_encoders[col] = le

        # Perform train/test split
        logger.info(f"Performing train/test split using RANDOM_SEED={config['RANDOM_SEED']}")
        
        # Create a combined stratification key
        stratify_key_col = '_stratify_key_'
        metadata_df[stratify_key_col] = metadata_df[target_cols[0]].astype(str)
        for col in target_cols[1:]:
            metadata_df[stratify_key_col] += '_' + metadata_df[col].astype(str)
            
        indices = metadata_df.index
        stratify_values = metadata_df[stratify_key_col]
        
        if config['TEST_SIZE'] <= 0 or config['TEST_SIZE'] >= 1:
            raise ValueError("TEST_SIZE must be > 0 and < 1")
            
        try:
            train_idx, test_idx = train_test_split(
                indices, 
                test_size=config['TEST_SIZE'],
                random_state=config['RANDOM_SEED'],
                stratify=stratify_values
            )
        except ValueError as e:
            logger.warning(f"Stratification failed: {e}. No stratify.")
            train_idx, test_idx = train_test_split(
                indices,
                test_size=config['TEST_SIZE'],
                random_state=config['RANDOM_SEED']
            )
            
        metadata_df.drop(columns=[stratify_key_col], inplace=True, errors='ignore')

        # Split features for train/test sets
        X_train_spec = features_spectral_df.loc[train_idx]
        X_test_spec = features_spectral_df.loc[test_idx]
        X_train_metab = features_metabolite_df.loc[train_idx]
        X_test_metab = features_metabolite_df.loc[test_idx]
        
        logger.info(f"Split sizes for SHAP: Train={len(train_idx)}, Test={len(test_idx)}")
        
        if len(test_idx) == 0 or len(train_idx) == 0:
            raise ValueError("Train or Test set is empty.")

        # Scale features
        logger.info("Scaling features (fit on train)...")
        scaler_spec = StandardScaler()
        scaler_metab = StandardScaler()
        
        X_train_spec_scaled = scaler_spec.fit_transform(X_train_spec)
        X_test_spec_scaled = scaler_spec.transform(X_test_spec)
        X_train_metab_scaled = scaler_metab.fit_transform(X_train_metab)
        X_test_metab_scaled = scaler_metab.transform(X_test_metab)
        
        if np.isnan(X_train_spec_scaled).any() or np.isinf(X_train_spec_scaled).any():
            raise ValueError("Bad values in scaled spec train")
            
        if np.isnan(X_train_metab_scaled).any() or np.isinf(X_train_metab_scaled).any():
            raise ValueError("Bad values in scaled metab train")

        # Convert to tensors
        logger.info("Converting to Tensors...")
        X_train_spec_tensor = torch.tensor(X_train_spec_scaled, dtype=torch.float32)
        X_train_metab_tensor = torch.tensor(X_train_metab_scaled, dtype=torch.float32)
        X_test_spec_tensor = torch.tensor(X_test_spec_scaled, dtype=torch.float32)
        X_test_metab_tensor = torch.tensor(X_test_metab_scaled, dtype=torch.float32)
        
        logger.info("--- Data Loading & Preprocessing for SHAP Finished ---")
        
        return (
            X_train_spec_tensor, X_train_metab_tensor,
            X_test_spec_tensor, X_test_metab_tensor,
            spectral_feature_names, metabolite_feature_names
        )
    except Exception as e:
        logger.error(f"Error in preprocessing: {e}", exc_info=True)
        raise


# ===== MODEL LOADING =====
def load_trained_model(model_path: str, spectral_dim: int, metabolite_dim: int, 
                      config: dict, device: torch.device) -> SimplifiedTransformer:
    """Load trained model from checkpoint file."""
    logger.info(f"Loading trained model state from: {model_path}")
    
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Model checkpoint not found: {model_path}")
        
    model = SimplifiedTransformer(
        spectral_dim=spectral_dim,
        metabolite_dim=metabolite_dim,
        hidden_dim=config['HIDDEN_DIM'],
        num_heads=config['NUM_HEADS'],
        num_layers=config['NUM_LAYERS'],
        num_classes=config['NUM_CLASSES'],
        dropout=config['DROPOUT']
    )
    
    try:
        model.load_state_dict(torch.load(model_path, map_location=device))
        model.to(device)
        model.eval()
        logger.info("Model loaded and set to eval mode.")
        return model
    except Exception as e:
        logger.error(f"Error loading model state dict: {e}", exc_info=True)
        raise


# ===== SHAP WRAPPER MODEL (Concatenated Input) =====
class ShapModelWrapper(nn.Module):
    """Wrapper around the transformer model for use with SHAP explainer.
    
    Handles splitting concatenated input into spectral and metabolite components.
    """
    
    def __init__(self, original_model, task_name, spectral_dim, metabolite_dim):
        super().__init__()
        self.original_model = original_model
        self.task_name = task_name
        self.spectral_dim = spectral_dim
        self.metabolite_dim = metabolite_dim
        self.device = next(original_model.parameters()).device
        self.num_classes = original_model.num_classes[task_name]
        
    def forward(self, combined_input):
        # Convert numpy array to tensor if needed
        if isinstance(combined_input, np.ndarray):
            combined_input = torch.from_numpy(combined_input).float()
            
        # Move input to correct device
        combined_input = combined_input.to(self.device)
        
        # Split combined input into spectral and metabolite components
        split_sizes = [self.spectral_dim, self.metabolite_dim]
        try:
            spectral, metabolite = torch.split(combined_input, split_sizes, dim=1)
        except Exception as e_split:
            logger.error(f"Error splitting SHAP input in wrapper ({self.task_name}): "
                        f"{e_split}. Shape: {combined_input.shape}, "
                        f"Splits: {split_sizes}. Returning zeros.")
            batch_size = combined_input.shape[0]
            return torch.zeros((batch_size, self.num_classes), device=self.device)
            
        # Forward pass through original model
        outputs = self.original_model(spectral, metabolite)
        return outputs[self.task_name]


# ===== SHAP CALCULATION (GradientExplainer, Concatenated Input) =====
def calculate_shap_values(model, X_train_spec, X_train_metab, X_test_spec, X_test_metab,
                          spectral_feature_names, metabolite_feature_names,
                          target_cols, config, device):
    """Calculate SHAP values using GradientExplainer with concatenated input approach."""
    logger.info(f"--- Starting SHAP Value Calculation (Using "
               f"{config['SHAP_EXPLAINER'].__name__} & Concatenated Input) ---")
    
    spectral_dim = len(spectral_feature_names)
    metabolite_dim = len(metabolite_feature_names)
    total_features = spectral_dim + metabolite_dim

    # Prepare background data
    bg_samples = config['SHAP_BACKGROUND_SAMPLES']
    n_train = X_train_spec.shape[0]
    
    if n_train == 0:
        raise ValueError("Training set empty.")
        
    if bg_samples >= n_train:
        logger.warning(f"BG samples ({bg_samples}) >= train ({n_train}). Using all.")
        bg_indices = np.arange(n_train)
        bg_samples = n_train
    else:
        logger.info(f"Selecting {bg_samples} BG samples.")
        bg_indices = np.random.choice(n_train, bg_samples, replace=False)
        
    background_spec = X_train_spec[bg_indices]
    background_metab = X_train_metab[bg_indices]
    background_combined = torch.cat((background_spec, background_metab), dim=1)
    logger.info(f"BG shape (Concat): {background_combined.shape}")

    # Prepare instance data
    instance_samples = config['SHAP_INSTANCE_SAMPLES']
    n_test = X_test_spec.shape[0]
    
    if n_test == 0:
        raise ValueError("Test set empty.")
        
    if instance_samples is None or instance_samples >= n_test:
        logger.info(f"Using all {n_test} test samples.")
        instance_indices = np.arange(n_test)
        instance_samples = n_test
    else:
        logger.info(f"Selecting {instance_samples} instance samples.")
        instance_indices = np.random.choice(n_test, instance_samples, replace=False)
        
    instance_spec = X_test_spec[instance_indices]
    instance_metab = X_test_metab[instance_indices]
    instance_combined = torch.cat((instance_spec, instance_metab), dim=1)
    logger.info(f"Instance shape (Concat): {instance_combined.shape}")

    # Calculate SHAP values for each task
    all_shap_values_combined = {}
    ExplainerClass = config['SHAP_EXPLAINER']

    for task_name in target_cols:
        logger.info(f"--- Calculating SHAP values for Task: {task_name} ---")
        start_task_time = time.time()
        
        # Create task-specific model wrapper
        shap_wrapper_model = ShapModelWrapper(
            model, task_name, spectral_dim, metabolite_dim
        ).to(device)
        shap_wrapper_model.eval()
        background_combined_dev = background_combined.to(device)

        # Initialize SHAP explainer
        try:
            explainer = ExplainerClass(shap_wrapper_model, background_combined_dev)
            logger.info(f"SHAP {ExplainerClass.__name__} instantiated for {task_name}.")
        except Exception as e_explain:
            logger.error(f"Failed to instantiate {ExplainerClass.__name__}: {e_explain}", 
                        exc_info=True)
            all_shap_values_combined[task_name] = None
            continue

        # Calculate SHAP values
        instance_combined_dev = instance_combined.to(device)
        logger.info(f"Calculating SHAP values for {instance_combined_dev.shape[0]} instances...")
        try:
            shap_values_raw = explainer.shap_values(instance_combined_dev)

            # Log SHAP output structure
            if isinstance(shap_values_raw, list):
                shapes_info_list = [f"{i}:{getattr(e, 'shape', 'N/A')}" 
                                   for i, e in enumerate(shap_values_raw)]
                types_info_list = [f"{i}:{type(e)}" 
                                  for i, e in enumerate(shap_values_raw)]
                logger.info(f"SHAP returned list (len={len(shap_values_raw)}). "
                           f"Shapes: [{'; '.join(shapes_info_list)}]. "
                           f"Types: [{'; '.join(types_info_list)}]")
            elif isinstance(shap_values_raw, np.ndarray):
                logger.info(f"SHAP returned ndarray shape {shap_values_raw.shape}.")
            else:
                logger.info(f"SHAP returned type: {type(shap_values_raw)}")

            # Validate and normalize SHAP output format
            num_expected_classes = shap_wrapper_model.num_classes
            n_samples_expected = instance_combined_dev.shape[0]
            n_features_expected = instance_combined_dev.shape[1]

            shap_values_list_np = []
            
            # Handle 3D array output
            if isinstance(shap_values_raw, np.ndarray) and shap_values_raw.ndim == 3:
                n_samples_out, n_features_out, n_classes_out = shap_values_raw.shape
                logger.info(f"SHAP returned 3D NumPy array shape: "
                           f"{(n_samples_out, n_features_out, n_classes_out)}. "
                           f"Converting to list of 2D arrays.")
                           
                if (n_samples_out != n_samples_expected or 
                    n_features_out != n_features_expected or 
                    n_classes_out != num_expected_classes):
                    logger.error(f"SHAP output array dimension mismatch! "
                                f"Got ({n_samples_out}, {n_features_out}, {n_classes_out}), "
                                f"expected ({n_samples_expected}, {n_features_expected}, "
                                f"{num_expected_classes}).")
                    raise ValueError(f"Dimension mismatch in SHAP output array for task {task_name}")
                    
                for i in range(n_classes_out):
                    shap_values_list_np.append(shap_values_raw[:, :, i])
                    
                logger.info(f"Converted 3D array into list of {len(shap_values_list_np)} arrays.")

            # Handle list output
            elif isinstance(shap_values_raw, list) and len(shap_values_raw) == num_expected_classes:
                logger.info("SHAP returned a list as expected. Converting elements to NumPy CPU arrays.")
                shap_values_list_np = [(v.cpu().numpy() if isinstance(v, torch.Tensor) else v) 
                                     for v in shap_values_raw]
                                     
                for i, arr in enumerate(shap_values_list_np):
                    if not isinstance(arr, np.ndarray) or arr.shape != (n_samples_expected, n_features_expected):
                        raise ValueError(f"List element {i} has incorrect shape/type: "
                                       f"{type(arr)}, {getattr(arr, 'shape', 'N/A')}")
            else:
                raise TypeError(f"Unexpected SHAP output format for {task_name}. "
                              f"Got type {type(shap_values_raw)} with unexpected structure/dims.")

            # Validate final output structure
            all_valid_structure = True
            if not isinstance(shap_values_list_np, list) or len(shap_values_list_np) != num_expected_classes:
                all_valid_structure = False
                logger.error(f"Validation failed: result is not a list of length {num_expected_classes}.")
            else:
                for i, arr in enumerate(shap_values_list_np):
                    if not isinstance(arr, np.ndarray) or arr.shape != (n_samples_expected, n_features_expected):
                        logger.error(f"Validation failed: List element {i} has incorrect shape/type: "
                                   f"{type(arr)}, {getattr(arr, 'shape', 'N/A')}. "
                                   f"Expected ({n_samples_expected}, {n_features_expected}).")
                        all_valid_structure = False
                        break
                        
            if not all_valid_structure:
                raise ValueError(f"Validation failed after attempting format conversion for {task_name}.")

            all_shap_values_combined[task_name] = shap_values_list_np
            logger.info(f"SHAP values calculated and formatted correctly for {task_name}.")

        except Exception as e_shap:
            logger.error(f"SHAP calc failed for {task_name}: {e_shap}", exc_info=True)
            all_shap_values_combined[task_name] = None
            continue

        end_task_time = time.time()
        logger.info(f"--- SHAP Task {task_name} finished. "
                   f"Duration: {end_task_time - start_task_time:.2f} sec ---")

    logger.info("--- SHAP Value Calculation Finished ---")
    return all_shap_values_combined, instance_spec.cpu(), instance_metab.cpu()


# ===== BASIC SHAP PLOTTING & SAVING =====
# (Same as before - returns dict of DataFrames)
def plot_and_save_basic_shap(all_shap_values_combined, instance_spec_cpu, instance_metab_cpu,
                             spectral_features, metabolite_features, target_cols,
                             pairing, config):
    logger.info(f"--- Generating Basic SHAP Plots & Saving Importance ({pairing}) ---")
    data_dir = config['SHAP_DATA_DIR']; plot_dir = config['SHAP_PLOT_DIR']; max_display = config['SHAP_MAX_DISPLAY']
    all_feature_names = spectral_features + metabolite_features
    num_spectral_features = len(spectral_features); num_metabolite_features = len(metabolite_features); num_total_features = len(all_feature_names)

    instance_spec_np = instance_spec_cpu.numpy() if isinstance(instance_spec_cpu, torch.Tensor) else instance_spec_cpu
    instance_metab_np = instance_metab_cpu.numpy() if isinstance(instance_metab_cpu, torch.Tensor) else instance_metab_cpu
    try: instance_features_combined_np = np.hstack((instance_spec_np, instance_metab_np)); instance_df = pd.DataFrame(instance_features_combined_np, columns=all_feature_names); num_instances_data = instance_df.shape[0]
    except ValueError as e: logger.error(f"Error creating instance DataFrame for {pairing}: {e}", exc_info=True); return {}

    aggregated_importance_dict = {}
    for task_name in target_cols:
        logger.info(f"Processing SHAP results for Task: {task_name}")
        if task_name not in all_shap_values_combined or all_shap_values_combined[task_name] is None: logger.warning(f"No valid SHAP values found for task {task_name}. Skipping."); continue
        shap_values_list = all_shap_values_combined[task_name]
        num_classes = len(shap_values_list)
        valid_structure = isinstance(shap_values_list, list) and num_classes > 0 and all(isinstance(a, np.ndarray) and len(a.shape) == 2 and a.shape[0] == num_instances_data and a.shape[1] == num_total_features for a in shap_values_list)
        if not valid_structure: logger.error(f"Invalid SHAP structure for {task_name}. Skipping."); continue

        plot_class_names = [f'Class {i}' for i in range(num_classes)]
        try:
            plt.figure(); shap.summary_plot(shap_values_list, instance_df, plot_type="bar", feature_names=all_feature_names, max_display=max_display, show=False, class_names=plot_class_names)
            plt.title(f'SHAP Importance (Bar) - {pairing} - Task: {task_name}'); plot_filename_bar = os.path.join(plot_dir, f"shap_summary_bar_{pairing}_{task_name}.png")
            plt.tight_layout(); plt.savefig(plot_filename_bar, dpi=config['FIG_DPI'], bbox_inches='tight'); plt.close(); logger.info(f"Saved SHAP Bar plot: {plot_filename_bar}")
        except Exception as e_plot_bar: logger.error(f"Error SHAP Bar plot {task_name}: {e_plot_bar}", exc_info=True); plt.close()

        try:
            all_classes_shap_np = np.array(shap_values_list); mean_abs_shap = np.mean(np.abs(all_classes_shap_np), axis=(0, 1))
            importance_df = pd.DataFrame({'Feature': all_feature_names, 'MeanAbsoluteShap': mean_abs_shap})
            feature_types = ['Spectral'] * num_spectral_features + ['Metabolite'] * num_metabolite_features
            if len(feature_types) == len(importance_df): importance_df['FeatureType'] = feature_types
            else: logger.warning(f"Feature type list length mismatch. Skipping adding FeatureType.")
            importance_df = importance_df.sort_values(by='MeanAbsoluteShap', ascending=False).reset_index(drop=True)
            importance_df['Task'] = task_name; importance_df['Pairing'] = pairing
            aggregated_importance_dict[task_name] = importance_df
            csv_filename = os.path.join(data_dir, f"shap_importance_{pairing}_{task_name}.csv")
            importance_df.to_csv(csv_filename, index=False); logger.info(f"Saved aggregated SHAP importance: {csv_filename}")
        except Exception as e_agg: logger.error(f"Error aggregating/saving SHAP importance {task_name}: {e_agg}", exc_info=True)

    logger.info(f"--- Basic SHAP Plotting and Saving Finished ({pairing}) ---")
    return aggregated_importance_dict


# ===== ADVANCED VISUALIZATION FUNCTIONS (Integrated) =====

# --- Helper for Advanced Plots ---
def truncate_feature_names(feature_names, max_length=FEATURE_NAME_MAX_LENGTH):
    """Truncate feature names for plotting."""
    return [name[:max_length-3] + '...' if len(name) > max_length else name for name in feature_names]

# --- SHAP Clustermap ---
def plot_shap_clustermap(shap_data: pd.DataFrame, pairing: str, config: dict):
    # (Same as version 1.1.0)
    logger.info(f"--- Generating SHAP Clustermap for {pairing} ---")
    output_dir = config['SHAP_PLOT_DIR']
    top_n = config['TOP_N_FEATURES_HEATMAP']
    cmap = config['CLUSTERMAP_CMAP']
    figsize = config['FIG_SIZE_SQUARE']

    if shap_data is None or shap_data.empty: logger.warning(f"No SHAP data provided for {pairing}, skipping clustermap."); return

    try:
        if 'Task' not in shap_data.columns: logger.error("Combined SHAP data missing 'Task' column. Cannot create clustermap."); return
        if 'FeatureType' not in shap_data.columns: logger.warning("Combined SHAP data missing 'FeatureType'. Row colors missing."); shap_data['FeatureType'] = 'Unknown'

        idx_max = shap_data.groupby('Feature')['MeanAbsoluteShap'].idxmax()
        top_features_df = shap_data.loc[idx_max]
        top_features = top_features_df.nlargest(top_n, 'MeanAbsoluteShap')['Feature'].tolist()
        if not top_features: logger.warning(f"Could not determine top {top_n} features for {pairing}."); return
        logger.info(f"Plotting top {len(top_features)} features based on max SHAP value across tasks.")

        plot_data = shap_data[shap_data['Feature'].isin(top_features)]
        pivot_table = plot_data.pivot_table(index='Feature', columns='Task', values='MeanAbsoluteShap', fill_value=0)

        feature_types = plot_data[['Feature', 'FeatureType']].drop_duplicates().set_index('Feature')
        feature_types = feature_types.reindex(pivot_table.index)
        row_colors = feature_types['FeatureType'].map(config['OMICS_PALETTE']).fillna('grey').rename('Omics Type')

        g = sns.clustermap(pivot_table, cmap=cmap, figsize=figsize,
                           row_colors=row_colors if not row_colors.empty else None,
                           linewidths=0.5, linecolor='lightgray', dendrogram_ratio=(.2, .1),
                           cbar_pos=(0.02, 0.8, 0.03, 0.18), z_score=0, annot=False)

        title_text = f'Top {len(top_features)} Feature SHAP Importance ({pairing})'
        g.ax_heatmap.set_title(title_text, fontsize=config['FONT_SIZE_TITLE'], pad=20)
        g.ax_heatmap.set_xlabel("Prediction Task", fontsize=config['FONT_SIZE_LABEL'])
        g.ax_heatmap.set_ylabel("Feature", fontsize=config['FONT_SIZE_LABEL'])
        g.ax_heatmap.tick_params(axis='x', labelsize=config['FONT_SIZE_TICK'])
        g.ax_heatmap.tick_params(axis='y', labelsize=max(6, config['FONT_SIZE_TICK'] - 2))

        try:
            handles = [plt.Rectangle((0, 0), 1, 1, color=color) for color in config['OMICS_PALETTE'].values() if color != 'grey'] # Exclude grey if added
            labels = [k for k,v in config['OMICS_PALETTE'].items() if v != 'grey']
            if 'Unknown' in feature_types['FeatureType'].unique(): # Add Unknown if present
                 handles.append(plt.Rectangle((0,0),1,1, color='grey'))
                 labels.append('Unknown')
            if handles: # Only add legend if there are omics types
                 g.fig.legend(handles=handles, labels=labels, title='Omics Type',
                              bbox_to_anchor=(0.02, 0.02), loc='lower left', frameon=False)
        except Exception as e_legend: logger.warning(f"Could not create/position row color legend: {e_legend}")

        plt.setp(g.ax_heatmap.get_xticklabels(), rotation=0)
        fpath = os.path.join(output_dir, f"shap_clustermap_{pairing}_top{top_n}.png")
        plt.savefig(fpath, dpi=config['FIG_DPI'], bbox_inches='tight'); plt.close(g.fig)
        logger.info(f"Saved SHAP clustermap: {fpath}")
    except Exception as e: logger.error(f"Error generating SHAP clustermap for {pairing}: {e}", exc_info=True); plt.close()


# --- Omics Contribution Stacked Bar ---
def plot_omics_contribution_stacked_bar(shap_data: pd.DataFrame, pairing: str, config: dict):
    """Generate stacked bar plot showing contribution of each omics type to prediction tasks."""
    logger.info(f"--- Generating Omics Contribution Stacked Bar Plot for {pairing} ---")
    
    output_dir = config['SHAP_PLOT_DIR']
    palette = config['OMICS_PALETTE'].copy()  # Use a copy to potentially add 'Unknown'

    if shap_data is None or shap_data.empty:
        logger.warning(f"No SHAP data provided for {pairing}, skipping contribution plot.")
        return

    try:
        # Validate required columns
        if not all(c in shap_data.columns for c in ['Task', 'FeatureType', 'MeanAbsoluteShap']):
            logger.error("Missing required columns (Task, FeatureType, MeanAbsoluteShap). "
                        "Cannot create contribution plot.")
            return

        # Handle unknown feature types
        shap_data['FeatureType'] = shap_data['FeatureType'].fillna('Unknown')
        if 'Unknown' in shap_data['FeatureType'].unique() and 'Unknown' not in palette:
            palette['Unknown'] = 'grey'

        # Calculate contributions by omics type for each task
        contribution = shap_data.groupby(['Task', 'FeatureType'])['MeanAbsoluteShap'].sum().reset_index()
        total_shap_per_task = contribution.groupby('Task')['MeanAbsoluteShap'].sum().reset_index()
        total_shap_per_task = total_shap_per_task.rename(columns={'MeanAbsoluteShap': 'TotalShap'})
        
        contribution = pd.merge(contribution, total_shap_per_task, on='Task')
        contribution['Proportion'] = 0.0
        non_zero_mask = contribution['TotalShap'] != 0
        contribution.loc[non_zero_mask, 'Proportion'] = (
            contribution.loc[non_zero_mask, 'MeanAbsoluteShap'] / 
            contribution.loc[non_zero_mask, 'TotalShap']
        ) * 100

        # Create pivot table for plotting
        pivot_prop = contribution.pivot(
            index='Task',
            columns='FeatureType',
            values='Proportion'
        ).fillna(0)
        
        # Select columns in order matching palette
        plot_order = [ft for ft in palette.keys() if ft in pivot_prop.columns]
        pivot_prop = pivot_prop[plot_order]

        # Create stacked bar plot
        fig, ax = plt.subplots(figsize=config['FIG_SIZE_WIDE'])
        pivot_prop.plot(
            kind='bar',
            stacked=True,
            color=[palette[col] for col in pivot_prop.columns],
            ax=ax,
            width=0.8
        )

        # Set plot labels and appearance
        ax.set_xlabel("Prediction Task", fontsize=config['FONT_SIZE_LABEL'])
        ax.set_ylabel("Proportion of Total SHAP Importance (%)", fontsize=config['FONT_SIZE_LABEL'])
        ax.set_title(f"Relative Contribution of Omics Types to Task Prediction ({pairing})",
                    fontsize=config['FONT_SIZE_TITLE'])
        ax.tick_params(axis='x', labelsize=config['FONT_SIZE_TICK'], rotation=0)
        ax.tick_params(axis='y', labelsize=config['FONT_SIZE_TICK'])
        ax.legend(title='Omics Type', bbox_to_anchor=(1.02, 0), loc='lower left')
        ax.set_ylim(0, 100)
        
        plt.tight_layout(rect=[0, 0, 0.88, 1])

        # Save plot
        fpath = os.path.join(output_dir, f"shap_omics_contribution_stackedbar_{pairing}.png")
        plt.savefig(fpath, dpi=config['FIG_DPI'], bbox_inches='tight')
        plt.close(fig)
        logger.info(f"Saved Omics Contribution plot: {fpath}")
        
    except Exception as e:
        logger.error(f"Error generating Omics Contribution plot for {pairing}: {e}",
                    exc_info=True)
        plt.close()


# --- Faceted Top Features Bar Plot ---
def plot_faceted_top_features(shap_data: pd.DataFrame, pairing: str, config: dict):
    """
    Creates a faceted bar plot showing the top M features for each task side by side.
    """
    logger.info(f"--- Generating Faceted Top Features Plot for {pairing} ---")
    
    output_dir = config['SHAP_PLOT_DIR']
    top_m = config['TOP_M_FEATURES_PER_TASK']
    palette = config['OMICS_PALETTE'].copy()
    figsize = config['FIG_SIZE_TALL']  # Use tall figure size

    if shap_data is None or shap_data.empty:
        logger.warning(f"No SHAP data provided for {pairing}, skipping faceted plot.")
        return

    try:
        # Validate required columns
        if not all(c in shap_data.columns for c in ['Task', 'Feature', 'MeanAbsoluteShap', 'FeatureType']):
            logger.error("Missing required columns for faceted plot. Skipping.")
            return

        # Handle unknown feature types
        shap_data['FeatureType'] = shap_data['FeatureType'].fillna('Unknown')
        if 'Unknown' in shap_data['FeatureType'].unique() and 'Unknown' not in palette:
            palette['Unknown'] = 'grey'

        # Get top M features *per task*
        task_top_features_list = []
        unique_tasks = shap_data['Task'].unique()
        
        for task in unique_tasks:
            task_data = shap_data[shap_data['Task'] == task]
            if not task_data.empty:
                top_m_df = task_data.nlargest(top_m, 'MeanAbsoluteShap')
                task_top_features_list.append(top_m_df)

        if not task_top_features_list:
            logger.warning(f"No top features found for any task in {pairing}. Skipping faceted plot.")
            return

        # Combine all task-specific top features
        top_features_df = pd.concat(task_top_features_list, ignore_index=True)

        # Create the faceted plot using catplot
        g = sns.catplot(
            data=top_features_df,
            x='MeanAbsoluteShap',
            y='Feature',
            hue='FeatureType',
            col='Task',
            kind='bar',
            height=figsize[1] / len(unique_tasks) * 1.5,  # Adjust height based on number of tasks
            aspect=1.0,  # Adjust aspect ratio
            palette=palette,
            sharey=False,  # Each facet has its own y-axis
            legend=False,  # Turn off automatic legend, add manually later
            # Attempt to order y-axis globally (might not work perfectly with sharey=False)
            order=top_features_df.sort_values('MeanAbsoluteShap', ascending=False)['Feature'].tolist()
        )

        # Adjust plot appearance
        g.set_titles(col_template="{col_name}", size=config['FONT_SIZE_LABEL'])
        g.set_axis_labels("Mean |SHAP value|", "Feature")
        # Adjust title position
        g.fig.suptitle(f'Top {top_m} Features per Task ({pairing})',
                      fontsize=config['FONT_SIZE_TITLE'], y=1.03)

        # Truncate long feature names on y-axis for each facet
        for ax in g.axes.flat:
            labels = ax.get_yticklabels()
            truncated_labels = truncate_feature_names(
                [label.get_text() for label in labels],
                config['FEATURE_NAME_MAX_LENGTH']
            )
            ax.set_yticklabels(truncated_labels, fontsize=config['FONT_SIZE_TICK'])
            ax.tick_params(axis='x', labelsize=config['FONT_SIZE_TICK'])

        # Add a single legend
        handles = [plt.Rectangle((0, 0), 1, 1, color=color) 
                  for color in palette.values() if color != 'grey']
        labels = [k for k, v in palette.items() if v != 'grey']
        
        if 'Unknown' in top_features_df['FeatureType'].unique():
            handles.append(plt.Rectangle((0, 0), 1, 1, color='grey'))
            labels.append('Unknown')
            
        if handles:
            g.fig.legend(
                handles=handles,
                labels=labels,
                title='Omics Type',
                bbox_to_anchor=(1.05, 0),
                loc='lower left',
                frameon=False
            )

        # Adjust layout
        plt.tight_layout(rect=[0, 0, 0.9, 1])  # Make space for legend

        # Save plot
        fpath = os.path.join(output_dir, f"shap_faceted_top_features_{pairing}_top{top_m}.png")
        plt.savefig(fpath, dpi=config['FIG_DPI'], bbox_inches='tight')
        plt.close(g.fig)
        logger.info(f"Saved faceted top features plot: {fpath}")

    except Exception as e:
        logger.error(f"Error creating faceted top features plot for {pairing}: {e}", 
                    exc_info=True)
        plt.close()


# ===== MAIN EXECUTION =====
def main():
    """Main execution function for SHAP analysis and integrated visualization."""
    main_start_time = time.time()
    logger.info(f"--- Starting Main Execution ({SCRIPT_NAME} v{VERSION}) ---")

    # Define pairings to process
    pairings_to_process = ["Leaf", "Root"]

    # Loop through analysis pairings
    for pairing in pairings_to_process:
        logger.info(f"\n===== Processing SHAP Analysis & Plots for Pairing: {pairing} =====")
        pairing_start_time = time.time()
        aggregated_importance_all_tasks = {}  # To store results for advanced plots

        try:
            # 1. Load and preprocess data
            (X_train_spec, X_train_metab, X_test_spec, X_test_metab,
             spectral_features, metabolite_features) = load_and_preprocess_for_shap(
                 config=globals(), pairing=pairing
             )
            spectral_dim = len(spectral_features)
            metabolite_dim = len(metabolite_features)
            logger.info(f"Data loaded: Spec Dim={spectral_dim}, Metab Dim={metabolite_dim}")

            # 2. Load the trained model
            model_path = MODEL_PATHS.get(pairing)
            if not model_path or not os.path.exists(model_path):
                logger.error(f"Model path invalid/not found for {pairing}: {model_path}. Skipping.")
                continue
                
            model = load_trained_model(
                model_path, spectral_dim, metabolite_dim, config=globals(), device=DEVICE
            )

            # 3. Calculate SHAP values
            all_shap_values, instance_spec_cpu, instance_metab_cpu = calculate_shap_values(
                model, X_train_spec, X_train_metab, X_test_spec, X_test_metab,
                spectral_features, metabolite_features,
                TARGET_COLS, config=globals(), device=DEVICE
            )

            # 4. Plot BASIC SHAP summaries and save/collect aggregated importance CSVs
            aggregated_importance_all_tasks = plot_and_save_basic_shap(
                all_shap_values, instance_spec_cpu, instance_metab_cpu,
                spectral_features, metabolite_features, TARGET_COLS,
                pairing, config=globals()
            )

            # 5. Generate ADVANCED Visualizations if importance data was generated
            if aggregated_importance_all_tasks:
                logger.info(f"--- Generating Advanced Visualizations for {pairing} ---")
                try:
                    # Combine the task-specific importance dataframes into one
                    combined_shap_df = pd.concat(
                        aggregated_importance_all_tasks.values(), ignore_index=True
                    )

                    # Validate combined dataframe has required columns
                    req_cols = ['Feature', 'MeanAbsoluteShap', 'FeatureType', 'Task', 'Pairing']
                    if not all(col in combined_shap_df.columns for col in req_cols):
                        logger.error("Combined DataFrame is missing essential columns. "
                                    "Skipping advanced plots.")
                    else:
                        # Call advanced plotting functions
                        plot_shap_clustermap(combined_shap_df, pairing, config=globals())
                        plot_omics_contribution_stacked_bar(combined_shap_df, pairing, config=globals())
                        plot_faceted_top_features(combined_shap_df, pairing, config=globals())

                except Exception as e_combine:
                    logger.error(f"Error combining importance data or generating advanced plots "
                                f"for {pairing}: {e_combine}", exc_info=True)
            else:
                logger.warning(f"Skipping advanced visualizations for {pairing} as no "
                             f"aggregated importance data was generated.")

        except FileNotFoundError as e:
            logger.error(f"Skipping analysis for {pairing} due to missing file: {e}")
        except ValueError as e:
            logger.error(f"Skipping analysis for {pairing} due to data/value error: {e}")
        except Exception as e:
            logger.error(f"Error during analysis phase for {pairing}: {e}", exc_info=True)

        pairing_end_time = time.time()
        logger.info(f"===== Finished Analysis & Plots for {pairing}. "
                   f"Duration: {(pairing_end_time - pairing_start_time):.2f} seconds =====")

    main_end_time = time.time()
    total_duration_min = (main_end_time - main_start_time) / 60
    logger.info(f"--- Main Execution Finished --- "
               f"Total Duration: {total_duration_min:.2f} minutes ---")
    logger.info(f"Outputs saved in: {SHAP_OUTPUT_DIR}")
    logger.info("="*60)


# --- Entry Point ---
if __name__ == '__main__':
    try:
        main()
    except Exception as e:
        logger.critical(f"A critical error occurred during the main execution workflow: {e}",
                      exc_info=True)
        sys.exit(1)

--------------------------------------------------

plot_transformer_attention.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Multi-Wavelength Analysis for Plant Stress Response
==================================================

This script organizes and visualizes spectral-metabolite interactions
across different wavelengths. It creates multi-wavelength comparison
figures by working with existing plot files to present a cohesive
analysis of plant stress responses.

The script performs the following tasks:
1. Locates existing visualization files in the specified directory
2. Extracts and categorizes wavelength information from filenames
3. Creates multi-panel figures grouped by tissue and wavelength
4. Generates summary visualizations of attention patterns
"""

import os
import sys
import re
import glob
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from matplotlib.gridspec import GridSpec
import shutil
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# ===== CONFIGURATION =====
# Base directory and output directory
BASE_DIR = r"C:/Users/ms/Desktop/hyper/output/transformer/v3_feature_attention"
OUTPUT_DIR = r"C:/Users/ms/Desktop/hyper/output/transformer/v3_feature_attention/plots_attention_advanced"

# Create output directory if it doesn't exist
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Define physiologically relevant wavelength categories
WAVELENGTH_CATEGORIES = {
    'Water Bands': [1450, 1940, 970, 1200],           # Water absorption bands
    'Pigment Bands': [550, 660, 680, 700, 710],       # Chlorophyll and other pigments
    'NIR Structure': [850, 900, 1000, 1100, 1300],    # Cell structure
    'SWIR Features': [1600, 1700, 2000, 2200, 2300]   # Biochemical features
}

# ===== HELPER FUNCTIONS =====

def find_existing_plots(base_dir, pattern="*.png"):
    """Find all existing plot files in the directory tree.
    
    Args:
        base_dir: The root directory to search
        pattern: File pattern to match (default: "*.png")
        
    Returns:
        List of file paths
    """
    all_plots = []
    for root, dirs, files in os.walk(base_dir):
        for file in glob.glob(os.path.join(root, pattern)):
            all_plots.append(file)
    return all_plots

def extract_wavelength_from_filename(filename):
    """Extract wavelength information from a filename.
    
    Args:
        filename: The filename to parse
        
    Returns:
        Integer wavelength value or None if not found
    """
    # Patterns to look for
    patterns = [
        r'W_(\d+)',                          # W_550
        r'(\d+)nm',                          # 550nm
        r'wavelength[_-](\d+)',              # wavelength_550
        r'wl[_-](\d+)'                       # wl_550
    ]
    
    for pattern in patterns:
        match = re.search(pattern, os.path.basename(filename), re.IGNORECASE)
        if match:
            try:
                return int(match.group(1))
            except ValueError:
                pass
    
    return None

def categorize_wavelength(wavelength):
    """Categorize a wavelength into a physiological category.
    
    Args:
        wavelength: Wavelength value in nm
        
    Returns:
        String category name
    """
    if wavelength is None:
        return "Unknown"
        
    for category, wave_list in WAVELENGTH_CATEGORIES.items():
        # Find the closest wavelength in the category
        closest = min(wave_list, key=lambda x: abs(x - wavelength))
        if abs(closest - wavelength) <= 50:  # Within 50nm is close enough
            return category
    
    return "Other"

def extract_tissue_from_filename(filename):
    """Extract tissue information from a filename.
    
    Args:
        filename: The filename to parse
        
    Returns:
        String tissue name or None if not found
    """
    basename = os.path.basename(filename).lower()
    if "leaf" in basename:
        return "Leaf"
    elif "root" in basename:
        return "Root"
    else:
        return None

def find_comprehensive_temporal_plots(all_plots):
    """Find the comprehensive temporal plots with 4-panel visualizations.
    
    Args:
        all_plots: List of plot file paths
        
    Returns:
        List of dictionaries with plot metadata
    """
    # Look for plots that match the pattern of comprehensive analyses
    comp_plots = []
    
    patterns = [
        r'comprehensive.*analysis',
        r'temporal.*analysis',
        r'fig.*(\d+).*temporal', 
        r'fig.*(\d+).*attention',
        r'attention.*temporal',
        r'metabolome.*temporal'
    ]
    
    for plot in all_plots:
        basename = os.path.basename(plot).lower()
        for pattern in patterns:
            if re.search(pattern, basename):
                # Extract wavelength if possible
                wavelength = extract_wavelength_from_filename(plot)
                tissue = extract_tissue_from_filename(plot)
                
                if wavelength or tissue:
                    comp_plots.append({
                        'path': plot,
                        'wavelength': wavelength,
                        'wavelength_category': categorize_wavelength(wavelength),
                        'tissue': tissue
                    })
                    break
    
    return comp_plots

def find_attention_plots(all_plots):
    """Find plots related to attention between features.
    
    Args:
        all_plots: List of plot file paths
        
    Returns:
        List of dictionaries with plot metadata
    """
    attention_plots = []
    
    patterns = [
        r'attention.*heatmap',
        r'attention.*network',
        r'cross.*modal',
        r'feature.*attention',
        r'temporal.*profile'
    ]
    
    for plot in all_plots:
        basename = os.path.basename(plot).lower()
        for pattern in patterns:
            if re.search(pattern, basename):
                # Extract wavelength if possible
                wavelength = extract_wavelength_from_filename(plot)
                tissue = extract_tissue_from_filename(plot)
                
                attention_plots.append({
                    'path': plot,
                    'wavelength': wavelength,
                    'wavelength_category': categorize_wavelength(wavelength),
                    'tissue': tissue
                })
                break
    
    return attention_plots

def create_multi_wavelength_figure(comp_plots, output_dir):
    """Create a multi-panel figure showing multiple wavelengths and tissues.
    
    Args:
        comp_plots: List of dictionaries with plot metadata
        output_dir: Directory to save output figures
    """
    # Group plots by tissue
    tissue_plots = defaultdict(list)
    for plot in comp_plots:
        if plot['tissue']:
            tissue_plots[plot['tissue']].append(plot)
    
    # Sort each tissue's plots by wavelength category for biological relevance
    category_order = [
        'Water Bands', 'Pigment Bands', 'NIR Structure', 
        'SWIR Features', 'Other', 'Unknown'
    ]
    
    for tissue, plots in tissue_plots.items():
        # Sort plots by category order
        plots.sort(key=lambda x: (
            category_order.index(x['wavelength_category']) 
            if x['wavelength_category'] in category_order else 999,
            x['wavelength'] if x['wavelength'] else 9999
        ))
        
        # Take up to 3 most relevant plots for each tissue
        selected_plots = plots[:3]
        
        if len(selected_plots) == 0:
            continue
            
        try:
            # Create a multi-panel figure
            fig = plt.figure(figsize=(16, 10 * len(selected_plots)))
            gs = GridSpec(len(selected_plots), 1, figure=fig)
            
            # Add each plot as a panel
            for i, plot_info in enumerate(selected_plots):
                # Load the image
                img = mpimg.imread(plot_info['path'])
                
                # Create subplot
                ax = fig.add_subplot(gs[i, 0])
                
                # Display the image
                ax.imshow(img)
                ax.axis('off')
                
                # Add panel label
                wavelength_str = (f"W_{plot_info['wavelength']}" 
                                 if plot_info['wavelength'] else "Unknown")
                category_str = (f" ({plot_info['wavelength_category']})" 
                               if plot_info['wavelength_category'] != "Unknown" else "")
                ax.set_title(f"{chr(65+i)}) {wavelength_str}{category_str}", 
                             fontsize=14, loc='left')
            
            # Add figure title
            fig.suptitle(
                f"Figure 12: Multi-Wavelength Temporal Dynamics in {tissue} Tissue", 
                fontsize=16, y=0.99
            )
            
            # Add caption
            caption = (
                f"Figure 12. Comprehensive analysis of wavelength-metabolite "
                f"interactions across time in {tissue} tissue. "
                "Each panel shows a different physiologically relevant wavelength "
                "with four sub-panels: "
                "(A) Heatmaps comparing temporal attention patterns between genotypes, "
                "(B) Differential attention heatmap (G1-G2) highlighting metabolites "
                "with genotype-specific temporal patterns, "
                "(C) Temporal profiles for key metabolites, showing coordination "
                "patterns over time, "
                "(D) Day-specific comparisons revealing the evolution of "
                "differential attention. "
                "This multi-wavelength perspective reveals both conserved response "
                "mechanisms across spectral regions "
                "and wavelength-specific interactions associated with different "
                "physiological processes."
            )
            
            # Add the caption at the bottom
            fig.text(0.5, 0.01, caption, wrap=True, 
                     horizontalalignment='center', fontsize=12)
            
            # Adjust layout and save
            plt.tight_layout(rect=[0, 0.03, 1, 0.98])
            
            # Save figure
            output_path = os.path.join(
                output_dir, f"Figure12_Multi_Wavelength_Temporal_{tissue}.png"
            )
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"Created multi-wavelength figure for {tissue} tissue: {output_path}")
            
            # Also copy the individual plots to the output directory for reference
            for plot_info in selected_plots:
                basename = os.path.basename(plot_info['path'])
                shutil.copy2(plot_info['path'], os.path.join(output_dir, basename))
                print(f"Copied {basename} to output directory")
                
        except Exception as e:
            print(f"Error creating multi-wavelength figure for {tissue}: {e}")
            import traceback
            traceback.print_exc()
            plt.close('all')

def create_attention_summary_figure(attention_plots, output_dir):
    """Create a summary figure of key attention visualizations.
    
    Args:
        attention_plots: List of dictionaries with plot metadata
        output_dir: Directory to save output figures
    """
    # Filter for the most informative attention plots
    selected_plots = []
    
    # Selection criteria - prioritize certain types
    priority_patterns = [
        (r'network.*comparison', 'Network Comparison'),
        (r'attention.*heatmap', 'Attention Heatmap'),
        (r'temporal.*profile', 'Temporal Profile'),
        (r'cross.*modal', 'Cross-Modal Analysis')
    ]
    
    # Match plots to priority patterns
    plot_matches = []
    for plot in attention_plots:
        basename = os.path.basename(plot['path']).lower()
        for pattern, label in priority_patterns:
            if re.search(pattern, basename):
                plot_matches.append(
                    (plot, label, priority_patterns.index((pattern, label)))
                )
                break
    
    # Sort by priority
    plot_matches.sort(key=lambda x: x[2])
    
    # Take up to 4 plots, prioritizing different types and tissues
    selected_types = set()
    selected_tissues = set()
    
    for plot, label, _ in plot_matches:
        # Ensure diversity by selecting different types and tissues
        if len(selected_plots) >= 4:
            break
            
        if label not in selected_types or plot['tissue'] not in selected_tissues:
            selected_plots.append((plot, label))
            selected_types.add(label)
            if plot['tissue']:
                selected_tissues.add(plot['tissue'])
    
    if len(selected_plots) == 0:
        print("No suitable attention plots found for summary figure")
        return
        
    try:
        # Create a 2x2 grid figure
        fig, axes = plt.subplots(2, 2, figsize=(16, 16))
        axes = axes.flatten()
        
        # Add each plot to a panel
        for i, (plot_info, label) in enumerate(selected_plots):
            if i >= len(axes):
                break
                
            # Load the image
            img = mpimg.imread(plot_info['path'])
            
            # Display the image
            axes[i].imshow(img)
            axes[i].axis('off')
            
            # Add panel label
            tissue_str = f" ({plot_info['tissue']})" if plot_info['tissue'] else ""
            axes[i].set_title(f"{chr(65+i)}) {label}{tissue_str}", 
                              fontsize=14, loc='left')
        
        # Hide any unused axes
        for i in range(len(selected_plots), len(axes)):
            axes[i].axis('off')
            axes[i].set_visible(False)
        
        # Add figure title
        fig.suptitle("Summary of Cross-Modal Attention Analyses", 
                     fontsize=16, y=0.98)
        
        # Add caption (generic)
        caption = (
            "Summary of key visualizations from the cross-modal attention analysis. "
            "These visualizations highlight the interaction between spectral and "
            "metabolite features across different tissues, genotypes, and time points, "
            "revealing the underlying mechanisms of plant stress response coordination."
        )
        
        # Add the caption at the bottom
        fig.text(0.5, 0.02, caption, wrap=True, 
                 horizontalalignment='center', fontsize=12)
        
        # Adjust layout and save
        plt.tight_layout(rect=[0, 0.03, 1, 0.97])
        
        # Save figure
        output_path = os.path.join(output_dir, "Attention_Analysis_Summary.png")
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"Created attention summary figure: {output_path}")
        
    except Exception as e:
        print(f"Error creating attention summary figure: {e}")
        import traceback
        traceback.print_exc()
        plt.close('all')

# ===== MAIN FUNCTION =====

def main():
    """Main execution function."""
    print("Starting improved multi-wavelength analysis...")
    
    # Find all existing plot files
    all_plots = find_existing_plots(BASE_DIR)
    print(f"Found {len(all_plots)} plot files in the directory tree")
    
    # Find plots that have comprehensive temporal analysis
    comp_plots = find_comprehensive_temporal_plots(all_plots)
    print(f"Found {len(comp_plots)} comprehensive temporal analysis plots")
    
    # Print what we found for debugging
    for i, plot in enumerate(comp_plots[:10]):  # Print first 10 for brevity
        print(f"  {i+1}. {os.path.basename(plot['path'])}")
        print(f"     Wavelength: {plot['wavelength']}, Category: "
              f"{plot['wavelength_category']}, Tissue: {plot['tissue']}")
    
    # Find attention-related plots
    attention_plots = find_attention_plots(all_plots)
    print(f"Found {len(attention_plots)} attention-related plots")
    
    # Create multi-wavelength figure
    if comp_plots:
        create_multi_wavelength_figure(comp_plots, OUTPUT_DIR)
    else:
        print("No comprehensive temporal plots found to create multi-wavelength figure")
    
    # Create attention summary figure
    if attention_plots:
        create_attention_summary_figure(attention_plots, OUTPUT_DIR)
    else:
        print("No attention plots found to create summary figure")
    
    print("\nImproved multi-wavelength analysis completed!")
    
    # If we didn't find any plots, also check for and copy specific plots
    example_filenames = ["W_550", "W_1047", "1450nm", "660nm", "550nm", "970nm"]
    found_examples = []
    
    for example in example_filenames:
        for plot in all_plots:
            if example in os.path.basename(plot):
                found_examples.append(plot)
                dest_path = os.path.join(OUTPUT_DIR, os.path.basename(plot))
                try:
                    shutil.copy2(plot, dest_path)
                    print(f"Copied example plot: {os.path.basename(plot)}")
                except Exception as e:
                    print(f"Error copying {plot}: {e}")
    
    if not found_examples:
        print("\nNo example plots found to copy directly.")
        
        # Create a placeholder to explain how to use the existing plots
        try:
            fig, ax = plt.subplots(1, 1, figsize=(10, 6))
            ax.text(
                0.5, 0.5, 
                "No existing plots could be found for multi-wavelength analysis.\n\n"
                "To create Figure 12, combine the plots you already have into a "
                "multi-panel figure. These plots already contain the 4-panel layout "
                "with:\n"
                "- Genotype-specific attention heatmaps\n"
                "- Differential (G1-G2) heatmaps\n"
                "- Temporal profiles\n"
                "- G1 vs G2 scatter comparisons",
                ha='center', va='center', fontsize=14, wrap=True
            )
            ax.axis('off')
            plt.savefig(
                os.path.join(OUTPUT_DIR, "Figure12_Instructions.png"), 
                dpi=300, bbox_inches='tight'
            )
            plt.close()
        except Exception as e:
            print(f"Error creating instruction image: {e}")

if __name__ == "__main__":
    main()

--------------------------------------------------

process_attention_data.py
# -*- coding: utf-8 -*-
"""
Process Raw Attention Data (v3.0)

This script processes feature-level attention data from transformer models
analyzing multi-omics datasets. It handles both spectral-to-metabolite (S->M)
and metabolite-to-spectral (M->S) attention tensors.

Main functions:
1. Loads raw 4D attention tensors from HDF5 files and metadata from Feather/CSV
2. Validates data alignment
3. Calculates view-level statistics (Mean, StdDev, P95)
4. Processes feature-pair attention scores
5. Generates conditional attention metrics grouped by metadata fields
6. Exports all data to CSV files for further analysis

Input: HDF5 files containing attention tensors and feature names
Output: CSV files with processed attention statistics and feature pairs
"""

# ===== IMPORTS =====
import os
import sys
import time
import logging
import traceback
from datetime import datetime
import pandas as pd
import numpy as np
import h5py
try:
    # Try importing pyarrow for Feather file support
    import pyarrow
    # Check if necessary modules are available
    if not hasattr(pd, 'read_feather'):
        pyarrow = None # Disable if pandas integration is missing
        print("WARNING: Pandas Feather support not fully available. Will attempt .csv fallback.")
except ImportError:
    pyarrow = None
    print("WARNING: `pyarrow` library not found. Will not be able to read .feather files. Will attempt .csv fallback.")

# Import SciPy conditionally for percentile calculation
try:
    from scipy.stats import percentileofscore # For potential future use
except ImportError:
    print("WARNING: `scipy` not found. Percentile calculations might rely solely on NumPy.")
    # NumPy's percentile function is sufficient here, so no critical failure.

# ===== CONFIGURATION =====

# --- Script Info ---
SCRIPT_NAME = "process_attention_data_3"
VERSION = "3.0.0"

# --- Paths ---
BASE_OUTPUT_DIR = r"C:/Users/ms/Desktop/hyper/output/transformer/v3_feature_attention"

# Input HDF5 files
HDF5_PATHS = {
    "Leaf": os.path.join(BASE_OUTPUT_DIR, r"leaf/results/raw_attention_data_Leaf.h5"),
    "Root": os.path.join(BASE_OUTPUT_DIR, r"root/results/raw_attention_data_Root.h5")
}

# Input Metadata Files
METADATA_PATHS = {
    "Leaf": os.path.join(BASE_OUTPUT_DIR, r"leaf/results/raw_attention_metadata_Leaf.feather"),
    "Root": os.path.join(BASE_OUTPUT_DIR, r"root/results/raw_attention_metadata_Root.feather")
}
METADATA_CSV_FALLBACK_PATHS = {
    "Leaf": os.path.join(BASE_OUTPUT_DIR, r"leaf/results/raw_attention_metadata_Leaf.csv"),
    "Root": os.path.join(BASE_OUTPUT_DIR, r"root/results/raw_attention_metadata_Root.csv")
}

# Output Directories for Processed Data
OUTPUT_DIRS = {
    "Leaf": os.path.join(BASE_OUTPUT_DIR, r"processed_attention_leaf"),
    "Root": os.path.join(BASE_OUTPUT_DIR, r"processed_attention_root")
}
OUTPUT_STRUCTURE = "PerPairing"

# Log directory within the main script's output path
LOG_DIR = os.path.join(BASE_OUTPUT_DIR, f"{SCRIPT_NAME}_logs")

# --- Processing Parameters ---
TOP_K_PAIRS = 500
CONDITIONAL_GROUPING_COLS = ['Genotype', 'Treatment', 'Day']
METADATA_INDEX_COL = 'Row_names'
PERCENTILE_VALUE = 95  # Percentile to calculate for view-level stats

# ===== LOGGING =====
def setup_logging(log_dir: str, script_name: str, version: str) -> logging.Logger:
    """
    Setup logging to both file and console.
    
    Args:
        log_dir: Directory to store log files
        script_name: Name of the script for log filename
        version: Version string for log filename
        
    Returns:
        Configured logger instance
    """
    os.makedirs(log_dir, exist_ok=True)
    log_filename = f"{script_name}_{version}_{datetime.now():%Y%m%d_%H%M%S}.log"
    log_filepath = os.path.join(log_dir, log_filename)
    log_format = '%(asctime)s - %(levelname)s - [%(module)s:%(lineno)d] - %(message)s'
    date_format = '%Y-%m-%d %H:%M:%S'
    formatter = logging.Formatter(log_format, datefmt=date_format)
    
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    if logger.hasHandlers():
        logger.handlers.clear()
        
    file_handler = logging.FileHandler(log_filepath)
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    def handle_exception(exc_type, exc_value, exc_traceback):
        if issubclass(exc_type, KeyboardInterrupt):
            sys.__excepthook__(exc_type, exc_value, exc_traceback)
            return
        logger.error("Uncaught exception", exc_info=(exc_type, exc_value, exc_traceback))
    
    sys.excepthook = handle_exception
    logger.info(f"Logging setup complete. Log file: {log_filepath}")
    return logger

logger = setup_logging(LOG_DIR, SCRIPT_NAME, VERSION)

logger.info("="*60)
logger.info(f"Starting Script: {SCRIPT_NAME} v{VERSION}")
logger.info(f"Processing Attention Data")
logger.info(f"Base Output Dir: {BASE_OUTPUT_DIR}")
logger.info(f"Top K Pairs to Save separately: {TOP_K_PAIRS}")
logger.info(f"Conditional Grouping Columns: {CONDITIONAL_GROUPING_COLS}")
logger.info(f"Metadata Index Column: {METADATA_INDEX_COL}")
logger.info(f"Calculating {PERCENTILE_VALUE}th percentile for view-level stats.")
logger.info("="*60)


# ===== HELPER FUNCTIONS =====
def load_h5_tensor_data(filepath: str) -> dict or None:
    """
    Load raw attention tensors and feature names from HDF5 file.
    
    Args:
        filepath: Path to the HDF5 file
        
    Returns:
        Dictionary with tensors and feature names or None if loading failed
    """
    logger.info(f"Loading tensor/feature data from HDF5: {filepath}")
    if not os.path.exists(filepath):
        logger.error(f"HDF5 file not found: {filepath}")
        return None
        
    data = {}
    try:
        with h5py.File(filepath, 'r') as f:
            logger.info("Keys found in HDF5: " + str(list(f.keys())))
            
            if 'attention_spec_to_metab' in f:
                data['attn_s2m'] = f['attention_spec_to_metab'][()]
                logger.info(f"  Loaded 'attention_spec_to_metab' shape: {data['attn_s2m'].shape}")
            else:
                logger.error("  'attention_spec_to_metab' not found.")
                return None
                
            if 'attention_metab_to_spec' in f:
                data['attn_m2s'] = f['attention_metab_to_spec'][()]
                logger.info(f"  Loaded 'attention_metab_to_spec' shape: {data['attn_m2s'].shape}")
            else:
                logger.warning("  'attention_metab_to_spec' not found.")
                data['attn_m2s'] = None
                
            if 'spectral_feature_names' in f:
                data['spec_features'] = [s.decode('utf-8') for s in f['spectral_feature_names'][()]]
                logger.info(f"  Loaded {len(data['spec_features'])} spectral features.")
            else:
                logger.error("  'spectral_feature_names' not found.")
                return None
                
            if 'metabolite_feature_names' in f:
                data['metab_features'] = [s.decode('utf-8') for s in f['metabolite_feature_names'][()]]
                logger.info(f"  Loaded {len(data['metab_features'])} metabolite features.")
            else:
                logger.error("  'metabolite_feature_names' not found.")
                return None

        # Validation for 4D tensors
        tensor_shape = data['attn_s2m'].shape
        if len(tensor_shape) != 4:
            logger.error(f"Expected 4D S->M tensor, got {len(tensor_shape)}D.")
            return None
            
        n_samples, n_heads, n_spec, n_metab = tensor_shape
        if n_spec != len(data['spec_features']):
            logger.error(f"S->M Spectral dim mismatch: Tensor={n_spec}, Features={len(data['spec_features'])}")
            return None
            
        if n_metab != len(data['metab_features']):
            logger.error(f"S->M Metabolite dim mismatch: Tensor={n_metab}, Features={len(data['metab_features'])}")
            return None
            
        if data.get('attn_m2s') is not None:
            m2s_shape = data['attn_m2s'].shape
            if len(m2s_shape) != 4:
                logger.warning(f"Expected 4D M->S tensor, got {len(m2s_shape)}D. M->S calculations might fail.")
            elif m2s_shape != (n_samples, n_heads, n_metab, n_spec):
                logger.error(f"M->S shape {m2s_shape} inconsistent with S->M {tensor_shape}.")
                return None
                
        logger.info("HDF5 tensor/feature data loaded and validated successfully.")
        return data
        
    except Exception as e:
        logger.error(f"Failed HDF5 load/process {filepath}: {e}", exc_info=True)
        return None


def load_metadata_file(feather_path: str, csv_fallback_path: str, index_col: str) -> pd.DataFrame or None:
    """
    Load metadata from Feather file with CSV fallback.
    
    Args:
        feather_path: Path to Feather file
        csv_fallback_path: Path to CSV file as fallback
        index_col: Column to use as DataFrame index
        
    Returns:
        Metadata DataFrame or None if loading failed
    """
    metadata_df = None
    
    # Try Feather
    if pyarrow and os.path.exists(feather_path):
        logger.info(f"Attempting metadata load from Feather: {feather_path}")
        try:
            metadata_df = pd.read_feather(feather_path)
            # Feather might save index as a column, check and set
            if index_col in metadata_df.columns:
                metadata_df.set_index(index_col, inplace=True)
                logger.info(f"  Loaded Feather, shape: {metadata_df.shape}. Index '{index_col}'.")
            elif metadata_df.index.name == index_col:
                 logger.info(f"  Loaded Feather, shape: {metadata_df.shape}. Index '{index_col}' already set.")
            else:
                 logger.warning(f"  Feather loaded, but index '{index_col}' not found as column or index name. Current index: '{metadata_df.index.name}'.")
            return metadata_df
        except Exception as e_feather:
            logger.warning(f"  Feather load failed: {e_feather}. Trying CSV fallback.")
            metadata_df = None
            
    # Fallback to CSV
    if metadata_df is None and os.path.exists(csv_fallback_path):
        logger.info(f"Attempting metadata load from CSV fallback: {csv_fallback_path}")
        try:
            metadata_df = pd.read_csv(csv_fallback_path, index_col=index_col)
            if metadata_df.index.name == index_col:
                logger.info(f"  Loaded CSV, shape: {metadata_df.shape}. Index '{index_col}'.")
            else:
                logger.warning(f"  CSV loaded, but failed set index '{index_col}'. Current: '{metadata_df.index.name}'.")
            return metadata_df
        except Exception as e_csv:
            logger.error(f"  CSV load failed: {e_csv}")
            return None
            
    if metadata_df is None:
        logger.error(f"Metadata failed load. Checked Feather: {feather_path}, CSV: {csv_fallback_path}")
        return None
        
    return metadata_df

# ===== MAIN PROCESSING FUNCTION =====
def process_pairing_attention(pairing_name: str,
                             hdf5_path: str,
                             metadata_path: str,
                             metadata_csv_fallback: str,
                             output_path: str,
                             config: dict):
    """
    Process attention data for a specific tissue pairing.
    
    Args:
        pairing_name: Name of the tissue pairing (e.g., "Leaf", "Root")
        hdf5_path: Path to HDF5 file with attention tensors
        metadata_path: Path to Feather file with metadata
        metadata_csv_fallback: Path to CSV file with metadata (fallback)
        output_path: Directory to save processed outputs
        config: Dictionary with processing parameters
        
    Returns:
        None
    """
    logger.info(f"\n===== Processing Pairing: {pairing_name} =====")
    proc_start_time = time.time()
    os.makedirs(output_path, exist_ok=True)
    logger.info(f"Output directory: {output_path}")

    # Load Raw Tensor Data (Expect 4D)
    tensor_data = load_h5_tensor_data(hdf5_path)
    if tensor_data is None:
        logger.error(f"Failed tensor load for {pairing_name}. Skip.")
        return
        
    attn_s2m_raw = tensor_data['attn_s2m']
    attn_m2s_raw = tensor_data.get('attn_m2s', None)
    spec_features = tensor_data['spec_features']
    metab_features = tensor_data['metab_features']

    # Load Metadata
    metadata_df = load_metadata_file(metadata_path, metadata_csv_fallback, config['METADATA_INDEX_COL'])
    if metadata_df is None:
        logger.error(f"Failed metadata load for {pairing_name}. Skip.")
        return

    # Critical Validation
    n_samples_attn = attn_s2m_raw.shape[0]
    n_samples_meta = len(metadata_df)
    if n_samples_attn != n_samples_meta:
        logger.error(f"SAMPLE COUNT MISMATCH for {pairing_name}! Tensor N={n_samples_attn}, Metadata Rows={n_samples_meta}. Aborting.")
        return
    else:
        logger.info(f"Validation PASSED: Tensor samples ({n_samples_attn}) match metadata rows ({n_samples_meta}).")

    # Common parameters
    n_samples, n_heads, n_spec, n_metab = attn_s2m_raw.shape
    top_k = config['TOP_K_PAIRS']
    pct_val = config.get('PERCENTILE_VALUE', 95)  # Use default if not in config
    grouping_cols = [col for col in config['CONDITIONAL_GROUPING_COLS'] if col in metadata_df.columns]
    logger.info(f"Processing {n_samples} samples, {n_heads} heads, {n_spec} spectral features, {n_metab} metabolite features.")
    logger.info(f"Calculating {pct_val}th percentile.")

    # Initialize variables for M->S calculations
    attn_m2s_avg_heads = None
    pairs_overall_m2s_df = None
    conditional_mean_pairs_s2m_df = None
    conditional_mean_pairs_m2s_df = None
    m2s_data_valid = False  # Flag to track if M->S data is usable

    # Pre-check M->S data validity
    if attn_m2s_raw is not None and len(attn_m2s_raw.shape) == 4:
        if attn_m2s_raw.shape == (n_samples, n_heads, n_metab, n_spec):
            m2s_data_valid = True
            logger.info("M->S tensor found and shape is valid.")
        else:
            logger.warning(f"M->S tensor shape {attn_m2s_raw.shape} incorrect. Expected {(n_samples, n_heads, n_metab, n_spec)}. Skipping M->S calculations.")
    else:
        logger.warning("M->S attention tensor not found or not 4D. Skipping M->S calculations.")


    try:
        # 1. Calculate and Save Per-Sample View-Level Stats (Mean, Std, Pct)
        logger.info("Calculating per-sample view-level statistics...")
        
        # S->M Stats
        avg_attn_s2m_per_sample = np.mean(attn_s2m_raw, axis=(1, 2, 3))
        std_attn_s2m_per_sample = np.std(attn_s2m_raw, axis=(1, 2, 3))
        pXX_attn_s2m_per_sample = np.percentile(attn_s2m_raw, q=pct_val, axis=(1, 2, 3))
        logger.info(f"  Calculated AvgAttn_S2M (shape: {avg_attn_s2m_per_sample.shape})")
        logger.info(f"  Calculated StdAttn_S2M (shape: {std_attn_s2m_per_sample.shape})")
        logger.info(f"  Calculated P{pct_val}Attn_S2M (shape: {pXX_attn_s2m_per_sample.shape})")

        # M->S Stats (Conditional)
        avg_attn_m2s_per_sample = None
        std_attn_m2s_per_sample = None
        pXX_attn_m2s_per_sample = None
        if m2s_data_valid:
            avg_attn_m2s_per_sample = np.mean(attn_m2s_raw, axis=(1, 2, 3))
            std_attn_m2s_per_sample = np.std(attn_m2s_raw, axis=(1, 2, 3))
            pXX_attn_m2s_per_sample = np.percentile(attn_m2s_raw, q=pct_val, axis=(1, 2, 3))
            logger.info(f"  Calculated AvgAttn_M2S (shape: {avg_attn_m2s_per_sample.shape})")
            logger.info(f"  Calculated StdAttn_M2S (shape: {std_attn_m2s_per_sample.shape})")
            logger.info(f"  Calculated P{pct_val}Attn_M2S (shape: {pXX_attn_m2s_per_sample.shape})")

        # Combine with metadata
        view_level_df = metadata_df.copy()
        view_level_df['AvgAttn_S2M'] = avg_attn_s2m_per_sample
        view_level_df['StdAttn_S2M'] = std_attn_s2m_per_sample
        view_level_df[f'P{pct_val}Attn_S2M'] = pXX_attn_s2m_per_sample
        view_level_df['AvgAttn_M2S'] = avg_attn_m2s_per_sample if m2s_data_valid else np.nan
        view_level_df['StdAttn_M2S'] = std_attn_m2s_per_sample if m2s_data_valid else np.nan
        view_level_df[f'P{pct_val}Attn_M2S'] = pXX_attn_m2s_per_sample if m2s_data_valid else np.nan

        outfile_view_level = os.path.join(output_path, f"processed_view_level_attention_{pairing_name}.csv")
        view_level_df.to_csv(outfile_view_level, index=True, index_label=config['METADATA_INDEX_COL'], float_format='%.6e')
        logger.info(f"Saved per-sample view-level statistics (Mean, Std, P{pct_val}) to: {outfile_view_level}")


        # 2. Calculate Feature-Pair Averages (Overall & Conditional)

        # S->M Calculations
        logger.info("Averaging S->M tensor over heads for S->M feature-pair analysis...")
        attn_s2m_avg_heads = np.mean(attn_s2m_raw, axis=1)  # Shape (N, N_spec, N_metab)
        logger.info(f"  S->M Shape after averaging heads: {attn_s2m_avg_heads.shape}")

        logger.info("Calculating overall mean S->M feature-pair attention...")
        mean_attn_overall_s2m = np.mean(attn_s2m_avg_heads, axis=0)  # (N_spec, N_metab)
        pairs_overall_s2m_list = []
        for i in range(n_spec):
            for j in range(n_metab):
                pairs_overall_s2m_list.append({
                    'Spectral_Feature': spec_features[i], 
                    'Metabolite_Feature': metab_features[j],
                    'Mean_Attention_S2M_AvgHeads': mean_attn_overall_s2m[i, j]
                })
        pairs_overall_s2m_df = pd.DataFrame(pairs_overall_s2m_list)
        pairs_overall_s2m_df.sort_values('Mean_Attention_S2M_AvgHeads', ascending=False, inplace=True)
        outfile_mean_overall_s2m = os.path.join(output_path, f"processed_mean_attention_overall_{pairing_name}.csv")
        pairs_overall_s2m_df.to_csv(outfile_mean_overall_s2m, index=False, float_format='%.6e')
        logger.info(f"Saved FULL overall S->M mean attention ({len(pairs_overall_s2m_df)} pairs) to: {outfile_mean_overall_s2m}")
        
        outfile_top_overall_s2m = os.path.join(output_path, f"processed_top_{top_k}_pairs_overall_{pairing_name}.csv")
        pairs_overall_s2m_df.head(top_k).to_csv(outfile_top_overall_s2m, index=False, float_format='%.6e')
        logger.info(f"Saved overall top {top_k} S->M pairs subset to: {outfile_top_overall_s2m}")
        
        top_k_pairs_tuples_s2m = list(zip(
            pairs_overall_s2m_df['Spectral_Feature'].head(top_k), 
            pairs_overall_s2m_df['Metabolite_Feature'].head(top_k)
        ))
        top_k_index_s2m = pd.MultiIndex.from_tuples(
            top_k_pairs_tuples_s2m, 
            names=['Spectral_Feature', 'Metabolite_Feature']
        )
        logger.info(f"Identified overall top {len(top_k_index_s2m)} S->M pairs for trend analysis.")

        # M->S Calculations
        if m2s_data_valid:
            logger.info("Averaging M->S tensor over heads for M->S feature-pair analysis...")
            attn_m2s_avg_heads = np.mean(attn_m2s_raw, axis=1)  # Shape (N, N_metab, N_spec)
            logger.info(f"  M->S Shape after averaging heads: {attn_m2s_avg_heads.shape}")

            logger.info("Calculating overall mean M->S feature-pair attention...")
            mean_attn_overall_m2s = np.mean(attn_m2s_avg_heads, axis=0)  # (N_metab, N_spec)
            pairs_overall_m2s_list = []
            for j in range(n_metab):  # Outer loop metab
                for i in range(n_spec):  # Inner loop spec
                    pairs_overall_m2s_list.append({
                        'Metabolite_Feature': metab_features[j], 
                        'Spectral_Feature': spec_features[i],
                        'Mean_Attention_M2S_AvgHeads': mean_attn_overall_m2s[j, i]
                    })
            pairs_overall_m2s_df = pd.DataFrame(pairs_overall_m2s_list)
            pairs_overall_m2s_df.sort_values('Mean_Attention_M2S_AvgHeads', ascending=False, inplace=True)

            # Save M->S Overall Files
            outfile_mean_overall_m2s = os.path.join(output_path, f"processed_mean_attention_overall_M2S_{pairing_name}.csv")
            pairs_overall_m2s_df.to_csv(outfile_mean_overall_m2s, index=False, float_format='%.6e')
            logger.info(f"Saved FULL overall M->S mean attention ({len(pairs_overall_m2s_df)} pairs) to: {outfile_mean_overall_m2s}")
            
            outfile_top_overall_m2s = os.path.join(output_path, f"processed_top_{top_k}_pairs_overall_M2S_{pairing_name}.csv")
            pairs_overall_m2s_df.head(top_k).to_csv(outfile_top_overall_m2s, index=False, float_format='%.6e')
            logger.info(f"Saved overall top {top_k} M->S pairs subset to: {outfile_top_overall_m2s}")
        else:
            logger.warning("Skipping M->S Overall Mean calculations (M->S data invalid).")


        # Conditional Mean Attention (Combined Loop for S->M and M->S)
        logger.info(f"Calculating conditional mean feature-pair attention grouped by: {grouping_cols}")
        if not grouping_cols:
            logger.warning("No valid grouping columns. Skipping conditional analysis.")
            conditional_mean_pairs_s2m_df = None
            conditional_mean_pairs_m2s_df = None
        else:
            metadata_for_grouping = metadata_df.copy()
            for col in list(grouping_cols):  # Iterate over a copy for safe removal
                if col in metadata_for_grouping.columns:
                    dtype = metadata_for_grouping[col].dtype
                    # Convert non-string/object/category columns to string for grouping
                    if not pd.api.types.is_string_dtype(dtype) and not pd.api.types.is_object_dtype(dtype) and not pd.api.types.is_categorical_dtype(dtype):
                        logger.debug(f"Converting grouping column '{col}' (dtype: {dtype}) to string.")
                        try:
                            metadata_for_grouping[col] = metadata_for_grouping[col].astype(str)
                        except Exception as e_conv:
                            logger.error(f"Failed to convert '{col}' to string: {e_conv}. Removing from grouping.")
                            grouping_cols.remove(col)
                else:
                    logger.warning(f"Grouping column '{col}' not found in metadata, removing from list.")
                    grouping_cols.remove(col)

            if not grouping_cols:
                logger.warning("No valid grouping columns remain. Skipping conditional analysis.")
                conditional_mean_pairs_s2m_df = None
                conditional_mean_pairs_m2s_df = None
            else:
                logger.info(f"Using final grouping columns: {grouping_cols}")
                if metadata_for_grouping.index.name != config['METADATA_INDEX_COL']:
                    logger.error(f"Metadata index name issue.")
                    return
                try:
                    sample_id_to_int_pos = {sid: i for i, sid in enumerate(metadata_for_grouping.index)}
                except Exception as e_map:
                    logger.error(f"Failed index mapping: {e_map}")
                    return

                grouped_metadata = metadata_for_grouping.groupby(grouping_cols)
                logger.info(f"Found {len(grouped_metadata)} unique condition groups.")

                all_conditional_mean_pairs_s2m = []
                all_conditional_mean_pairs_m2s = []
                group_counter = 0
                
                for group_keys, group_sample_ids in grouped_metadata.groups.items():
                    group_counter += 1
                    group_keys = group_keys if isinstance(group_keys, tuple) else (group_keys,)
                    group_label_dict = dict(zip(grouping_cols, group_keys))
                    group_label_str = "_".join([f"{k}={v}" for k, v in group_label_dict.items()])
                    logger.debug(f"Processing group {group_counter}/{len(grouped_metadata)}: {group_label_dict} ({len(group_sample_ids)} samples)")
                    
                    if len(group_sample_ids) == 0:
                        continue
                        
                    integer_indices = [sample_id_to_int_pos.get(sid) for sid in group_sample_ids if sample_id_to_int_pos.get(sid) is not None]
                    if not integer_indices:
                        continue
                        
                    if len(integer_indices) < len(group_sample_ids):
                        logger.warning(f" {len(group_sample_ids) - len(integer_indices)} SampleIDs not mapped.")

                    # S->M Calculation
                    group_attn_s2m = attn_s2m_avg_heads[integer_indices, :, :]
                    mean_attn_group_s2m = np.mean(group_attn_s2m, axis=0)
                    group_pairs_s2m_list = []
                    for i in range(n_spec):
                        for j in range(n_metab):
                            group_pairs_s2m_list.append({
                                'Spectral_Feature': spec_features[i],
                                'Metabolite_Feature': metab_features[j],
                                'Mean_Attention_S2M_Group_AvgHeads': mean_attn_group_s2m[i, j]
                            })
                    group_pairs_s2m_df = pd.DataFrame(group_pairs_s2m_list)
                    for col, value in group_label_dict.items():
                        group_pairs_s2m_df[col] = value
                    group_pairs_s2m_df['N_Samples_Group'] = len(integer_indices)
                    all_conditional_mean_pairs_s2m.append(group_pairs_s2m_df)

                    # M->S Calculation (Conditional)
                    if m2s_data_valid and attn_m2s_avg_heads is not None:  # Double check avg_heads was created
                        group_attn_m2s = attn_m2s_avg_heads[integer_indices, :, :]
                        mean_attn_group_m2s = np.mean(group_attn_m2s, axis=0)
                        group_pairs_m2s_list = []
                        for j in range(n_metab):
                            for i in range(n_spec):
                                group_pairs_m2s_list.append({
                                    'Metabolite_Feature': metab_features[j],
                                    'Spectral_Feature': spec_features[i],
                                    'Mean_Attention_M2S_Group_AvgHeads': mean_attn_group_m2s[j, i]
                                })
                        group_pairs_m2s_df = pd.DataFrame(group_pairs_m2s_list)
                        for col, value in group_label_dict.items():
                            group_pairs_m2s_df[col] = value
                        group_pairs_m2s_df['N_Samples_Group'] = len(integer_indices)
                        all_conditional_mean_pairs_m2s.append(group_pairs_m2s_df)

                # Concatenate S->M results
                conditional_mean_pairs_s2m_df = None
                if all_conditional_mean_pairs_s2m:
                    conditional_mean_pairs_s2m_df = pd.concat(all_conditional_mean_pairs_s2m, ignore_index=True)
                    id_cols_s2m = grouping_cols + ['N_Samples_Group']
                    value_cols_s2m = ['Spectral_Feature', 'Metabolite_Feature', 'Mean_Attention_S2M_Group_AvgHeads']
                    conditional_mean_pairs_s2m_df = conditional_mean_pairs_s2m_df[id_cols_s2m + value_cols_s2m]
                    outfile_mean_conditional_s2m = os.path.join(output_path, f"processed_mean_attention_conditional_{pairing_name}.csv")
                    conditional_mean_pairs_s2m_df.to_csv(outfile_mean_conditional_s2m, index=False, float_format='%.6e')
                    logger.info(f"Saved FULL conditional S->M mean attention ({len(conditional_mean_pairs_s2m_df)} rows) to: {outfile_mean_conditional_s2m}")
                else:
                    logger.warning("No S->M conditional mean pairs generated.")

                # Concatenate M->S results
                conditional_mean_pairs_m2s_df = None
                if all_conditional_mean_pairs_m2s:
                    conditional_mean_pairs_m2s_df = pd.concat(all_conditional_mean_pairs_m2s, ignore_index=True)
                    id_cols_m2s = grouping_cols + ['N_Samples_Group']
                    value_cols_m2s = ['Metabolite_Feature', 'Spectral_Feature', 'Mean_Attention_M2S_Group_AvgHeads']
                    conditional_mean_pairs_m2s_df = conditional_mean_pairs_m2s_df[id_cols_m2s + value_cols_m2s]
                    outfile_mean_conditional_m2s = os.path.join(output_path, f"processed_mean_attention_conditional_M2S_{pairing_name}.csv")
                    conditional_mean_pairs_m2s_df.to_csv(outfile_mean_conditional_m2s, index=False, float_format='%.6e')
                    logger.info(f"Saved FULL conditional M->S mean attention ({len(conditional_mean_pairs_m2s_df)} rows) to: {outfile_mean_conditional_m2s}")
                else:
                    logger.warning("No M->S conditional mean pairs generated.")


        # Attention Trends for Overall Top K S->M Pairs
        logger.info(f"Extracting S->M attention trends for overall top {top_k} S->M pairs...")
        if conditional_mean_pairs_s2m_df is not None and not conditional_mean_pairs_s2m_df.empty and not top_k_index_s2m.empty:
            try:
                required_trend_cols = ['Spectral_Feature', 'Metabolite_Feature'] + grouping_cols
                missing_trend_cols = [c for c in required_trend_cols if c not in conditional_mean_pairs_s2m_df.columns]
                if missing_trend_cols:
                    logger.error(f"Cannot calc S->M trends. Missing columns: {missing_trend_cols}")
                else:
                    conditional_mean_pairs_s2m_df_indexed = conditional_mean_pairs_s2m_df.set_index(
                        ['Spectral_Feature', 'Metabolite_Feature']
                    )
                    trends_df = conditional_mean_pairs_s2m_df_indexed[
                        conditional_mean_pairs_s2m_df_indexed.index.isin(top_k_index_s2m)
                    ].reset_index()
                    
                    if not trends_df.empty:
                        trends_df.sort_values(
                            by=grouping_cols + ['Spectral_Feature', 'Metabolite_Feature'], 
                            inplace=True
                        )
                        outfile_trends = os.path.join(output_path, f"processed_attention_trends_top_{top_k}_{pairing_name}.csv")
                        trends_df.to_csv(outfile_trends, index=False, float_format='%.6e')
                        logger.info(f"Saved S->M attention trends for top {top_k} pairs ({len(trends_df)} rows) to: {outfile_trends}")
                    else:
                        logger.warning("No S->M rows matched top K pairs in conditional data. Trends file not saved.")
            except Exception as e_trends:
                logger.error(f"Error during S->M trends calculation: {e_trends}", exc_info=True)
        else:
            logger.warning("Skipping S->M trends calc (missing conditional means or top K S->M pairs).")

        # Placeholder for Sync Metrics
        logger.info("Placeholder for Synchronization Metrics calculation.")

    except Exception as e:
        logger.error(f"An error occurred during processing for {pairing_name}: {e}", exc_info=True)

    proc_end_time = time.time()
    logger.info(f"===== Finished processing {pairing_name}. Duration: {(proc_end_time - proc_start_time):.2f} seconds =====")


# ===== MAIN EXECUTION =====
def main():
    """
    Main execution function that processes each pairing.
    """
    main_start_time = time.time()
    logger.info("--- Starting Main Execution ---")
    
    # Configuration dictionary
    global_config = {
        'TOP_K_PAIRS': TOP_K_PAIRS,
        'CONDITIONAL_GROUPING_COLS': CONDITIONAL_GROUPING_COLS,
        'METADATA_INDEX_COL': METADATA_INDEX_COL,
        'PERCENTILE_VALUE': PERCENTILE_VALUE
    }
    pairings_to_process = list(HDF5_PATHS.keys())

    for pairing in pairings_to_process:
        hdf5_file = HDF5_PATHS.get(pairing)
        metadata_file = METADATA_PATHS.get(pairing)
        metadata_csv_fallback_file = METADATA_CSV_FALLBACK_PATHS.get(pairing)
        output_dir_pairing = OUTPUT_DIRS.get(pairing)

        if not all([hdf5_file, (metadata_file or metadata_csv_fallback_file), output_dir_pairing]):
            logger.warning(f"Missing essential file/dir path for pairing '{pairing}'. Skipping.")
            if not hdf5_file:
                logger.warning("  Reason: HDF5 path missing.")
            if not (metadata_file or metadata_csv_fallback_file):
                logger.warning("  Reason: Both metadata Feather and CSV paths missing.")
            if not output_dir_pairing:
                logger.warning("  Reason: Output directory path missing.")
            continue

        process_pairing_attention(
            pairing_name=pairing,
            hdf5_path=hdf5_file,
            metadata_path=metadata_file,
            metadata_csv_fallback=metadata_csv_fallback_file,
            output_path=output_dir_pairing,
            config=global_config
        )

    main_end_time = time.time()
    total_duration_min = (main_end_time - main_start_time) / 60
    logger.info(f"--- Main Execution Finished --- Total Duration: {total_duration_min:.2f} minutes ---")
    logger.info("="*60)


# --- Entry Point ---
if __name__ == '__main__':
    try:
        main()
    except Exception as e:
        logger.critical(f"Critical error in main execution: {e}", exc_info=True)
        sys.exit(1)

--------------------------------------------------

train_transformer_attn.py
# -*- coding: utf-8 -*-
"""
Transformer Model for Multi-Omic Plant Stress Response Analysis

This script implements a Transformer model with feature-level cross-attention
for analyzing plant stress response using multiple omics data types. The model 
takes filtered feature sets (~50 features/view) selected by MOFA+ as input
and performs multi-task classification for genotype, treatment, and day.

Key components:
1. Data loading and preprocessing from MOFA+ outputs
2. Transformer with feature-level cross-attention mechanism
3. Multi-task learning for classification tasks
4. Extraction and analysis of cross-modal attention patterns
5. Baseline model comparison with traditional ML methods

Outputs include model performance metrics, feature importance scores,
cross-modal feature pairs, and raw attention tensors for further analysis.

Workflow:
1. Load preprocessed data (_50feat.csv files).
2. Optionally subset features further for testing.
3. Define Transformer model with per-feature embeddings and cross-attention.
4. Train the model for multi-task classification (Genotype, Treatment, Day).
5. Evaluate on the test set.
6. Extract FEATURE-LEVEL attention weights (Spec<->Metab).
7. Calculate basic cross-modal pairs and feature importance from attention.
8. **Save raw attention tensor, predictions, and full test metadata for offline analysis/visualization.**
9. Run baseline models for comparison.
"""

# ===== IMPORTS =====
import os
import sys
import time
import logging
import traceback
from datetime import datetime
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix
# Visualization imports removed as per request
# import matplotlib.pyplot as plt
# import seaborn as sns
import h5py # For saving large attention tensors efficiently

# Baselines (Keep for comparison)
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline # Needed for baseline CV

# ===== CONFIGURATION =====

# --- Script Info ---
SCRIPT_NAME = "transformer_multi_omic_v3_attention"
VERSION = "1.2.2_AvgWeightsFix" # Version update

# --- Paths ---
BASE_DIR = r"C:/Users/ms/Desktop/hyper"
# Input files are from the mofa50 run
MOFA_OUTPUT_DIR = os.path.join(BASE_DIR, "output", "mofa") # <<< POINT TO mofa50 SUBDIR
# Specific output directory for this version
TRANSFORMER_BASE_OUTPUT_DIR = os.path.join(BASE_DIR, "output", "transformer")
OUTPUT_DIR = os.path.join(TRANSFORMER_BASE_OUTPUT_DIR, "v3_feature_attention") # <<< NEW OUTPUT DIR
CHECKPOINT_SUBDIR = os.path.join(OUTPUT_DIR, "checkpoints")
LOG_SUBDIR = os.path.join(OUTPUT_DIR, "logs")
RESULTS_SUBDIR = os.path.join(OUTPUT_DIR, "results") # For CSVs, attention data

# Ensure output directories exist
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(CHECKPOINT_SUBDIR, exist_ok=True)
os.makedirs(LOG_SUBDIR, exist_ok=True)
os.makedirs(RESULTS_SUBDIR, exist_ok=True)

# --- Analysis Pairing ---
# Choose 'Leaf' or 'Root' to determine which data pairing to analyze
ANALYSIS_PAIRING = "Root" # Or "Leaf" - CHANGE THIS MANUALLY

# --- Input Data Files --- (Using _50feat suffix)
INPUT_FILES = {
    "Leaf": {
        "spectral": os.path.join(MOFA_OUTPUT_DIR, "transformer_input_leaf_spectral.csv"),
        "metabolite": os.path.join(MOFA_OUTPUT_DIR, "transformer_input_leaf_metabolite.csv"),
    },
    "Root": {
        "spectral": os.path.join(MOFA_OUTPUT_DIR, "transformer_input_root_spectral.csv"),
        "metabolite": os.path.join(MOFA_OUTPUT_DIR, "transformer_input_root_metabolite.csv"),
    }
}

# --- Feature Subsetting (Optional for Testing) ---
# Set to a small number (e.g., 20) to test run quickly. Set high (e.g., 1000) to use all available features.
MAX_FEATURES_PER_VIEW_RUNTIME = 1000 # Example for quick test run

# --- Data & Columns ---
METADATA_COLS = ['Row_names', 'Vac_id', 'Genotype', 'Entry', 'Tissue.type',
                 'Batch', 'Treatment', 'Replication', 'Day']
TARGET_COLS = ['Genotype', 'Treatment', 'Day']

# --- Model Hyperparameters ---
HIDDEN_DIM = 64
NUM_HEADS = 4 # Keep relatively low for memory
NUM_LAYERS = 2 # Keep shallow
DROPOUT = 0.1
NUM_CLASSES = {'Genotype': 2, 'Treatment': 2, 'Day': 3}

# --- Training Hyperparameters ---
LEARNING_RATE = 5e-5 # Can try slightly higher for smaller model/data
BATCH_SIZE = 16 # <<< Start VERY SMALL due to attention complexity & 6GB VRAM
EPOCHS = 150 # Train potentially longer, but rely on early stopping
EARLY_STOPPING_PATIENCE = 15 # Increase patience slightly
WEIGHT_DECAY = 1e-5

# --- Data Handling ---
VAL_SIZE = 0.15
TEST_SIZE = 0.15
NUM_WORKERS = 0 # <<< Set to 0 for Windows, especially with complex models/limited RAM, can increase later if needed
RANDOM_SEED = 42
ENCODING_MAPS = {'Genotype': {'G1': 0, 'G2': 1}, 'Treatment': {0: 0, 1: 1}, 'Day': {1: 0, 2: 1, 3: 2}}

# --- Device ---
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ===== LOGGING =====
def setup_logging(log_dir: str, script_name: str, version: str) -> logging.Logger:
    """Set up logging with file and console handlers."""
    os.makedirs(log_dir, exist_ok=True)
    log_filename = f"{script_name}_{version}_{datetime.now():%Y%m%d_%H%M%S}.log"
    log_filepath = os.path.join(log_dir, log_filename)
    log_format = '%(asctime)s - %(levelname)s - [%(module)s:%(lineno)d] - %(message)s'
    date_format = '%Y-%m-%d %H:%M:%S'
    formatter = logging.Formatter(log_format, datefmt=date_format)
    
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    if logger.hasHandlers():
        logger.handlers.clear()
        
    file_handler = logging.FileHandler(log_filepath)
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    def handle_exception(exc_type, exc_value, exc_traceback):
        if issubclass(exc_type, KeyboardInterrupt):
            sys.__excepthook__(exc_type, exc_value, exc_traceback)
            return
        logger.error("Uncaught exception", exc_info=(exc_type, exc_value, exc_traceback))
    
    sys.excepthook = handle_exception
    logger.info(f"Logging setup complete. Log file: {log_filepath}")
    return logger

logger = setup_logging(LOG_SUBDIR, SCRIPT_NAME, VERSION)

logger.info("="*60)
logger.info(f"Starting Script: {SCRIPT_NAME} v{VERSION} (Feature-Level Attention)")
logger.info(f"Analysis Pairing: {ANALYSIS_PAIRING}")
logger.info(f"Output Directory: {OUTPUT_DIR}")
logger.info(f"Input Directory (MOFA Features): {MOFA_OUTPUT_DIR}")
logger.info(f"Using Device: {DEVICE}")
logger.info(f"Batch Size: {BATCH_SIZE}, Num Workers: {NUM_WORKERS}")
logger.info(f"Max Features Per View (Runtime): {MAX_FEATURES_PER_VIEW_RUNTIME}")
logger.info(f"Model Params: Hidden Dim={HIDDEN_DIM}, Heads={NUM_HEADS}, Layers={NUM_LAYERS}")
logger.info("="*60)

# ===== DATA LOADING & PREPROCESSING =====

class PlantOmicsDataset(Dataset):
    """PyTorch Dataset for paired spectral and metabolite data."""
    def __init__(self, spectral_features, metabolite_features, targets):
        if not spectral_features.index.equals(metabolite_features.index) or \
           not spectral_features.index.equals(targets.index):
            raise ValueError("Indices of spectral, metabolite, and target dataframes do not match!")
        self.spectral_data = torch.tensor(spectral_features.values, dtype=torch.float32)
        self.metabolite_data = torch.tensor(metabolite_features.values, dtype=torch.float32)
        self.targets = torch.tensor(targets.values, dtype=torch.long)
        self.sample_ids = spectral_features.index.tolist()
        
    def __len__(self):
        return len(self.spectral_data)
    
    def __getitem__(self, idx):
        return {
            'spectral': self.spectral_data[idx],
            'metabolite': self.metabolite_data[idx],
            'targets': self.targets[idx],
            'sample_id': self.sample_ids[idx]
        }

def load_and_preprocess_data(config: dict) -> tuple:
    """
    Loads, aligns, preprocesses, and splits data for training.
    
    Args:
        config: Dictionary containing configuration parameters
        
    Returns:
        Tuple containing data loaders, feature names, indices, scalers,
        encoders, dataframes, and test metadata
    """
    pairing = config['ANALYSIS_PAIRING']
    max_features = config['MAX_FEATURES_PER_VIEW_RUNTIME']
    logger.info(f"--- Starting Data Loading & Preprocessing ({pairing}, Max Feat={max_features}) ---")

    spectral_path = config['INPUT_FILES'][pairing]['spectral']
    metabolite_path = config['INPUT_FILES'][pairing]['metabolite']

    try:
        logger.info(f"Loading spectral data: {spectral_path}")
        df_spectral_raw = pd.read_csv(spectral_path, index_col='Row_names', na_values='NA')
        logger.info(f"Loading metabolite data: {metabolite_path}")
        df_metabolite_raw = pd.read_csv(metabolite_path, index_col='Row_names', na_values='NA')
    except Exception as e:
        logger.error(f"Error loading data files: {e}")
        raise

    # Align samples between datasets
    common_indices = df_spectral_raw.index.intersection(df_metabolite_raw.index)
    if len(common_indices) == 0:
        raise ValueError("Alignment failed: No common Row_names.")
    if len(common_indices) < len(df_spectral_raw.index) or len(common_indices) < len(df_metabolite_raw.index):
        logger.warning(f"Found {len(common_indices)} common samples. Proceeding with common samples only.")
    
    df_spectral_raw = df_spectral_raw.loc[common_indices]
    df_metabolite_raw = df_metabolite_raw.loc[common_indices]
    logger.info(f"Data aligned by Row_names index. Samples: {len(common_indices)}")

    # Extract metadata and features
    meta_cols = config['METADATA_COLS']
    target_cols = config['TARGET_COLS']
    meta_cols_to_extract = [col for col in meta_cols if col != 'Row_names' and col in df_spectral_raw.columns]

    # Store the full metadata for ALL common samples before splitting
    full_metadata_df = df_spectral_raw[meta_cols_to_extract].copy()
    
    # Keep index name consistent
    if full_metadata_df.index.name != 'Row_names':
        logger.warning(f"Renaming full_metadata_df index from '{full_metadata_df.index.name}' to 'Row_names'.")

    # Extract features
    features_spectral_df = df_spectral_raw.drop(columns=meta_cols_to_extract, errors='ignore')
    features_metabolite_df = df_metabolite_raw.drop(columns=meta_cols_to_extract, errors='ignore')

    # Optional Feature Subsetting
    if max_features < features_spectral_df.shape[1]:
        logger.info(f"Subsetting spectral features from {features_spectral_df.shape[1]} to {max_features}.")
        features_spectral_df = features_spectral_df.iloc[:, :max_features]
    if max_features < features_metabolite_df.shape[1]:
        logger.info(f"Subsetting metabolite features from {features_metabolite_df.shape[1]} to {max_features}.")
        features_metabolite_df = features_metabolite_df.iloc[:, :max_features]

    spectral_feature_names = features_spectral_df.columns.tolist()
    metabolite_feature_names = features_metabolite_df.columns.tolist()
    logger.info(f"Using {len(spectral_feature_names)} spectral features and {len(metabolite_feature_names)} metabolite features.")

    # Target Encoding
    targets_encoded = pd.DataFrame(index=full_metadata_df.index)
    label_encoders = {}
    missing_targets = [col for col in target_cols if col not in full_metadata_df.columns]
    if missing_targets:
        raise ValueError(f"Missing target columns in metadata: {missing_targets}")

    for col in target_cols:
        if col in config['ENCODING_MAPS']:
            targets_encoded[col] = full_metadata_df[col].map(config['ENCODING_MAPS'][col])
            # Create LabelEncoder consistent with the map for inverse transform later
            le = LabelEncoder()
            sorted_items = sorted(config['ENCODING_MAPS'][col].items(), key=lambda item: item[1])
            le.classes_ = np.array([item[0] for item in sorted_items])
            label_encoders[col] = le
        else:
            logger.warning(f"Used LabelEncoder for target '{col}'.")
            le = LabelEncoder()
            targets_encoded[col] = le.fit_transform(full_metadata_df[col])
            label_encoders[col] = le
            
        if targets_encoded[col].isnull().any():
            nan_indices = targets_encoded[targets_encoded[col].isnull()].index.tolist()
            logger.error(f"NaN values found in encoded target '{col}' for samples: {nan_indices}. "
                         f"Original values: {full_metadata_df.loc[nan_indices, col].unique()}. Check mapping.")
            raise ValueError(f"Encoding failed for target '{col}'.")
    
    logger.info("Target encoding complete.")

    # Train/Val/Test Split (Stratified)
    try:
        full_metadata_df['stratify_key'] = full_metadata_df[target_cols[0]].astype(str)
        for col in target_cols[1:]:
            full_metadata_df['stratify_key'] += '_' + full_metadata_df[col].astype(str)
    except KeyError as e:
        logger.error(f"Stratification key column missing: {e}")
        raise

    indices = full_metadata_df.index
    stratify_values = full_metadata_df['stratify_key']
    if stratify_values.isnull().any():
        logger.warning("NaNs found in stratification key.")

    try:
        train_idx, temp_idx = train_test_split(
            indices, 
            test_size=(config['VAL_SIZE'] + config['TEST_SIZE']), 
            random_state=config['RANDOM_SEED'], 
            stratify=stratify_values
        )
    except ValueError:
        logger.warning("Stratification failed for train/temp split. Proceeding without stratification.")
        train_idx, temp_idx = train_test_split(
            indices, 
            test_size=(config['VAL_SIZE'] + config['TEST_SIZE']), 
            random_state=config['RANDOM_SEED']
        )
        
    relative_test_size = config['TEST_SIZE'] / (config['VAL_SIZE'] + config['TEST_SIZE'])
    temp_stratify_values = stratify_values.loc[temp_idx]
    
    try:
        val_idx, test_idx = train_test_split(
            temp_idx, 
            test_size=relative_test_size, 
            random_state=config['RANDOM_SEED'], 
            stratify=temp_stratify_values
        )
    except ValueError:
        logger.warning("Stratification failed for val/test split. Proceeding without stratification.")
        val_idx, test_idx = train_test_split(
            temp_idx, 
            test_size=relative_test_size, 
            random_state=config['RANDOM_SEED']
        )

    # Split feature and target datasets
    X_train_spec = features_spectral_df.loc[train_idx]
    X_val_spec = features_spectral_df.loc[val_idx]
    X_test_spec = features_spectral_df.loc[test_idx]
    
    X_train_metab = features_metabolite_df.loc[train_idx]
    X_val_metab = features_metabolite_df.loc[val_idx]
    X_test_metab = features_metabolite_df.loc[test_idx]
    
    y_train = targets_encoded.loc[train_idx]
    y_val = targets_encoded.loc[val_idx]
    y_test = targets_encoded.loc[test_idx]
    
    logger.info(f"Split sizes: Train={len(train_idx)}, Val={len(val_idx)}, Test={len(test_idx)}")

    # Get Test Metadata Subset
    test_metadata_df = full_metadata_df.loc[test_idx].copy()
    test_metadata_df = test_metadata_df.drop(columns=['stratify_key'], errors='ignore')
    logger.info(f"Extracted test metadata subset with shape: {test_metadata_df.shape}")

    # Convert potentially numeric columns before returning
    logger.info("Converting key metadata columns to numeric where appropriate...")
    numeric_cols_to_try = ['Entry', 'Treatment', 'Replication', 'Day']
    for col in numeric_cols_to_try:
        if col in test_metadata_df.columns:
            original_sum_possible = False
            original_sum = 0
            original_dtype = test_metadata_df[col].dtype

            # Check original sum only if it's already numeric
            if pd.api.types.is_numeric_dtype(original_dtype):
                try:
                    original_sum = test_metadata_df[col].sum()
                    original_sum_possible = True
                except TypeError:
                    logger.debug(f"Could not calculate original sum for numeric column '{col}' (dtype: {original_dtype}).")

            try:
                # Use errors='coerce'. If it fails, NaNs are introduced.
                converted_col = pd.to_numeric(test_metadata_df[col], errors='coerce')
                nans_introduced = converted_col.isnull().any() and not test_metadata_df[col].isnull().any()

                # If NaNs were introduced, or if original was int, convert to float for HDF5 compatibility
                if nans_introduced or pd.api.types.is_integer_dtype(original_dtype):
                    # Check if the converted column is actually numeric before attempting astype
                    if pd.api.types.is_numeric_dtype(converted_col.dtype):
                        if not pd.api.types.is_float_dtype(converted_col.dtype):
                            logger.info(f"Converting column '{col}' to float64 due to NaNs or original int type.")
                            test_metadata_df[col] = converted_col.astype(np.float64)
                        else:
                            # Already float, assign the converted column (which might have new NaNs)
                            test_metadata_df[col] = converted_col
                elif pd.api.types.is_numeric_dtype(converted_col.dtype):
                    # Conversion successful, no new NaNs, wasn't originally int -> assign converted
                    test_metadata_df[col] = converted_col

                # Log if sum changed significantly (potential issue indicator)
                # Check only if the column *is now* numeric and original sum was possible
                if pd.api.types.is_numeric_dtype(test_metadata_df[col].dtype) and original_sum_possible:
                    try:
                        new_sum = test_metadata_df[col].sum()
                        # Use a tolerance for float comparison, handle NaNs
                        if not np.isclose(original_sum, new_sum, rtol=1e-5, atol=1e-8, equal_nan=True):
                            logger.warning(f"Sum for column '{col}' changed after numeric conversion ({original_sum} -> {new_sum}). Check data.")
                    except TypeError:
                        logger.debug(f"Could not calculate new sum for column '{col}' after potential conversion.")

            except Exception as e_conv:
                logger.error(f"Failed to process column '{col}' for numeric conversion: {e_conv}. Leaving as is.")

    # Feature Scaling
    scaler_spec = StandardScaler()
    X_train_spec_scaled = scaler_spec.fit_transform(X_train_spec)
    X_val_spec_scaled = scaler_spec.transform(X_val_spec)
    X_test_spec_scaled = scaler_spec.transform(X_test_spec)
    
    scaler_metab = StandardScaler()
    X_train_metab_scaled = scaler_metab.fit_transform(X_train_metab)
    X_val_metab_scaled = scaler_metab.transform(X_val_metab)
    X_test_metab_scaled = scaler_metab.transform(X_test_metab)
    
    if np.isnan(X_train_spec_scaled).any() or np.isinf(X_train_spec_scaled).any():
        raise ValueError("Bad values in scaled spectral data")
    if np.isnan(X_train_metab_scaled).any() or np.isinf(X_train_metab_scaled).any():
        raise ValueError("Bad values in scaled metabolite data")
        
    # Convert scaled arrays back to dataframes with original indices and column names
    X_train_spec_scaled_df = pd.DataFrame(X_train_spec_scaled, index=train_idx, columns=spectral_feature_names)
    X_val_spec_scaled_df = pd.DataFrame(X_val_spec_scaled, index=val_idx, columns=spectral_feature_names)
    X_test_spec_scaled_df = pd.DataFrame(X_test_spec_scaled, index=test_idx, columns=spectral_feature_names)
    
    X_train_metab_scaled_df = pd.DataFrame(X_train_metab_scaled, index=train_idx, columns=metabolite_feature_names)
    X_val_metab_scaled_df = pd.DataFrame(X_val_metab_scaled, index=val_idx, columns=metabolite_feature_names)
    X_test_metab_scaled_df = pd.DataFrame(X_test_metab_scaled, index=test_idx, columns=metabolite_feature_names)
    
    scalers = {'spectral': scaler_spec, 'metabolite': scaler_metab}

    # Create datasets and dataloaders
    train_dataset = PlantOmicsDataset(X_train_spec_scaled_df, X_train_metab_scaled_df, y_train)
    val_dataset = PlantOmicsDataset(X_val_spec_scaled_df, X_val_metab_scaled_df, y_val)
    test_dataset = PlantOmicsDataset(X_test_spec_scaled_df, X_test_metab_scaled_df, y_test)
    
    train_loader = DataLoader(train_dataset, batch_size=config['BATCH_SIZE'], shuffle=True, 
                              num_workers=config['NUM_WORKERS'], pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=config['BATCH_SIZE'], shuffle=False, 
                            num_workers=config['NUM_WORKERS'], pin_memory=True)
    test_loader = DataLoader(test_dataset, batch_size=config['BATCH_SIZE'], shuffle=False, 
                             num_workers=config['NUM_WORKERS'], pin_memory=True)
    
    logger.info("DataLoaders created.")
    logger.info("--- Data Loading & Preprocessing Finished ---")

    return (train_loader, val_loader, test_loader,
            spectral_feature_names, metabolite_feature_names,
            train_idx.tolist(), val_idx.tolist(), test_idx.tolist(),
            scalers, label_encoders,
            X_train_spec_scaled_df, X_val_spec_scaled_df, X_test_spec_scaled_df,
            X_train_metab_scaled_df, X_val_metab_scaled_df, X_test_metab_scaled_df,
            y_train, y_val, y_test,
            test_metadata_df)


# ===== MODEL DEFINITION =====
class CrossAttentionLayer(nn.Module):
    """Implements cross-attention between two modalities (sequences)."""
    def __init__(self, hidden_dim, num_heads, dropout=0.1):
        super().__init__()
        self.cross_attn_1_to_2 = nn.MultiheadAttention(
            hidden_dim, num_heads, dropout=dropout, batch_first=True
        )
        self.cross_attn_2_to_1 = nn.MultiheadAttention(
            hidden_dim, num_heads, dropout=dropout, batch_first=True
        )
        self.norm1 = nn.LayerNorm(hidden_dim)
        self.norm2 = nn.LayerNorm(hidden_dim)
        self.dropout = nn.Dropout(dropout)
        self.ffn = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 2, hidden_dim)
        )
        self.norm3 = nn.LayerNorm(hidden_dim)  # Shared norm after FFN

    def forward(self, x1, x2):
        """
        Process inputs through cross-attention in both directions.
        
        Args:
            x1: Features from modality 1 (batch, seq_len_1, hidden_dim)
            x2: Features from modality 2 (batch, seq_len_2, hidden_dim)
            
        Returns:
            out1, out2: Updated features for each modality
            attn_weights_1_to_2: Attention weights (batch, heads, seq_len_1, seq_len_2)
            attn_weights_2_to_1: Attention weights (batch, heads, seq_len_2, seq_len_1)
        """
        # Use average_attn_weights=False to get per-head weights
        attn_output_1, attn_weights_1_to_2 = self.cross_attn_1_to_2(
            query=x1, key=x2, value=x2,
            average_attn_weights=False
        )
        x1 = self.norm1(x1 + self.dropout(attn_output_1))

        attn_output_2, attn_weights_2_to_1 = self.cross_attn_2_to_1(
            query=x2, key=x1, value=x1,
            average_attn_weights=False
        )
        x2 = self.norm2(x2 + self.dropout(attn_output_2))

        # Feed-forward networks and residual connections
        ffn_output1 = self.ffn(x1)
        x1 = self.norm3(x1 + self.dropout(ffn_output1))
        
        ffn_output2 = self.ffn(x2)
        x2 = self.norm3(x2 + self.dropout(ffn_output2))

        return x1, x2, attn_weights_1_to_2, attn_weights_2_to_1


class SimplifiedTransformer(nn.Module):
    """Simplified Transformer model with feature-level cross-attention."""
    def __init__(self, spectral_dim, metabolite_dim, hidden_dim, num_heads, 
                 num_layers, num_classes, dropout=0.1):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_classes = num_classes
        
        # Feature embeddings (convert each feature to hidden dimension)
        self.spectral_feature_embedding = nn.Linear(1, hidden_dim)
        self.metabolite_feature_embedding = nn.Linear(1, hidden_dim)
        
        # Positional encodings (learnable)
        self.pos_encoding_spec = nn.Parameter(torch.randn(1, spectral_dim, hidden_dim) * 0.02)
        self.pos_encoding_metab = nn.Parameter(torch.randn(1, metabolite_dim, hidden_dim) * 0.02)
        
        # Layer normalization and dropout
        self.embedding_norm_spec = nn.LayerNorm(hidden_dim)
        self.embedding_norm_metab = nn.LayerNorm(hidden_dim)
        self.embedding_dropout = nn.Dropout(dropout)
        
        # Cross-attention layers
        self.cross_attention_layers = nn.ModuleList([
            CrossAttentionLayer(hidden_dim, num_heads, dropout) 
            for _ in range(num_layers)
        ])
        
        # Multi-task classification heads
        self.output_heads = nn.ModuleDict()
        total_pooled_dim = hidden_dim * 2
        for task_name, n_class in num_classes.items():
            self.output_heads[task_name] = nn.Sequential(
                nn.LayerNorm(total_pooled_dim),
                nn.Linear(total_pooled_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(hidden_dim, n_class)
            )
        
        # Storage for attention weights
        self.attention_weights = {}

    def forward(self, spectral, metabolite):
        """
        Forward pass through the model.
        
        Args:
            spectral: Spectral features (batch, spectral_dim)
            metabolite: Metabolite features (batch, metabolite_dim)
            
        Returns:
            Dictionary of outputs for each task
        """
        # Reshape inputs for per-feature embeddings
        spec_reshaped = spectral.unsqueeze(-1)
        metab_reshaped = metabolite.unsqueeze(-1)
        
        # Embed each feature
        spec_emb = self.spectral_feature_embedding(spec_reshaped)
        metab_emb = self.metabolite_feature_embedding(metab_reshaped)
        
        # Add positional encodings
        spec_emb = spec_emb + self.pos_encoding_spec
        metab_emb = metab_emb + self.pos_encoding_metab
        
        # Apply normalization and dropout
        spec_emb = self.embedding_dropout(self.embedding_norm_spec(spec_emb))
        metab_emb = self.embedding_dropout(self.embedding_norm_metab(metab_emb))

        # Reset attention weights storage
        self.attention_weights = {}

        # Process through cross-attention layers
        for i, layer in enumerate(self.cross_attention_layers):
            spec_emb, metab_emb, attn_1_to_2, attn_2_to_1 = layer(spec_emb, metab_emb)
            if i == len(self.cross_attention_layers) - 1:
                # Store attention weights from final layer
                self.attention_weights = {'1_to_2': attn_1_to_2, '2_to_1': attn_2_to_1}

        # Global pooling across features
        spec_pooled = spec_emb.mean(dim=1)
        metab_pooled = metab_emb.mean(dim=1)
        
        # Concatenate pooled representations
        combined_pooled = torch.cat([spec_pooled, metab_pooled], dim=1)
        
        # Apply task-specific heads
        outputs = {}
        for task_name, head in self.output_heads.items():
            outputs[task_name] = head(combined_pooled)
            
        return outputs


# ===== TRAINING FUNCTIONS =====
def train_one_epoch(model, dataloader, optimizer, criterion, device, target_cols):
    """
    Train the model for one epoch.
    
    Args:
        model: The model to train
        dataloader: DataLoader for training data
        optimizer: The optimizer
        criterion: Loss function
        device: Device to train on (CPU/GPU)
        target_cols: List of target column names
        
    Returns:
        avg_loss: Average loss over the epoch
        epoch_metrics: Dictionary of metrics for each task
    """
    model.train()
    total_loss = 0.0
    all_preds = {task: [] for task in target_cols}
    all_targets = {task: [] for task in target_cols}
    
    for batch in dataloader:
        spectral = batch['spectral'].to(device)
        metab = batch['metabolite'].to(device)
        targets = batch['targets'].to(device)
        
        optimizer.zero_grad()
        outputs = model(spectral, metab)
        
        loss = 0
        for task_idx, task_name in enumerate(target_cols):
            loss += criterion(outputs[task_name], targets[:, task_idx])
            
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        total_loss += loss.item()
        
        with torch.no_grad():
            for task_idx, task_name in enumerate(target_cols):
                preds = torch.argmax(outputs[task_name], dim=1)
                all_preds[task_name].extend(preds.cpu().numpy())
                all_targets[task_name].extend(targets[:, task_idx].cpu().numpy())
    
    avg_loss = total_loss / len(dataloader)
    epoch_metrics = {}
    
    for task_name in target_cols:
        y_true = all_targets[task_name]
        y_pred = all_preds[task_name]
        accuracy = accuracy_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)
        precision = precision_score(y_true, y_pred, average='macro', zero_division=0)
        recall = recall_score(y_true, y_pred, average='macro', zero_division=0)
        epoch_metrics[task_name] = {
            'accuracy': accuracy,
            'f1': f1,
            'precision': precision,
            'recall': recall
        }
    
    return avg_loss, epoch_metrics


# ===== EVALUATION FUNCTION =====
def evaluate(model, dataloader, criterion, device, target_cols):
    """
    Evaluate the model and collect attention weights.
    
    Args:
        model: The model to evaluate
        dataloader: DataLoader for evaluation data
        criterion: Loss function
        device: Device to evaluate on (CPU/GPU)
        target_cols: List of target column names
        
    Returns:
        avg_loss: Average loss over the dataset
        eval_metrics: Dictionary of metrics for each task
        preds_df: DataFrame of predictions
        targets_df: DataFrame of true targets
        final_attention: Dictionary of attention weights
    """
    model.eval()
    total_loss = 0.0
    all_preds = {task: [] for task in target_cols}
    all_targets = {task: [] for task in target_cols}
    all_sample_ids = []
    raw_attention_batches = {'1_to_2': [], '2_to_1': []}  # Store weights batch by batch on CPU

    with torch.no_grad():
        for batch_idx, batch in enumerate(dataloader):
            spectral = batch['spectral'].to(device)
            metab = batch['metabolite'].to(device)
            targets = batch['targets'].to(device)
            sample_ids = batch['sample_id']
            
            outputs = model(spectral, metab)
            
            loss = 0
            for task_idx, task_name in enumerate(target_cols):
                loss += criterion(outputs[task_name], targets[:, task_idx])
            
            total_loss += loss.item()
            
            for task_idx, task_name in enumerate(target_cols):
                preds = torch.argmax(outputs[task_name], dim=1)
                all_preds[task_name].extend(preds.cpu().numpy())
                all_targets[task_name].extend(targets[:, task_idx].cpu().numpy())
                
            all_sample_ids.extend(sample_ids)

            # Collect attention weights if available
            if hasattr(model, 'attention_weights') and model.attention_weights and '1_to_2' in model.attention_weights:
                attn_1_to_2_batch = model.attention_weights['1_to_2'].detach().cpu()
                logger.info(f"Evaluate Batch {batch_idx}: Raw S->M attention shape: {attn_1_to_2_batch.shape}")
                raw_attention_batches['1_to_2'].append(attn_1_to_2_batch)

                if '2_to_1' in model.attention_weights:
                    attn_2_to_1_batch = model.attention_weights['2_to_1'].detach().cpu()
                    logger.info(f"Evaluate Batch {batch_idx}: Raw M->S attention shape: {attn_2_to_1_batch.shape}")
                    raw_attention_batches['2_to_1'].append(attn_2_to_1_batch)

    # Calculate average loss and evaluation metrics
    avg_loss = total_loss / len(dataloader)
    eval_metrics = {}
    
    for task_name in target_cols:
        y_true = all_targets[task_name]
        y_pred = all_preds[task_name]
        accuracy = accuracy_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)
        precision = precision_score(y_true, y_pred, average='macro', zero_division=0)
        recall = recall_score(y_true, y_pred, average='macro', zero_division=0)
        eval_metrics[task_name] = {
            'accuracy': accuracy,
            'f1': f1,
            'precision': precision,
            'recall': recall
        }

    # Concatenate attention weights from batches
    final_attention = None
    if raw_attention_batches['1_to_2']:
        logger.info(f"Evaluate: Attempting to concatenate {len(raw_attention_batches['1_to_2'])} S->M attention batches.")
        try:
            # Concatenate along batch dim (dim=0)
            final_attn_1_to_2 = torch.cat(raw_attention_batches['1_to_2'], dim=0)
            
            # Check shape after concatenation
            if len(final_attn_1_to_2.shape) != 4:
                logger.error(f"Evaluate: Concatenated S->M attention has wrong shape: {final_attn_1_to_2.shape}. Expected 4D.")
                final_attention = None
            else:
                final_attn_2_to_1 = None
                if raw_attention_batches['2_to_1']:
                    logger.info(f"Evaluate: Attempting to concatenate {len(raw_attention_batches['2_to_1'])} M->S attention batches.")
                    final_attn_2_to_1 = torch.cat(raw_attention_batches['2_to_1'], dim=0)
                    if len(final_attn_2_to_1.shape) != 4:
                        logger.error(f"Evaluate: Concatenated M->S attention has wrong shape: {final_attn_2_to_1.shape}. Expected 4D.")
                        final_attn_2_to_1 = None

                final_attention = {'1_to_2': final_attn_1_to_2}
                if final_attn_2_to_1 is not None:
                    final_attention['2_to_1'] = final_attn_2_to_1

                log_s2m_shape = final_attn_1_to_2.shape
                log_m2s_shape = final_attn_2_to_1.shape if final_attn_2_to_1 is not None else "N/A"
                logger.info(f"Concatenated final attention tensors. S->M shape: {log_s2m_shape}, M->S shape: {log_m2s_shape}")

        except Exception as e_cat:
            logger.error(f"Could not concatenate attention batches: {e_cat}. Attention analysis will be skipped.", exc_info=True)
            final_attention = None
    else:
        logger.warning("Evaluate: No attention weights collected in batches.")
        final_attention = None

    # Create dataframes from predictions and targets
    preds_df = pd.DataFrame(all_preds, index=all_sample_ids)
    targets_df = pd.DataFrame(all_targets, index=all_sample_ids)
    
    # Ensure index name is consistent
    preds_df.index.name = 'Row_names'
    targets_df.index.name = 'Row_names'
    
    return avg_loss, eval_metrics, preds_df, targets_df, final_attention


# ===== ANALYSIS FUNCTIONS =====
def analyze_attention_feature_level(attention_weights, spectral_feature_names, metabolite_feature_names,
                                   output_dir, pairing, top_k_pairs=200):
    """
    Analyzes feature-level attention weights, saves summaries to CSV,
    and saves tensors and feature names to HDF5.
    
    Args:
        attention_weights: Dictionary of attention weight tensors
        spectral_feature_names: List of spectral feature names
        metabolite_feature_names: List of metabolite feature names
        output_dir: Directory to save outputs
        pairing: Analysis pairing (e.g., "Leaf" or "Root")
        top_k_pairs: Number of top attention pairs to save
        
    Returns:
        cross_modal_pairs_df: DataFrame of cross-modal feature pairs
        feature_importance_df: DataFrame of feature importance scores
    """
    logger.info("--- Starting Feature-Level Attention Analysis (HDF5 Tensors/Features + Summary CSVs) ---")
    
    # Input validation
    if attention_weights is None or '1_to_2' not in attention_weights or \
       not isinstance(attention_weights['1_to_2'], torch.Tensor) or \
       attention_weights['1_to_2'].numel() == 0:
        logger.warning("Attention weights invalid/empty. Skipping HDF5/CSV summary saving.")
        return None, None

    cross_modal_pairs_df, feature_importance_df = None, None  # Initialize

    try:
        # Get the 4D tensor from evaluate()
        attn_s2m_raw = attention_weights['1_to_2']  # Shape: (N, H, N_spec, N_metab)

        # Dimension validation
        n_samples, n_heads, n_spec, n_metab = attn_s2m_raw.shape

        if n_spec != len(spectral_feature_names):
            logger.error("Spectral dimension mismatch.")
            return None, None
        if n_metab != len(metabolite_feature_names):
            logger.error("Metabolite dimension mismatch.")
            return None, None
            
        logger.info(f"Validated attention tensor shape for HDF5: {(n_samples, n_heads, n_spec, n_metab)}")

        # Calculate summaries (average over heads and samples)
        logger.info("Calculating attention summaries (pairs, importance)...")
        attn_s2m_avg_heads_samples = attn_s2m_raw.mean(dim=(0, 1)).numpy()  # Avg over samples & heads

        # 1. Cross-Modal Pairs
        pairs = []
        for i in range(n_spec):
            for j in range(n_metab):
                pairs.append({
                    'Spectral_Feature': spectral_feature_names[i],
                    'Metabolite_Feature': metabolite_feature_names[j],
                    'Mean_Attention_S2M_AvgHeadsSamples': attn_s2m_avg_heads_samples[i, j]
                })
        cross_modal_pairs_df = pd.DataFrame(pairs).sort_values('Mean_Attention_S2M_AvgHeadsSamples', ascending=False)
        logger.info(f"Calculated cross-modal pairs summary (Top {top_k_pairs} pairs).")

        # 2. Feature Importance
        spec_importance_mag = attn_s2m_avg_heads_samples.sum(axis=1)
        metab_importance_mag = attn_s2m_avg_heads_samples.sum(axis=0)
        importance_list = []
        for i, name in enumerate(spectral_feature_names):
            importance_list.append({
                'Feature': name,
                'View': 'Spectral',
                'Importance_Magnitude_S2M_AvgHeadsSamples': spec_importance_mag[i]
            })
        for j, name in enumerate(metabolite_feature_names):
            importance_list.append({
                'Feature': name,
                'View': 'Metabolite',
                'Importance_Magnitude_S2M_AvgHeadsSamples': metab_importance_mag[j]
            })
        feature_importance_df = pd.DataFrame(importance_list).sort_values('Importance_Magnitude_S2M_AvgHeadsSamples', ascending=False)
        logger.info(f"Calculated feature importance summary ({len(feature_importance_df)} features).")

        # Save summaries to CSV files
        pairs_outfile = os.path.join(output_dir, f"transformer_cross_modal_pairs_{pairing}.csv")
        importance_outfile = os.path.join(output_dir, f"transformer_feature_importance_{pairing}.csv")
        try:
            cross_modal_pairs_df.head(top_k_pairs).to_csv(pairs_outfile, index=False)
            logger.info(f"Saved cross-modal pairs summary CSV to: {pairs_outfile}")
            if not os.path.exists(pairs_outfile):
                logger.error(f"CSV file not found after saving: {pairs_outfile}")
        except Exception as e_csv:
            logger.error(f"Failed to save pairs CSV: {e_csv}", exc_info=True)
            
        try:
            feature_importance_df.to_csv(importance_outfile, index=False)
            logger.info(f"Saved feature importance summary CSV to: {importance_outfile}")
            if not os.path.exists(importance_outfile):
                logger.error(f"CSV file not found after saving: {importance_outfile}")
        except Exception as e_csv:
            logger.error(f"Failed to save importance CSV: {e_csv}", exc_info=True)

        # Save tensors and features to HDF5
        logger.info("Saving raw tensors and features to HDF5...")
        attention_output_path = os.path.join(output_dir, f"raw_attention_data_{pairing}.h5")
        try:
            with h5py.File(attention_output_path, 'w') as f:
                logger.info(f"Opened HDF5 file for writing: {attention_output_path}")

                # 1. Save attention tensors
                logger.info("Saving attention tensors...")
                try:
                    attn_s2m_np = attn_s2m_raw.numpy()
                    f.create_dataset('attention_spec_to_metab', data=attn_s2m_np, compression="gzip")
                    logger.info(f"Saved attention_spec_to_metab (Shape: {attn_s2m_np.shape}).")
                except Exception as e:
                    logger.error(f"Failed to save S->M attention: {e}", exc_info=True)

                # Save M->S attention if available
                if '2_to_1' in attention_weights and attention_weights['2_to_1'] is not None:
                    try:
                        # Get the M->S tensor
                        attn_m2s_raw = attention_weights['2_to_1']

                        # Validate M->S tensor shape
                        n_samples_m2s, n_heads_m2s, n_metab_m2s, n_spec_m2s = attn_m2s_raw.shape
                        valid_m2s_shape = True
                        if n_samples_m2s != n_samples:
                            logger.error(f"M->S Sample count ({n_samples_m2s}) mismatch with S->M ({n_samples}). Cannot save M->S.")
                            valid_m2s_shape = False
                        if n_metab_m2s != len(metabolite_feature_names):
                            logger.error(f"M->S Metabolite dim mismatch: Tensor={n_metab_m2s}, Features={len(metabolite_feature_names)}. Cannot save M->S.")
                            valid_m2s_shape = False
                        if n_spec_m2s != len(spectral_feature_names):
                            logger.error(f"M->S Spectral dim mismatch: Tensor={n_spec_m2s}, Features={len(spectral_feature_names)}. Cannot save M->S.")
                            valid_m2s_shape = False

                        if valid_m2s_shape:
                            attn_m2s_np = attn_m2s_raw.numpy()
                            f.create_dataset('attention_metab_to_spec', data=attn_m2s_np, compression="gzip")
                            logger.info(f"Saved attention_metab_to_spec (Shape: {attn_m2s_np.shape}).")
                        else:
                            logger.error("Skipping save of attention_metab_to_spec due to shape validation errors.")

                    except Exception as e:
                        logger.error(f"Failed to save M->S attention ('attention_metab_to_spec'): {e}", exc_info=True)
                else:
                    logger.warning("'2_to_1' (M->S) attention weights not found or None. Skipping save of 'attention_metab_to_spec'.")

                # 2. Save feature names
                logger.info("Saving feature names...")
                try:
                    f.create_dataset('spectral_feature_names', 
                                     data=np.array(spectral_feature_names, dtype=h5py.string_dtype(encoding='utf-8')))
                    f.create_dataset('metabolite_feature_names', 
                                     data=np.array(metabolite_feature_names, dtype=h5py.string_dtype(encoding='utf-8')))
                    logger.info("Saved feature name datasets.")
                except Exception as e:
                    logger.error(f"Failed to save feature names: {e}", exc_info=True)

            # Final verification
            logger.info(f"Successfully completed writing to HDF5 file: {attention_output_path}")
            if os.path.exists(attention_output_path):
                logger.info(f"Verified HDF5 file exists: {attention_output_path}")
            else:
                logger.error(f"HDF5 saving reported success, but file not found: {attention_output_path}")

        except OSError as e_os:
            logger.error(f"Failed to save data to HDF5 (OSError): {e_os}", exc_info=True)
        except Exception as e_h5:
            logger.error(f"Failed to save data to HDF5 (Other Error): {e_h5}", exc_info=True)

    except Exception as e_outer:
        logger.error(f"Error during overall feature-level attention analysis: {e_outer}", exc_info=True)
        return None, None

    logger.info("--- Feature-Level Attention Analysis Finished ---")
    # Return the calculated summary DataFrames
    return cross_modal_pairs_df, feature_importance_df


# ===== VISUALIZATION FUNCTIONS (REMOVED / COMMENTED OUT) =====
# def plot_attention_heatmap(...):
#     logger.info("--- Visualization skipped ---")
# def plot_final_visualizations(...):
#     logger.info("--- Visualization skipped ---")

# ===== BASELINE MODELS =====
def run_baseline_models(X_train_spec, X_train_metab, y_train, X_test_spec, X_test_metab, 
                        y_test, target_cols, output_dir, pairing, random_seed):
    """
    Train and evaluate baseline models for comparison.
    
    Args:
        X_train_spec: Training spectral features
        X_train_metab: Training metabolite features
        y_train: Training targets
        X_test_spec: Test spectral features
        X_test_metab: Test metabolite features
        y_test: Test targets
        target_cols: List of target column names
        output_dir: Directory to save results
        pairing: Analysis pairing (e.g., "Leaf" or "Root")
        random_seed: Random seed for reproducibility
        
    Returns:
        baseline_df: DataFrame with baseline model results
    """
    logger.info("--- Running Baseline Models ---")
    baseline_results = []
    
    # Combine features for baseline models
    X_train_combined = pd.concat([X_train_spec, X_train_metab], axis=1)
    X_test_combined = pd.concat([X_test_spec, X_test_metab], axis=1)
    logger.info(f"Combined feature shape for baselines: Train={X_train_combined.shape}, Test={X_test_combined.shape}")
    
    try:
        # Random Forest
        logger.info("Training Random Forest...")
        rf_preds_dict = {}
        for task_name in target_cols:
            rf_task = RandomForestClassifier(
                n_estimators=200, 
                random_state=random_seed,
                n_jobs=-1,
                max_depth=20,
                min_samples_leaf=5
            )
            rf_task.fit(X_train_combined, y_train[task_name])
            rf_preds_dict[task_name] = rf_task.predict(X_test_combined)
            
        logger.info("Evaluating Random Forest...")
        for task_name in target_cols:
            task_target = y_test[task_name]
            task_preds = rf_preds_dict[task_name]
            accuracy = accuracy_score(task_target, task_preds)
            f1 = f1_score(task_target, task_preds, average='macro', zero_division=0)
            baseline_results.append({
                'Model': 'RandomForest',
                'Task': task_name,
                'Metric': 'Accuracy',
                'Score': accuracy
            })
            baseline_results.append({
                'Model': 'RandomForest',
                'Task': task_name,
                'Metric': 'F1_Macro',
                'Score': f1
            })
    except Exception as e:
        logger.error(f"Error running RandomForest baseline: {e}")
        traceback.print_exc()
        
    try:
        # KNN
        logger.info("Training K-Nearest Neighbors (KNN)...")
        knn_preds_dict = {}
        n_neighbors_val = 5
        for task_name in target_cols:
            knn_task = KNeighborsClassifier(
                n_neighbors=n_neighbors_val,
                n_jobs=-1,
                weights='distance'
            )
            knn_task.fit(X_train_combined, y_train[task_name])
            knn_preds_dict[task_name] = knn_task.predict(X_test_combined)
            
        logger.info("Evaluating K-Nearest Neighbors...")
        for task_name in target_cols:
            task_target = y_test[task_name]
            task_preds = knn_preds_dict[task_name]
            accuracy = accuracy_score(task_target, task_preds)
            f1 = f1_score(task_target, task_preds, average='macro', zero_division=0)
            baseline_results.append({
                'Model': f'KNN (k={n_neighbors_val})',
                'Task': task_name,
                'Metric': 'Accuracy',
                'Score': accuracy
            })
            baseline_results.append({
                'Model': f'KNN (k={n_neighbors_val})',
                'Task': task_name,
                'Metric': 'F1_Macro',
                'Score': f1
            })
    except Exception as e:
        logger.error(f"Error running KNN baseline: {e}")
        traceback.print_exc()
        
    # Save baseline results
    if baseline_results:
        baseline_df = pd.DataFrame(baseline_results)
        outfile = os.path.join(output_dir, f"transformer_baseline_comparison_{pairing}.csv")
        baseline_df.to_csv(outfile, index=False)
        logger.info(f"Baseline comparison results saved to {outfile}")
    else:
        logger.warning("No baseline results generated.")
        baseline_df = None
        
    logger.info("--- Baseline Models Finished ---")
    return baseline_df


# ===== MAIN EXECUTION =====
def main():
    """Main execution function."""
    start_time = time.time()
    logger.info(f"--- Starting Main Execution ({SCRIPT_NAME} v{VERSION}) ---")
    
    # Set random seeds for reproducibility
    np.random.seed(RANDOM_SEED)
    torch.manual_seed(RANDOM_SEED)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(RANDOM_SEED)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

    try:
        # Load and preprocess data
        (train_loader, val_loader, test_loader,
         spectral_feature_names, metabolite_feature_names,
         train_ids, val_ids, test_ids,
         scalers, label_encoders,
         X_train_spec_scaled_df, X_val_spec_scaled_df, X_test_spec_scaled_df,
         X_train_metab_scaled_df, X_val_metab_scaled_df, X_test_metab_scaled_df,
         y_train_df, y_val_df, y_test_df,
         test_metadata_df
         ) = load_and_preprocess_data(config=globals())
    except Exception as e:
        logger.error(f"Failed during data loading. Exiting. Error: {e}", exc_info=True)
        return

    # Initialize model, loss, and optimizer
    logger.info("Initializing model, loss, and optimizer...")
    spectral_dim = len(spectral_feature_names)
    metabolite_dim = len(metabolite_feature_names)
    
    model = SimplifiedTransformer(
        spectral_dim=spectral_dim,
        metabolite_dim=metabolite_dim,
        hidden_dim=HIDDEN_DIM,
        num_heads=NUM_HEADS,
        num_layers=NUM_LAYERS,
        num_classes=NUM_CLASSES,
        dropout=DROPOUT
    ).to(DEVICE)
    
    logger.info(f"Model initialized (Feature Dims: Spec={spectral_dim}, Metab={metabolite_dim})")
    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f"Total trainable parameters: {total_params:,}")
    
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)

    # Training phase
    logger.info("--- Starting Training ---")
    best_val_loss = float('inf')
    epochs_no_improve = 0
    training_start_time = time.time()
    
    pairing_checkpoint_dir = os.path.join(CHECKPOINT_SUBDIR, ANALYSIS_PAIRING)
    os.makedirs(pairing_checkpoint_dir, exist_ok=True)
    best_model_path = os.path.join(pairing_checkpoint_dir, f"best_model_{SCRIPT_NAME}_{ANALYSIS_PAIRING}.pth")

    for epoch in range(1, EPOCHS + 1):
        logger.info(f"Epoch {epoch}/{EPOCHS}")
        
        # Train for one epoch
        train_loss, train_metrics = train_one_epoch(
            model, train_loader, optimizer, criterion, DEVICE, TARGET_COLS
        )
        
        # Validate
        val_loss, val_metrics, _, _, val_attention = evaluate(
            model, val_loader, criterion, DEVICE, TARGET_COLS
        )
        
        # Log metrics
        logger.info(f"Epoch {epoch}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}")
        for task in TARGET_COLS:
            logger.info(f"  {task}: Train Acc={train_metrics[task]['accuracy']:.4f}, "
                       f"Val Acc={val_metrics[task]['accuracy']:.4f} | "
                       f"Train F1={train_metrics[task]['f1']:.4f}, "
                       f"Val F1={val_metrics[task]['f1']:.4f}")

        # Check for improvement
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            epochs_no_improve = 0
            torch.save(model.state_dict(), best_model_path)
            logger.info(f"Val loss improved. Saved best model to {best_model_path}")
        else:
            epochs_no_improve += 1
            logger.info(f"Val loss did not improve for {epochs_no_improve} epochs.")
            
        # Early stopping
        if epochs_no_improve >= EARLY_STOPPING_PATIENCE:
            logger.info(f"Early stopping triggered after {epoch} epochs.")
            break
            
    training_duration = time.time() - training_start_time
    logger.info(f"--- Training Finished --- Duration: {training_duration:.2f} seconds")

    # Evaluation phase
    logger.info("--- Evaluating on Test Set using Best Model ---")
    if os.path.exists(best_model_path):
        model.load_state_dict(torch.load(best_model_path))
        logger.info(f"Loaded best model from {best_model_path}")
    else:
        logger.warning("Best model checkpoint not found. Evaluating with the last state.")
        
    # Evaluate on test set
    test_loss, test_metrics, test_preds_df, test_targets_df, test_attention = evaluate(
        model, test_loader, criterion, DEVICE, TARGET_COLS
    )
    
    # Log test metrics
    logger.info(f"Test Loss: {test_loss:.4f}")
    performance_data = []
    for task in TARGET_COLS:
        logger.info(f"  Test Metrics for {task}: "
                  f"Acc={test_metrics[task]['accuracy']:.4f}, "
                  f"F1={test_metrics[task]['f1']:.4f}, "
                  f"Prec={test_metrics[task]['precision']:.4f}, "
                  f"Rec={test_metrics[task]['recall']:.4f}")
        
        performance_data.append({
            'Task': task, 'Metric': 'Accuracy', 'Score': test_metrics[task]['accuracy']
        })
        performance_data.append({
            'Task': task, 'Metric': 'F1_Macro', 'Score': test_metrics[task]['f1']
        })
        performance_data.append({
            'Task': task, 'Metric': 'Precision_Macro', 'Score': test_metrics[task]['precision']
        })
        performance_data.append({
            'Task': task, 'Metric': 'Recall_Macro', 'Score': test_metrics[task]['recall']
        })
        
    # Save performance metrics
    performance_df = pd.DataFrame(performance_data)
    perf_outfile = os.path.join(RESULTS_SUBDIR, f"transformer_class_performance_{ANALYSIS_PAIRING}.csv")
    performance_df.to_csv(perf_outfile, index=False)
    logger.info(f"Transformer test performance metrics saved to {perf_outfile}")

    # Prepare data for saving
    logger.info("Preparing test predictions, metadata, and attention for saving...")
    test_preds_df_renamed = test_preds_df.rename(columns={col: f"{col}_pred_encoded" for col in TARGET_COLS})
    test_targets_df_renamed = test_targets_df.rename(columns={col: f"{col}_true_encoded" for col in TARGET_COLS})

    # Align metadata with predictions/targets
    test_metadata_aligned = None
    if isinstance(test_metadata_df, pd.DataFrame) and not test_metadata_df.empty:
        try:
            # Ensure index names match if necessary
            if test_metadata_df.index.name != test_preds_df_renamed.index.name:
                logger.warning(f"Renaming test_metadata_df index to match predictions: '{test_preds_df_renamed.index.name}'")
                test_metadata_df = test_metadata_df.rename_axis(test_preds_df_renamed.index.name)
                
            # Align using the index (Row_names)
            test_metadata_aligned = test_metadata_df.reindex(test_preds_df_renamed.index)
            if test_metadata_aligned.isnull().any(axis=None):
                logger.warning("NaNs found in aligned metadata. Check if all test samples had metadata.")
                
            logger.info(f"Successfully aligned test metadata ({test_metadata_aligned.shape}) with predictions/targets.")
        except Exception as e_reidx:
            logger.error(f"Failed to align metadata with predictions: {e_reidx}. Cannot proceed with saving combined results or validated attention.", exc_info=True)
            test_metadata_aligned = None
    else:
        logger.error("Test metadata DataFrame is invalid or empty. Cannot proceed.")
        test_metadata_aligned = None

    # Validate alignment before saving
    logger.info("--- Validating Alignment Before Saving ---")
    validation_passed = False
    if test_attention is not None and '1_to_2' in test_attention and \
       isinstance(test_attention['1_to_2'], torch.Tensor) and \
       test_metadata_aligned is not None:

        n_samples_tensor = test_attention['1_to_2'].shape[0]
        n_samples_metadata = len(test_metadata_aligned)

        if n_samples_tensor == n_samples_metadata:
            logger.info(f"Validation PASSED: Tensor samples ({n_samples_tensor}) match metadata rows ({n_samples_metadata}).")
            validation_passed = True
        else:
            logger.error(f"VALIDATION FAILED: Tensor samples ({n_samples_tensor}) DO NOT match metadata rows ({n_samples_metadata})!")
            logger.error("Cannot guarantee alignment between HDF5 tensors and metadata file. Aborting further saving steps that depend on this alignment.")
    else:
        logger.error("VALIDATION SKIPPED: Attention tensors or aligned metadata are missing/invalid.")
        if test_attention is None or '1_to_2' not in test_attention:
            logger.error("Reason: Attention data missing or invalid.")
        if test_metadata_aligned is None:
            logger.error("Reason: Aligned metadata is missing or invalid (check alignment process).")

    # Save combined predictions + metadata CSV
    if test_metadata_aligned is not None:
        logger.info("Saving test predictions and true labels with metadata...")
        
        # Include the Row_names index in the CSV
        test_results_combined_df = pd.concat([test_metadata_aligned, test_preds_df_renamed, test_targets_df_renamed], axis=1)
        results_outfile = os.path.join(RESULTS_SUBDIR, f"transformer_test_predictions_metadata_{ANALYSIS_PAIRING}.csv")
        
        try:
            test_results_combined_df.to_csv(results_outfile, index=True, index_label='Row_names')
            logger.info(f"Test predictions, true labels, and metadata saved to CSV: {results_outfile}")
        except Exception as e_csv:
            logger.error(f"Failed to save predictions+metadata CSV: {e_csv}", exc_info=True)
    else:
        logger.warning("Skipping save of combined predictions+metadata CSV due to missing/unaligned metadata.")

    # Save attention data and separate metadata (only if validation passed)
    if validation_passed:
        logger.info("--- Saving Attention Tensors (HDF5) and Separate Metadata File ---")

        # Save metadata separately
        if test_metadata_aligned.index.name != 'Row_names':
            test_metadata_aligned = test_metadata_aligned.rename_axis('Row_names')

        metadata_outfile_feather = os.path.join(RESULTS_SUBDIR, f"raw_attention_metadata_{ANALYSIS_PAIRING}.feather")
        try:
            # Feather requires index to be reset or non-named for standard saving
            test_metadata_aligned.reset_index().to_feather(metadata_outfile_feather)
            logger.info(f"Test metadata saved separately to Feather file: {metadata_outfile_feather}")
        except ImportError:
            logger.warning("`pyarrow` not installed. Falling back to saving metadata as CSV.")
            metadata_outfile_csv = os.path.join(RESULTS_SUBDIR, f"raw_attention_metadata_{ANALYSIS_PAIRING}.csv")
            try:
                test_metadata_aligned.to_csv(metadata_outfile_csv, index=True, index_label='Row_names')
                logger.info(f"Test metadata saved separately to CSV file: {metadata_outfile_csv}")
            except Exception as e_csv_meta:
                logger.error(f"Failed to save metadata to CSV: {e_csv_meta}", exc_info=True)
        except Exception as e_feather:
            logger.error(f"Failed to save metadata to Feather: {e_feather}", exc_info=True)

        # Call attention analysis
        logger.info("Calling feature-level attention analysis (HDF5/Summary CSV saving)...")
        cross_modal_df, feat_importance_df = analyze_attention_feature_level(
            attention_weights=test_attention,
            spectral_feature_names=spectral_feature_names,
            metabolite_feature_names=metabolite_feature_names,
            output_dir=RESULTS_SUBDIR,
            pairing=ANALYSIS_PAIRING
        )
    else:
        logger.warning("Skipping saving of HDF5 attention data and separate metadata due to validation failure or missing data.")

    # Run baseline models for comparison
    run_baseline_models(
        X_train_spec_scaled_df, X_train_metab_scaled_df, y_train_df,
        X_test_spec_scaled_df, X_test_metab_scaled_df, y_test_df,
        TARGET_COLS, RESULTS_SUBDIR, ANALYSIS_PAIRING, RANDOM_SEED
    )

    logger.info("--- Visualization Phase Skipped ---")

    # Finish execution
    total_duration = time.time() - start_time
    logger.info(f"--- Main Execution Finished --- Total Duration: {total_duration / 60:.2f} minutes ---")
    logger.info("="*60)


# --- Entry Point ---
if __name__ == '__main__':
    try:
        main()
    except Exception as e:
        logger.error(f"An error occurred during main execution: {e}", exc_info=True)
        sys.exit(1)

--------------------------------------------------

train_transformer_knn.py
# -*- coding: utf-8 -*-
"""
Transformer Model for Multi-Omic Plant Stress Response Analysis

This script implements a Transformer-based model for analyzing multi-omic plant stress response data.
It takes spectral and metabolite feature sets as input and uses a Transformer architecture with 
cross-attention mechanisms to perform multi-task classification (Genotype, Treatment, Day) while
extracting cross-modal attention patterns between different feature types.

Key capabilities:
1. Multi-task classification on plant stress response variables
2. Cross-modal attention analysis between spectral and metabolite features
3. Feature importance calculation based on attention mechanisms
4. Comparative performance evaluation against baseline models (Random Forest, KNN)

The model architecture incorporates cross-attention layers that enable bidirectional
information flow between the different omics data types, providing insights into 
their interactions and relative importance in prediction tasks.
"""

# ===== IMPORTS =====
import os
import sys
import time
import logging
import argparse
from datetime import datetime
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import traceback
from scipy.stats import mannwhitneyu
from statsmodels.stats.multitest import multipletests

# ===== CONFIGURATION =====
# --- Script Info ---
SCRIPT_NAME = "transformer_multi_omic_v1"
VERSION = "1.0.0_Skeleton"

# --- Analysis Pairing ---
ANALYSIS_PAIRING = "Leaf"  # Options: "Leaf" or "Root"

# --- Paths ---
BASE_DIR = r"C:/Users/ms/Desktop/hyper"
OUTPUT_DIR = os.path.join(BASE_DIR, "output", "transformer")
MOFA_OUTPUT_DIR = os.path.join(BASE_DIR, "output", "mofa")
CODE_DIR = os.path.join(BASE_DIR, "analysis")

# Construct input file paths dynamically based on ANALYSIS_PAIRING
INPUT_FILES = {
    "Leaf": {
        "spectral": os.path.join(MOFA_OUTPUT_DIR, "transformer_input_leaf_spectral.csv"),
        "metabolite": os.path.join(MOFA_OUTPUT_DIR, "transformer_input_leaf_metabolite.csv"),
    },
    "Root": {
        "spectral": os.path.join(MOFA_OUTPUT_DIR, "transformer_input_root_spectral.csv"),
        "metabolite": os.path.join(MOFA_OUTPUT_DIR, "transformer_input_root_metabolite.csv"),
    }
}

# --- Data & Columns ---
METADATA_COLS = ['Row_names', 'Vac_id', 'Genotype', 'Entry', 'Tissue.type',
                 'Batch', 'Treatment', 'Replication', 'Day']
TARGET_COLS = ['Genotype', 'Treatment', 'Day']

# --- Model Hyperparameters ---
HIDDEN_DIM = 64
NUM_HEADS = 4
NUM_LAYERS = 2
DROPOUT = 0.1
NUM_CLASSES = {
    'Genotype': 2,
    'Treatment': 2,
    'Day': 3
}

# --- Training Hyperparameters ---
LEARNING_RATE = 1e-5
BATCH_SIZE = 32
EPOCHS = 100
EARLY_STOPPING_PATIENCE = 10
WEIGHT_DECAY = 1e-5

# --- Data Handling ---
VAL_SIZE = 0.15
TEST_SIZE = 0.15
NUM_WORKERS = 0
RANDOM_SEED = 42

# --- Target Encoding ---
ENCODING_MAPS = {
    'Genotype': {'G1': 0, 'G2': 1},
    'Treatment': {0: 0, 1: 1},
    'Day': {1: 0, 2: 1, 3: 2}
}

# --- Device ---
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ===== LOGGING =====
def setup_logging(log_dir: str, script_name: str, version: str) -> logging.Logger:
    """
    Sets up file and console logging.
    
    Args:
        log_dir: Directory to store log files
        script_name: Name of the script for log file naming
        version: Version string for log file naming
        
    Returns:
        Logger object configured for both file and console output
    """
    os.makedirs(log_dir, exist_ok=True)
    log_filename = f"{script_name}_{version}_{datetime.now():%Y%m%d_%H%M%S}.log"
    log_filepath = os.path.join(log_dir, log_filename)

    log_format = '%(asctime)s - %(levelname)s - [%(module)s:%(lineno)d] - %(message)s'
    date_format = '%Y-%m-%d %H:%M:%S'
    formatter = logging.Formatter(log_format, datefmt=date_format)

    logger = logging.getLogger()
    logger.setLevel(logging.INFO)

    if logger.hasHandlers():
        logger.handlers.clear()

    file_handler = logging.FileHandler(log_filepath)
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)

    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

    def handle_exception(exc_type, exc_value, exc_traceback):
        if issubclass(exc_type, KeyboardInterrupt):
            sys.__excepthook__(exc_type, exc_value, exc_traceback)
            return
        logger.error("Uncaught exception", exc_info=(exc_type, exc_value, exc_traceback))

    sys.excepthook = handle_exception

    logger.info(f"Logging setup complete. Log file: {log_filepath}")
    return logger

# Initialize logger (call this early)
logger = setup_logging(os.path.join(OUTPUT_DIR, "logs"), SCRIPT_NAME, VERSION)

logger.info("="*60)
logger.info(f"Starting Script: {SCRIPT_NAME} v{VERSION}")
logger.info(f"Analysis Pairing: {ANALYSIS_PAIRING}")
logger.info(f"Output Directory: {OUTPUT_DIR}")
logger.info(f"Input Directory (MOFA Output): {MOFA_OUTPUT_DIR}")
logger.info(f"Using Device: {DEVICE}")
logger.info(f"Batch Size: {BATCH_SIZE}, Num Workers: {NUM_WORKERS}")
logger.info(f"Model Params: Hidden Dim={HIDDEN_DIM}, Heads={NUM_HEADS}, Layers={NUM_LAYERS}")
logger.info("="*60)


# ===== DATA LOADING & PREPROCESSING =====

class PlantOmicsDataset(Dataset):
    """PyTorch Dataset for paired spectral and metabolite data."""
    
    def __init__(self, spectral_features, metabolite_features, targets):
        """
        Args:
            spectral_features (pd.DataFrame): Scaled spectral features (samples x features).
            metabolite_features (pd.DataFrame): Scaled metabolite features (samples x features).
            targets (pd.DataFrame): Encoded target labels (samples x num_targets).
        """
        if not spectral_features.index.equals(metabolite_features.index) or \
           not spectral_features.index.equals(targets.index):
            raise ValueError("Indices of spectral, metabolite, and target dataframes do not match!")

        # Convert to tensors
        self.spectral_data = torch.tensor(spectral_features.values, dtype=torch.float32)
        self.metabolite_data = torch.tensor(metabolite_features.values, dtype=torch.float32)
        # Ensure targets are Long type for CrossEntropyLoss
        self.targets = torch.tensor(targets.values, dtype=torch.long)

        self.sample_ids = spectral_features.index.tolist()  # Keep track of sample IDs

    def __len__(self):
        return len(self.spectral_data)

    def __getitem__(self, idx):
        """Returns a dictionary for clarity."""
        return {
            'spectral': self.spectral_data[idx],
            'metabolite': self.metabolite_data[idx],
            'targets': self.targets[idx],  # Shape: (num_targets,)
            'sample_id': self.sample_ids[idx]  # Include sample ID if needed later
        }

def load_and_preprocess_data(config: dict) -> tuple:
    """
    Loads, aligns, preprocesses, splits, and scales the data for the specified pairing.
    
    Args:
        config: Dictionary containing configuration parameters
        
    Returns:
        tuple: Contains dataloaders, feature names, sample IDs, scalers, encoders,
               scaled dataframes, target dataframes, and full metadata
    """
    pairing = config['ANALYSIS_PAIRING']
    logger.info(f"--- Starting Data Loading & Preprocessing for Pairing: {pairing} ---")

    # --- 1. Load Data ---
    spectral_path = config['INPUT_FILES'][pairing]['spectral']
    metabolite_path = config['INPUT_FILES'][pairing]['metabolite']
    try:
        logger.info(f"Loading spectral data from: {spectral_path}")
        df_spectral_raw = pd.read_csv(spectral_path, index_col='Row_names', na_values='NA')
        logger.info(f"Loaded spectral data shape: {df_spectral_raw.shape}")
        logger.info(f"First 5 spectral indices: {df_spectral_raw.index[:5].tolist()}")

        logger.info(f"Loading metabolite data from: {metabolite_path}")
        df_metabolite_raw = pd.read_csv(metabolite_path, index_col='Row_names', na_values='NA')
        logger.info(f"Loaded metabolite data shape: {df_metabolite_raw.shape}")
        logger.info(f"First 5 metabolite indices: {df_metabolite_raw.index[:5].tolist()}")

    except FileNotFoundError as e:
        logger.error(f"Input file not found: {e}")
        raise
    except KeyError as e:
        logger.error(f"Column 'Row_names' not found in one of the input files: {e}. Check CSV generation.")
        raise
    except Exception as e:
        logger.error(f"Error loading data files: {e}")
        traceback.print_exc()
        raise


    # --- 2. Align Data (using index) ---
    # Now the indices should be the actual Row_names
    common_indices = df_spectral_raw.index.intersection(df_metabolite_raw.index)
    if len(common_indices) == 0:
        logger.error("No common Row_names found between spectral and metabolite files after loading.")
        raise ValueError("Alignment failed: No common Row_names.")
    elif len(common_indices) < len(df_spectral_raw.index) or len(common_indices) < len(df_metabolite_raw.index):
        logger.warning(f"Found {len(common_indices)} common samples out of "
                       f"{len(df_spectral_raw.index)} (spectral) and {len(df_metabolite_raw.index)} (metabolite). "
                       f"Proceeding with common samples only.")
        # Keep only common samples
        df_spectral_raw = df_spectral_raw.loc[common_indices]
        df_metabolite_raw = df_metabolite_raw.loc[common_indices]
    else:
        logger.info(f"Data aligned by Row_names index. Number of samples: {len(common_indices)}")


    # --- 3. Identify Columns & Separate ---
    meta_cols = config['METADATA_COLS']
    target_cols = config['TARGET_COLS']

    # Ensure 'Row_names' is handled correctly as it's now the index
    meta_cols_to_extract = [col for col in meta_cols if col != 'Row_names' and col in df_spectral_raw.columns]

    # Check if all metadata columns exist (excluding Row_names which is index)
    if not all(col in df_spectral_raw.columns for col in meta_cols_to_extract) or \
       not all(col in df_metabolite_raw.columns for col in meta_cols_to_extract):
        logger.warning(f"Missing one or more metadata columns in input files (checked: {meta_cols_to_extract}). Check METADATA_COLS.")
        # Allow proceeding if some meta cols are missing, but targets must exist

    # Separate metadata (use one source, assumed identical after alignment) and features
    # -----> FIX 1: Rename 'metadata' to 'full_metadata_df' <-----
    full_metadata_df = df_spectral_raw[meta_cols_to_extract].copy()
    # Keep index named correctly (should be 'Row_names' from load)
    if full_metadata_df.index.name != 'Row_names':
        logger.warning(f"Index name of full_metadata_df is '{full_metadata_df.index.name}', expected 'Row_names'.")
    # full_metadata_df['Row_names'] = full_metadata_df.index # Optional: Add as column if needed elsewhere

    features_spectral_df = df_spectral_raw.drop(columns=meta_cols_to_extract, errors='ignore') # errors='ignore' in case some meta cols were missing
    spectral_feature_names = features_spectral_df.columns.tolist()
    logger.info(f"Identified {len(spectral_feature_names)} spectral features.")

    features_metabolite_df = df_metabolite_raw.drop(columns=meta_cols_to_extract, errors='ignore')
    metabolite_feature_names = features_metabolite_df.columns.tolist()
    logger.info(f"Identified {len(metabolite_feature_names)} metabolite features.")

    # --- 4. Target Encoding ---
    logger.info("Encoding target variables...")
    # -----> Use the renamed variable 'full_metadata_df' <-----
    targets_encoded = pd.DataFrame(index=full_metadata_df.index)
    label_encoders = {}
    
    # --- DEBUG START ---
    if 'Treatment' in full_metadata_df.columns:
        # Convert to string and strip whitespace just in case, then find unique
        unique_treatments = full_metadata_df['Treatment'].astype(str).str.strip().unique()
        logger.info(f"Unique values found in 'Treatment' column BEFORE encoding: {unique_treatments}")
    else:
         logger.warning("Debugging: 'Treatment' column not found in metadata just before encoding loop.")
    # --- DEBUG END ---
    
    # Make sure target columns exist in the extracted metadata
    missing_targets = [col for col in target_cols if col not in full_metadata_df.columns]
    if missing_targets:
         logger.error(f"Target columns {missing_targets} not found in extracted full_metadata_df.")
         raise ValueError(f"Missing target columns: {missing_targets}")

    for col in target_cols:
        if col in config['ENCODING_MAPS']:
            logger.info(f"  Applying predefined map for '{col}'.")
            targets_encoded[col] = full_metadata_df[col].map(config['ENCODING_MAPS'][col])
            le = LabelEncoder()
            # Ensure classes_ are set in the order corresponding to 0, 1, 2... encoding
            sorted_items = sorted(config['ENCODING_MAPS'][col].items(), key=lambda item: item[1])
            le.classes_ = np.array([item[0] for item in sorted_items])
            label_encoders[col] = le
        else:
            logger.warning(f"  Encoding map not found for '{col}'. Using LabelEncoder.")
            le = LabelEncoder()
            targets_encoded[col] = le.fit_transform(full_metadata_df[col])
            label_encoders[col] = le
            logger.info(f"    '{col}' classes: {le.classes_} -> {le.transform(le.classes_)}")
        if targets_encoded[col].isnull().any():
             nan_indices = targets_encoded[targets_encoded[col].isnull()].index.tolist()
             logger.error(f"NaN values found in encoded target '{col}' for samples: {nan_indices}. "
                          f"Original values: {full_metadata_df.loc[nan_indices, col].unique()}. "
                          f"Check ENCODING_MAPS and original data.")
             raise ValueError(f"Encoding failed for target '{col}'.")
    logger.info("Target encoding complete.")

    # --- 5. Stratified Train/Validation/Test Split ---
    logger.info("Performing stratified train/validation/test split...")
    # -----> Use the renamed variable 'full_metadata_df' <-----
    try:
        full_metadata_df['stratify_key'] = full_metadata_df[target_cols[0]].astype(str)
        for col in target_cols[1:]:
            full_metadata_df['stratify_key'] += '_' + full_metadata_df[col].astype(str)
    except KeyError as e:
         logger.error(f"Stratification key column missing in full_metadata_df: {e}")
         raise

    indices = full_metadata_df.index
    stratify_values = full_metadata_df['stratify_key']

    # ... (NaN check in stratify_values) ...
    if stratify_values.isnull().any():
        logger.warning("NaN values found in stratification key. Stratification might be affected.")

    # ... (train_test_split calls using indices, stratify_values) ...
    try:
        train_idx, temp_idx = train_test_split(
            indices,
            test_size=(config['VAL_SIZE'] + config['TEST_SIZE']),
            random_state=config['RANDOM_SEED'],
            stratify=stratify_values
        )
    except ValueError as e:
         logger.warning(f"Could not stratify fully (possibly due to small groups or NaN in key): {e}. Proceeding without full stratification guarantee.")
         train_idx, temp_idx = train_test_split(
            indices,
            test_size=(config['VAL_SIZE'] + config['TEST_SIZE']),
            random_state=config['RANDOM_SEED'] # Fallback to non-stratified split
         )

    relative_test_size = config['TEST_SIZE'] / (config['VAL_SIZE'] + config['TEST_SIZE'])
    temp_stratify_values = stratify_values.loc[temp_idx]
    # ... (NaN check in temp_stratify_values) ...
    if temp_stratify_values.isnull().any():
        logger.warning("NaN values found in stratification key for temp set. Val/Test stratification might be affected.")

    try:
         val_idx, test_idx = train_test_split(
             temp_idx,
             test_size=relative_test_size,
             random_state=config['RANDOM_SEED'],
             stratify=temp_stratify_values
         )
    except ValueError as e:
         logger.warning(f"Could not stratify Temp set fully (possibly due to small groups or NaN in key): {e}. Proceeding without full stratification guarantee.")
         val_idx, test_idx = train_test_split(
             temp_idx,
             test_size=relative_test_size,
             random_state=config['RANDOM_SEED'] # Fallback to non-stratified split
         )

    X_train_spec = features_spectral_df.loc[train_idx]
    X_val_spec = features_spectral_df.loc[val_idx]
    X_test_spec = features_spectral_df.loc[test_idx]

    X_train_metab = features_metabolite_df.loc[train_idx]
    X_val_metab = features_metabolite_df.loc[val_idx]
    X_test_metab = features_metabolite_df.loc[test_idx]

    # -----> Use targets_encoded which was created using full_metadata_df index <-----
    y_train = targets_encoded.loc[train_idx]
    y_val = targets_encoded.loc[val_idx]
    y_test = targets_encoded.loc[test_idx]

    train_meta_ids = train_idx.tolist()
    val_meta_ids = val_idx.tolist()
    test_meta_ids = test_idx.tolist()

    logger.info(f"Split sizes: Train={len(train_idx)}, Validation={len(val_idx)}, Test={len(test_idx)}")

    # --- NEW DEBUG BLOCK ---
    logger.info("--- Debugging X_train_metab BEFORE Scaling ---")
    # Check for NaNs
    nan_counts_metab = X_train_metab.isnull().sum()
    nan_cols_metab = nan_counts_metab[nan_counts_metab > 0]
    if not nan_cols_metab.empty:
        logger.warning(f"NaNs found in X_train_metab BEFORE scaling in {len(nan_cols_metab)} columns.")
        logger.warning(f"Columns with NaNs (and counts):\n{nan_cols_metab}")
    else:
        logger.info("No NaNs found in X_train_metab before scaling.")

    # Check for Infs and non-numeric types
    inf_cols_metab = []
    non_numeric_cols_metab = []
    for col in X_train_metab.columns:
        if pd.api.types.is_numeric_dtype(X_train_metab[col]):
            if np.isinf(X_train_metab[col]).any():
                inf_cols_metab.append(col)
        else:
            non_numeric_cols_metab.append(col)

    if non_numeric_cols_metab:
        logger.warning(f"Non-numeric dtypes found in X_train_metab BEFORE scaling in {len(non_numeric_cols_metab)} columns: {non_numeric_cols_metab}")
        logger.warning("  These columns cannot be checked for Infs or scaled directly.")

    if inf_cols_metab:
        logger.warning(f"Infs found in X_train_metab BEFORE scaling in {len(inf_cols_metab)} columns: {inf_cols_metab}")
    elif not non_numeric_cols_metab: # Only log 'No Infs' if no non-numeric columns were found either
        logger.info("No Infs found in numeric columns of X_train_metab before scaling.")

    logger.info("--- End Debugging X_train_metab ---")
    # --- END NEW DEBUG BLOCK ---

    # --- 6. Scaling (Fit on Train only) ---
    logger.info("Scaling features (fitting on training data only)...")
    scaler_spec = StandardScaler()
    X_train_spec_scaled = scaler_spec.fit_transform(X_train_spec)
    X_val_spec_scaled = scaler_spec.transform(X_val_spec)
    X_test_spec_scaled = scaler_spec.transform(X_test_spec)

    scaler_metab = StandardScaler()
    X_train_metab_scaled = scaler_metab.fit_transform(X_train_metab)
    X_val_metab_scaled = scaler_metab.transform(X_val_metab)
    X_test_metab_scaled = scaler_metab.transform(X_test_metab)

    # --- Add this check after scaling ---
    logger.info("Checking for NaNs/Infs in scaled training data...")
    if np.isnan(X_train_spec_scaled).any() or np.isinf(X_train_spec_scaled).any():
         logger.error("NaNs or Infs found in SCALED spectral training data!")
         raise ValueError("Bad values in scaled spectral data")
    if np.isnan(X_train_metab_scaled).any() or np.isinf(X_train_metab_scaled).any():
         logger.error("NaNs or Infs found in SCALED metabolite training data!")
         raise ValueError("Bad values in scaled metabolite data")
    logger.info("No NaNs/Infs found in scaled training data.")
    # --- End check ---

    X_train_spec_scaled_df = pd.DataFrame(X_train_spec_scaled, index=train_idx, columns=spectral_feature_names)
    X_val_spec_scaled_df = pd.DataFrame(X_val_spec_scaled, index=val_idx, columns=spectral_feature_names)
    X_test_spec_scaled_df = pd.DataFrame(X_test_spec_scaled, index=test_idx, columns=spectral_feature_names)

    X_train_metab_scaled_df = pd.DataFrame(X_train_metab_scaled, index=train_idx, columns=metabolite_feature_names)
    X_val_metab_scaled_df = pd.DataFrame(X_val_metab_scaled, index=val_idx, columns=metabolite_feature_names)
    X_test_metab_scaled_df = pd.DataFrame(X_test_metab_scaled, index=test_idx, columns=metabolite_feature_names)

    scalers = {'spectral': scaler_spec, 'metabolite': scaler_metab}
    logger.info("Feature scaling complete.")

    # --- 7. Create Datasets and DataLoaders ---
    logger.info("Creating PyTorch Datasets and DataLoaders...")
    train_dataset = PlantOmicsDataset(X_train_spec_scaled_df, X_train_metab_scaled_df, y_train)
    val_dataset = PlantOmicsDataset(X_val_spec_scaled_df, X_val_metab_scaled_df, y_val)
    test_dataset = PlantOmicsDataset(X_test_spec_scaled_df, X_test_metab_scaled_df, y_test)

    train_loader = DataLoader(train_dataset, batch_size=config['BATCH_SIZE'], shuffle=True, num_workers=config['NUM_WORKERS'], pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=config['BATCH_SIZE'], shuffle=False, num_workers=config['NUM_WORKERS'], pin_memory=True)
    test_loader = DataLoader(test_dataset, batch_size=config['BATCH_SIZE'], shuffle=False, num_workers=config['NUM_WORKERS'], pin_memory=True)
    logger.info("DataLoaders created.")

    logger.info("--- Data Loading & Preprocessing Finished ---")

    # -----> FIX 2: Add 'full_metadata_df' to the return statement <-----
    return (train_loader, val_loader, test_loader,
            spectral_feature_names, metabolite_feature_names,
            train_meta_ids, val_meta_ids, test_meta_ids,
            scalers, label_encoders,
            X_train_spec_scaled_df, X_val_spec_scaled_df, X_test_spec_scaled_df,
            X_train_metab_scaled_df, X_val_metab_scaled_df, X_test_metab_scaled_df,
            y_train, y_val, y_test, # Return encoded target splits
            full_metadata_df # <--- ADD THIS
            )


# ===== MODEL DEFINITION =====
class CrossAttentionLayer(nn.Module):
    """Implements cross-attention between two modalities."""
    
    def __init__(self, hidden_dim, num_heads, dropout=0.1):
        super().__init__()
        # Attention from modality 1 (query) to modality 2 (key/value)
        self.cross_attn_1_to_2 = nn.MultiheadAttention(
            hidden_dim, num_heads, dropout=dropout, batch_first=True
        )
        # Attention from modality 2 (query) to modality 1 (key/value)
        self.cross_attn_2_to_1 = nn.MultiheadAttention(
            hidden_dim, num_heads, dropout=dropout, batch_first=True
        )

        self.norm1 = nn.LayerNorm(hidden_dim)
        self.norm2 = nn.LayerNorm(hidden_dim)
        self.dropout = nn.Dropout(dropout)

        self.ffn = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 2, hidden_dim)
        )
        self.norm3 = nn.LayerNorm(hidden_dim)

    def forward(self, x1, x2):
        """
        Processes inputs through cross-attention mechanisms.
        
        Args:
            x1: Features from modality 1 (batch, seq_len=1, hidden_dim)
            x2: Features from modality 2 (batch, seq_len=1, hidden_dim)
            
        Returns:
            out1, out2: Updated features for each modality
            attn_weights_1_to_2: Attention weights from modality 1 to 2
            attn_weights_2_to_1: Attention weights from modality 2 to 1
        """
        # Cross-Attention: x1 attends to x2
        attn_output_1, attn_weights_1_to_2 = self.cross_attn_1_to_2(x1, x2, x2)
        x1 = self.norm1(x1 + self.dropout(attn_output_1))  # Residual connection + Norm

        # Cross-Attention: x2 attends to x1
        attn_output_2, attn_weights_2_to_1 = self.cross_attn_2_to_1(x2, x1, x1)
        x2 = self.norm2(x2 + self.dropout(attn_output_2))  # Residual connection + Norm

        # Apply FFN to both
        ffn_output1 = self.ffn(x1)
        x1 = self.norm3(x1 + self.dropout(ffn_output1))

        ffn_output2 = self.ffn(x2)
        x2 = self.norm3(x2 + self.dropout(ffn_output2))

        return x1, x2, attn_weights_1_to_2, attn_weights_2_to_1


class SimplifiedTransformer(nn.Module):
    """Simplified Transformer model for multi-omic classification."""
    
    def __init__(self, spectral_dim, metabolite_dim, hidden_dim, num_heads, 
                 num_layers, num_classes, dropout=0.1):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_classes = num_classes  # Dict: {'Genotype': 2, 'Treatment': 2, 'Day': 3}

        # Input Embedding layers (Linear projection)
        self.spectral_embedding = nn.Linear(spectral_dim, hidden_dim)
        self.metabolite_embedding = nn.Linear(metabolite_dim, hidden_dim)

        # Simple Positional Encoding (Learned parameter) - Add to embeddings
        self.pos_encoding_spec = nn.Parameter(torch.randn(1, 1, hidden_dim) * 0.02)
        self.pos_encoding_metab = nn.Parameter(torch.randn(1, 1, hidden_dim) * 0.02)

        self.embedding_norm_spec = nn.LayerNorm(hidden_dim)
        self.embedding_norm_metab = nn.LayerNorm(hidden_dim)
        self.embedding_dropout = nn.Dropout(dropout)

        # Stack of Cross-Attention Layers
        self.cross_attention_layers = nn.ModuleList([
            CrossAttentionLayer(hidden_dim, num_heads, dropout) for _ in range(num_layers)
        ])

        # Multi-task Classification Heads
        self.output_heads = nn.ModuleDict()
        total_feature_dim = hidden_dim * 2  # Concatenate outputs of both modalities

        for task_name, n_class in num_classes.items():
            self.output_heads[task_name] = nn.Sequential(
                nn.LayerNorm(total_feature_dim),
                nn.Linear(total_feature_dim, hidden_dim // 2),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(hidden_dim // 2, n_class)
            )

        self.attention_weights = []  # To store attention weights during forward pass

    def forward(self, spectral, metabolite):
        """
        Forward pass through the model.
        
        Args:
            spectral: Spectral input features (batch, spectral_dim)
            metabolite: Metabolite input features (batch, metabolite_dim)
            
        Returns:
            dict: Dictionary mapping task names to prediction outputs
        """
        # 1. Embed inputs
        spec_emb = self.spectral_embedding(spectral)
        metab_emb = self.metabolite_embedding(metabolite)

        # 2. Add positional encoding and reshape for attention
        spec_emb = spec_emb.unsqueeze(1) + self.pos_encoding_spec
        metab_emb = metab_emb.unsqueeze(1) + self.pos_encoding_metab

        # Apply Norm and Dropout after pos enc
        spec_emb = self.embedding_dropout(self.embedding_norm_spec(spec_emb))
        metab_emb = self.embedding_dropout(self.embedding_norm_metab(metab_emb))

        # 3. Pass through Cross-Attention Layers
        self.attention_weights = []  # Clear previous weights
        all_attn_1_to_2 = []
        all_attn_2_to_1 = []
        for i, layer in enumerate(self.cross_attention_layers):
            spec_emb, metab_emb, attn_1_to_2, attn_2_to_1 = layer(spec_emb, metab_emb)
            # Store attention weights (e.g., from the last layer for analysis)
            if i == len(self.cross_attention_layers) - 1:
                self.attention_weights = {'1_to_2': attn_1_to_2, '2_to_1': attn_2_to_1}
            # Optionally store all layer weights
            all_attn_1_to_2.append(attn_1_to_2)
            all_attn_2_to_1.append(attn_2_to_1)

        # 4. Prepare for Classification Head
        spec_out = spec_emb.squeeze(1)
        metab_out = metab_emb.squeeze(1)

        # Concatenate features from both modalities
        combined_features = torch.cat([spec_out, metab_out], dim=1)

        # 5. Multi-task Classification
        outputs = {}
        for task_name, head in self.output_heads.items():
            outputs[task_name] = head(combined_features)

        return outputs


# ===== TRAINING FUNCTIONS =====
def train_one_epoch(model, dataloader, optimizer, criterion, device, target_cols):
    """
    Trains the model for one epoch.
    
    Args:
        model: Neural network model to train
        dataloader: DataLoader containing training data
        optimizer: Optimizer for updating model weights
        criterion: Loss function
        device: Device to run computations on (CPU/GPU)
        target_cols: List of target column names
        
    Returns:
        tuple: (average_loss, metrics_dict) for the epoch
    """
    model.train()
    total_loss = 0.0
    all_preds = {task: [] for task in target_cols}
    all_targets = {task: [] for task in target_cols}

    for i, batch in enumerate(dataloader):
        # Move data to device
        spectral = batch['spectral'].to(device)
        metabolite = batch['metabolite'].to(device)
        targets = batch['targets'].to(device)  # Shape: (batch, num_targets)

        # Zero gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(spectral, metabolite)  # Dict of task outputs

        # Calculate loss (sum across tasks)
        loss = 0
        for task_idx, task_name in enumerate(target_cols):
            task_output = outputs[task_name]
            task_target = targets[:, task_idx]  # Get the correct target column
            loss += criterion(task_output, task_target)

        # Backward pass and optimization
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients
        optimizer.step()

        total_loss += loss.item()

        # Store predictions and targets for metric calculation later
        with torch.no_grad():
            for task_idx, task_name in enumerate(target_cols):
                task_output = outputs[task_name]
                task_target = targets[:, task_idx]
                preds = torch.argmax(task_output, dim=1)
                all_preds[task_name].extend(preds.cpu().numpy())
                all_targets[task_name].extend(task_target.cpu().numpy())

        # Log progress periodically
        if (i + 1) % (len(dataloader) // 5) == 0:  # Log ~5 times per epoch
            logger.info(f"  Batch {i+1}/{len(dataloader)}, Loss: {loss.item():.4f}")

    avg_loss = total_loss / len(dataloader)

    # Calculate epoch metrics
    epoch_metrics = {}
    for task_name in target_cols:
        accuracy = accuracy_score(all_targets[task_name], all_preds[task_name])
        # Use macro avg for f1/precision/recall as classes might be imbalanced
        f1 = f1_score(all_targets[task_name], all_preds[task_name], average='macro', zero_division=0)
        precision = precision_score(all_targets[task_name], all_preds[task_name], average='macro', zero_division=0)
        recall = recall_score(all_targets[task_name], all_preds[task_name], average='macro', zero_division=0)
        epoch_metrics[task_name] = {'accuracy': accuracy, 'f1': f1, 'precision': precision, 'recall': recall}

    return avg_loss, epoch_metrics


def evaluate(model, dataloader, criterion, device, target_cols):
    """
    Evaluates the model on the validation or test set.
    
    Args:
        model: Neural network model to evaluate
        dataloader: DataLoader containing evaluation data
        criterion: Loss function
        device: Device to run computations on (CPU/GPU)
        target_cols: List of target column names
        
    Returns:
        tuple: (average_loss, metrics_dict, predictions_df, targets_df, attention_weights)
    """
    model.eval()
    total_loss = 0.0
    all_preds = {task: [] for task in target_cols}
    all_targets = {task: [] for task in target_cols}
    all_sample_ids = []
    raw_attention_weights = {'1_to_2': [], '2_to_1': []}  # For analysis

    with torch.no_grad():
        for batch in dataloader:
            spectral = batch['spectral'].to(device)
            metabolite = batch['metabolite'].to(device)
            targets = batch['targets'].to(device)
            sample_ids = batch['sample_id']  # List of IDs in the batch

            outputs = model(spectral, metabolite)

            loss = 0
            for task_idx, task_name in enumerate(target_cols):
                task_output = outputs[task_name]
                task_target = targets[:, task_idx]
                loss += criterion(task_output, task_target)

            total_loss += loss.item()

            for task_idx, task_name in enumerate(target_cols):
                task_output = outputs[task_name]
                task_target = targets[:, task_idx]
                preds = torch.argmax(task_output, dim=1)
                all_preds[task_name].extend(preds.cpu().numpy())
                all_targets[task_name].extend(task_target.cpu().numpy())

            all_sample_ids.extend(sample_ids)

            # Store attention weights from the last layer (if available)
            if hasattr(model, 'attention_weights') and model.attention_weights:
                # Detach and move to CPU to avoid memory leaks
                raw_attention_weights['1_to_2'].append(model.attention_weights['1_to_2'].detach().cpu())
                raw_attention_weights['2_to_1'].append(model.attention_weights['2_to_1'].detach().cpu())

    avg_loss = total_loss / len(dataloader)

    eval_metrics = {}
    for task_name in target_cols:
        accuracy = accuracy_score(all_targets[task_name], all_preds[task_name])
        f1 = f1_score(all_targets[task_name], all_preds[task_name], average='macro', zero_division=0)
        precision = precision_score(all_targets[task_name], all_preds[task_name], average='macro', zero_division=0)
        recall = recall_score(all_targets[task_name], all_preds[task_name], average='macro', zero_division=0)
        eval_metrics[task_name] = {'accuracy': accuracy, 'f1': f1, 'precision': precision, 'recall': recall}

    # Concatenate attention weights from all batches
    if raw_attention_weights['1_to_2']:
        # weights are (batch, num_heads, query_len, key_len) - here query/key len is 1
        # Concatenate along batch dimension (dim 0)
        final_attn_1_to_2 = torch.cat(raw_attention_weights['1_to_2'], dim=0)  # Shape (N_samples, H, 1, 1)
        final_attn_2_to_1 = torch.cat(raw_attention_weights['2_to_1'], dim=0)  # Shape (N_samples, H, 1, 1)
        final_attention = {'1_to_2': final_attn_1_to_2, '2_to_1': final_attn_2_to_1}
    else:
        final_attention = None

    # Return predictions/targets as dataframes for easier analysis
    preds_df = pd.DataFrame(all_preds, index=all_sample_ids)
    targets_df = pd.DataFrame(all_targets, index=all_sample_ids)

    return avg_loss, eval_metrics, preds_df, targets_df, final_attention

# ===== ANALYSIS FUNCTIONS =====
def analyze_attention_view_level(attention_weights, test_metadata, output_dir, pairing):
    """
    Analyzes view-level attention weights using provided test set metadata.
    
    Calculates average view-to-view scores per sample, merges with metadata,
    and performs group comparisons based on the metadata.

    Args:
        attention_weights (dict): {'1_to_2': tensor(N, H, 1, 1), '2_to_1': tensor(N, H, 1, 1)}
                                  Raw attention weights from the test set.
        test_metadata (pd.DataFrame): DataFrame with metadata variables for test samples
                                      (index=Row_names/sample_id), containing original labels.
        output_dir (str): Directory to save results.
        pairing (str): 'Leaf' or 'Root'.

    Returns:
        tuple: (view_attention_df, grouped_attn_df, stats_df)
               - view_attention_df (pd.DataFrame or None): Per-sample scores merged with metadata.
               - grouped_attn_df (pd.DataFrame or None): Grouped statistics.
               - stats_df (pd.DataFrame or None): Statistical test results.
               Returns (None, None, None) if analysis fails.
    """
    logger.info("--- Starting VIEW-LEVEL Attention Analysis (Using Passed Metadata) ---")
    view_attention_df = None
    grouped_attn = None
    stat_df = None

    # --- Input Validation ---
    if not isinstance(attention_weights, dict) or \
       '1_to_2' not in attention_weights or '2_to_1' not in attention_weights or \
       not isinstance(attention_weights['1_to_2'], torch.Tensor) or \
       not isinstance(attention_weights['2_to_1'], torch.Tensor) or \
       attention_weights['1_to_2'].numel() == 0 or attention_weights['2_to_1'].numel() == 0:
        logger.warning("Attention weights dictionary is invalid, incomplete, or contains empty tensors. Skipping view-level analysis.")
        return None, None, None

    if not isinstance(test_metadata, pd.DataFrame) or test_metadata.empty:
        logger.error("Invalid or empty test_metadata DataFrame provided. Cannot proceed.")
        return None, None, None

    n_samples_attn = attention_weights['1_to_2'].shape[0]
    n_samples_meta = len(test_metadata)
    if n_samples_attn != n_samples_meta:
        logger.error(f"Mismatch between attention weight samples ({n_samples_attn}) and metadata samples ({n_samples_meta}). Cannot proceed.")
        # Ideally, the main function should ensure alignment before calling.
        # If they don't match here, it indicates a problem upstream.
        return None, None, None
    else:
        logger.info(f"Attention weights and metadata aligned for {n_samples_attn} test samples.")

    # --- Calculate Average Attention Scores ---
    try:
        attn_1_to_2_raw = attention_weights['1_to_2'] # Shape (N, H, 1, 1)
        attn_2_to_1_raw = attention_weights['2_to_1'] # Shape (N, H, 1, 1)

        avg_attn_1_to_2_per_sample = attn_1_to_2_raw.mean(dim=1).squeeze().cpu().numpy() # Shape (N,)
        avg_attn_2_to_1_per_sample = attn_2_to_1_raw.mean(dim=1).squeeze().cpu().numpy() # Shape (N,)

        # Ensure numpy arrays are 1D even if N=1
        if avg_attn_1_to_2_per_sample.ndim == 0: avg_attn_1_to_2_per_sample = avg_attn_1_to_2_per_sample.reshape(1)
        if avg_attn_2_to_1_per_sample.ndim == 0: avg_attn_2_to_1_per_sample = avg_attn_2_to_1_per_sample.reshape(1)

        # Create a DataFrame for scores, using the index from test_metadata
        scores_df = pd.DataFrame({
            'AvgAttn_Spec_to_Metab': avg_attn_1_to_2_per_sample,
            'AvgAttn_Metab_to_Spec': avg_attn_2_to_1_per_sample
        }, index=test_metadata.index) # Use the index from the input metadata

        # --- Merge with Passed Metadata ---
        # test_metadata already contains the original labels and correct index
        view_attention_df = test_metadata.join(scores_df, how='inner') # Inner join is safest

        if view_attention_df.empty:
             logger.error("Joining calculated attention scores with test_metadata resulted in an empty DataFrame. Check index alignment.")
             return None, None, None

        # Save per-sample scores with metadata
        outfile_samples = os.path.join(output_dir, f"transformer_view_level_attention_{pairing}.csv")
        view_attention_df.to_csv(outfile_samples, index=True, index_label=view_attention_df.index.name or 'Row_names') # Save with index
        logger.info(f"Saved per-sample view-level attention scores (with metadata) to {outfile_samples}")

        # --- Grouped Analysis ---
        # Use columns directly from view_attention_df (which came from test_metadata)
        grouping_cols_present = [col for col in TARGET_COLS if col in view_attention_df.columns] # Use TARGET_COLS for potential groups
        if not grouping_cols_present:
            logger.warning("No target columns ('Genotype', 'Treatment', 'Day') found in the merged attention data. Skipping grouped analysis.")
        else:
            logger.info(f"Calculating grouped view-level attention statistics by {grouping_cols_present}...")
            grouped_attn = view_attention_df.groupby(grouping_cols_present).agg(
                Mean_Attn_S2M=('AvgAttn_Spec_to_Metab', 'mean'),
                Std_Attn_S2M=('AvgAttn_Spec_to_Metab', 'std'),
                Median_Attn_S2M=('AvgAttn_Spec_to_Metab', 'median'),
                Mean_Attn_M2S=('AvgAttn_Metab_to_Spec', 'mean'),
                Std_Attn_M2S=('AvgAttn_Metab_to_Spec', 'std'),
                Median_Attn_M2S=('AvgAttn_Metab_to_Spec', 'median'),
                N_Samples=('AvgAttn_Spec_to_Metab', 'count')
            ).reset_index()

            grouped_outfile = os.path.join(output_dir, f"transformer_grouped_view_attention_{pairing}.csv")
            grouped_attn.to_csv(grouped_outfile, index=False)
            logger.info(f"Saved grouped view-level attention scores to {grouped_outfile}")

        # --- Statistical Tests (Mann-Whitney U) ---
        logger.info("Performing statistical tests on view-level attention...")
        stat_results = []
        attention_vars = ['AvgAttn_Spec_to_Metab', 'AvgAttn_Metab_to_Spec']

        # (Keep the safe_mwu helper function as defined in your provided code)
        def safe_mwu(group1_vals, group2_vals, var_name, comp_name, g1_name, g2_name):
            # ... (implementation of safe_mwu from your code) ...
            # Remove NaNs before checking length and performing test
            group1_vals = group1_vals.dropna()
            group2_vals = group2_vals.dropna()

            if len(group1_vals) < 3 or len(group2_vals) < 3:
                # logger.debug(f"Skipping MWU for {comp_name} on {var_name}: Insufficient non-NaN samples ({len(group1_vals)} vs {len(group2_vals)}).")
                return None
            try:
                # Use nan_policy='omit' just in case, although we dropped NaNs above
                stat, p_val = mannwhitneyu(group1_vals, group2_vals, alternative='two-sided', nan_policy='omit')
                # Check for p-value of NaN which can happen if input arrays are constant
                if np.isnan(p_val):
                    # Check if arrays are identical or constant
                    if len(group1_vals.unique()) == 1 and len(group2_vals.unique()) == 1 and group1_vals.unique()[0] == group2_vals.unique()[0]:
                         logger.debug(f"MWU for {comp_name} on {var_name}: Both groups identical constant value.")
                         return {'Variable': var_name, 'Comparison': comp_name, 'Group1': g1_name, 'Group2': g2_name, 'Statistic': stat, 'P_value': 1.0, 'Note': 'Identical constant values'}
                    else:
                         logger.warning(f"MWU for {comp_name} on {var_name} resulted in NaN p-value (inputs might be constant or have issues). Stat={stat}")
                         return {'Variable': var_name, 'Comparison': comp_name, 'Group1': g1_name, 'Group2': g2_name, 'Statistic': stat, 'P_value': np.nan, 'Note': 'MWU NaN p-value'}
                return {'Variable': var_name, 'Comparison': comp_name, 'Group1': g1_name, 'Group2': g2_name, 'Statistic': stat, 'P_value': p_val}
            except ValueError as ve:
                 # Handle specific ValueError cases
                 if "identical" in str(ve).lower() or "same distribution" in str(ve).lower():
                     logger.debug(f"MWU ValueError (Identical) for {comp_name} on {var_name}: {ve}")
                     # Check if they are truly identical constants
                     if len(group1_vals.unique()) == 1 and len(group2_vals.unique()) == 1 and group1_vals.unique()[0] == group2_vals.unique()[0]:
                         return {'Variable': var_name, 'Comparison': comp_name, 'Group1': g1_name, 'Group2': g2_name, 'Statistic': np.nan, 'P_value': 1.0, 'Note': 'Identical constant values'}
                     else:
                         return {'Variable': var_name, 'Comparison': comp_name, 'Group1': g1_name, 'Group2': g2_name, 'Statistic': np.nan, 'P_value': 1.0, 'Note': 'Identical distribution reported'}
                 elif "continuity correction" in str(ve).lower():
                      logger.debug(f"MWU continuity correction error for {comp_name} on {var_name}: {ve}. This might indicate many ties.")
                      # Attempt to calculate anyway, but note the issue. P-value might be less reliable.
                      try:
                          stat, p_val = mannwhitneyu(group1_vals, group2_vals, alternative='two-sided', nan_policy='omit', use_continuity=False) # Try without correction
                          logger.warning(f"Retried MWU without continuity correction for {comp_name} on {var_name}. P-value={p_val}")
                          return {'Variable': var_name, 'Comparison': comp_name, 'Group1': g1_name, 'Group2': g2_name, 'Statistic': stat, 'P_value': p_val, 'Note': 'Continuity correction issue (many ties?)'}
                      except Exception as ve_retry:
                          logger.warning(f"Retry MWU failed after continuity error for {comp_name} on {var_name}: {ve_retry}. Setting p=NaN.")
                          return {'Variable': var_name, 'Comparison': comp_name, 'Group1': g1_name, 'Group2': g2_name, 'Statistic': np.nan, 'P_value': np.nan, 'Note': 'Continuity error & retry failed'}
                 else:
                     logger.warning(f"Unhandled Mann-Whitney U ValueError for {comp_name} on {var_name}: {ve}")
                     return {'Variable': var_name, 'Comparison': comp_name, 'Group1': g1_name, 'Group2': g2_name, 'Statistic': np.nan, 'P_value': np.nan, 'Note': 'MWU ValueError'}
            except Exception as e:
                 logger.error(f"Unexpected error during Mann-Whitney U for {comp_name} on {var_name}: {e}")
                 return {'Variable': var_name, 'Comparison': comp_name, 'Group1': g1_name, 'Group2': g2_name, 'Statistic': np.nan, 'P_value': np.nan, 'Note': 'MWU Unexpected Error'}

        # Perform comparisons using columns directly from view_attention_df
        for att_var in attention_vars:
            # Compare Genotypes (overall)
            if 'Genotype' in view_attention_df.columns:
                genotypes = sorted(view_attention_df['Genotype'].dropna().unique()) # Sort for consistency
                if len(genotypes) == 2:
                    g1_vals = view_attention_df[view_attention_df['Genotype'] == genotypes[0]][att_var]
                    g2_vals = view_attention_df[view_attention_df['Genotype'] == genotypes[1]][att_var]
                    result = safe_mwu(g1_vals, g2_vals, att_var, 'Genotype', genotypes[0], genotypes[1])
                    if result: stat_results.append(result)

            # Compare Treatments (overall)
            if 'Treatment' in view_attention_df.columns:
                # Use Treatment column directly (assuming it contains comparable values like 0/1 or 'T0'/'T1')
                # Convert to string just to be safe with potential mixed types (e.g., 0 and '0')
                view_attention_df['Treatment_Str'] = view_attention_df['Treatment'].astype(str)
                treatments = sorted(view_attention_df['Treatment_Str'].dropna().unique())
                if len(treatments) == 2:
                    t0_vals = view_attention_df[view_attention_df['Treatment_Str'] == treatments[0]][att_var]
                    t1_vals = view_attention_df[view_attention_df['Treatment_Str'] == treatments[1]][att_var]
                    result = safe_mwu(t0_vals, t1_vals, att_var, 'Treatment', treatments[0], treatments[1])
                    if result: stat_results.append(result)
                view_attention_df.drop(columns=['Treatment_Str'], inplace=True, errors='ignore') # Clean up

            # Compare Treatment within each Genotype/Day
            if all(col in view_attention_df.columns for col in ['Genotype', 'Day', 'Treatment']):
                 for day in sorted(view_attention_df['Day'].dropna().unique()):
                    for geno in sorted(view_attention_df['Genotype'].dropna().unique()):
                         subset = view_attention_df[(view_attention_df['Day'] == day) & (view_attention_df['Genotype'] == geno)].copy()
                         if subset.empty: continue
                         subset['Treatment_Str'] = subset['Treatment'].astype(str)
                         treatments_sub = sorted(subset['Treatment_Str'].dropna().unique())
                         if len(treatments_sub) == 2:
                             t0_vals_sub = subset[subset['Treatment_Str'] == treatments_sub[0]][att_var]
                             t1_vals_sub = subset[subset['Treatment_Str'] == treatments_sub[1]][att_var]
                             comp_name = f'Treatment_in_G{geno}_D{day}'
                             result = safe_mwu(t0_vals_sub, t1_vals_sub, att_var, comp_name, treatments_sub[0], treatments_sub[1])
                             if result: stat_results.append(result)

        # --- FDR Correction and Saving ---
        if stat_results:
            stat_df = pd.DataFrame(stat_results)
            valid_pvals = stat_df['P_value'].dropna()
            if not valid_pvals.empty:
                try:
                    reject, pvals_corrected, _, _ = multipletests(valid_pvals, alpha=0.05, method='fdr_bh')
                    stat_df.loc[valid_pvals.index, 'P_value_FDR'] = pvals_corrected
                    stat_df.loc[valid_pvals.index, 'Significant_FDR'] = reject
                    # Fill NaNs for rows that didn't have a valid p-value initially or for correction results
                    stat_df['P_value_FDR'] = stat_df['P_value_FDR'] # Already assigned to specific indices
                    stat_df['Significant_FDR'] = stat_df['Significant_FDR'].fillna(False) # Fill NaNs in the boolean column
                except Exception as e_fdr:
                    logger.error(f"Error applying FDR correction: {e_fdr}. Skipping FDR.", exc_info=True)
                    stat_df['P_value_FDR'] = np.nan
                    stat_df['Significant_FDR'] = False
            else:
                stat_df['P_value_FDR'] = np.nan
                stat_df['Significant_FDR'] = False

            stat_outfile = os.path.join(output_dir, f"transformer_view_attention_stats_{pairing}.csv")
            stat_df.sort_values(['Comparison', 'Variable', 'P_value_FDR'], inplace=True, na_position='last')
            stat_df.to_csv(stat_outfile, index=False)
            logger.info(f"Saved view-level attention statistical comparisons to {stat_outfile}")
        else:
             logger.info("No statistical tests performed or yielded valid results for view-level attention.")

    except Exception as e_attn:
        logger.error(f"Error during view-level attention analysis: {e_attn}", exc_info=True)
        # Ensure function returns None tuple on error
        return None, None, None

    logger.info("--- View-Level Attention Analysis Finished ---")
    # Return the potentially generated dataframes
    return view_attention_df, grouped_attn, stat_df

# ===== VISUALIZATION FUNCTIONS =====
def plot_attention_heatmap(attention_weights, output_dir, pairing): # Removed spectral_features, metabolite_features, num_samples
    """ Plots a basic attention distribution for debugging. """ # Updated docstring
    logger.info("--- Generating Basic Attention Distribution Plot ---") # Updated log message
    # Check dictionary and tensor validity
    if not isinstance(attention_weights, dict) or \
       '1_to_2' not in attention_weights or \
       not isinstance(attention_weights['1_to_2'], torch.Tensor) or \
       attention_weights['1_to_2'].numel() == 0:
        logger.warning("Cannot plot attention distribution, attention_weights['1_to_2'] invalid or empty.")
        return

    # Example: Visualize average attention (across heads) from spec to metab
    try:
        # Squeeze is safe here as input is (N, H, 1, 1) -> mean(dim=1) -> (N, 1, 1) -> squeeze -> (N,)
        attn_1_to_2 = attention_weights['1_to_2'].mean(dim=1).squeeze().cpu().numpy()
        # Check if squeeze resulted in 0-dim array (if N=1)
        if attn_1_to_2.ndim == 0:
            attn_1_to_2 = attn_1_to_2.reshape(1) # Ensure it's at least 1D

        logger.info("Plotting distribution of average attention scores (Spectral -> Metabolite).")

        # Plot simple distribution of attention scores
        plt.figure(figsize=(8, 5))
        sns.histplot(attn_1_to_2, bins=30, kde=True) # Directly use the numpy array
        plt.title(f'Distribution of Avg Attention Scores (Spec->Metab) - {pairing}')
        plt.xlabel('Average Attention Score (Spec->Metab)')
        plt.ylabel('Frequency')
        plt.tight_layout() # Adjust layout

        viz_dir = os.path.join(output_dir, "visualizations") # Ensure viz dir exists
        os.makedirs(viz_dir, exist_ok=True)
        outfile = os.path.join(viz_dir, f"debug_attn_distribution_{pairing}.png")

        plt.savefig(outfile)
        plt.close()
        logger.info(f"Saved debug attention distribution plot to {outfile}")

    except Exception as e:
         logger.error(f"Error plotting basic attention distribution: {e}")
         traceback.print_exc()

def plot_final_visualizations(results_dict, output_dir, pairing):
    """ Generates all final publication-style visualizations. """
    logger.info("--- Generating Final Visualizations (Placeholder) ---")
    viz_dir = os.path.join(output_dir, "visualizations")
    os.makedirs(viz_dir, exist_ok=True)

    # Example calls to placeholder plot functions:
    # plot_network(results_dict['cross_modal_pairs'], viz_dir, pairing)
    # plot_heatmap(results_dict['temporal_attention'], viz_dir, pairing)
    # ... other plots from alternative_plan5.txt Section 5 ...

    logger.warning("Final visualization functions are placeholders.")
    logger.info("--- Final Visualizations Finished (Placeholder) ---")


# ===== BASELINE MODELS =====
# Placeholder functions - Implement details in Phase 5
def run_baseline_models(X_train_spec, X_train_metab, y_train,
                        X_test_spec, X_test_metab, y_test,
                        target_cols, output_dir, pairing, random_seed):
    """ Trains and evaluates baseline models (RF, LogReg). """
    logger.info("--- Running Baseline Models ---")

    baseline_results = []

    # Combine features for baselines that don't handle multi-view inherently
    X_train_combined = pd.concat([X_train_spec, X_train_metab], axis=1)
    X_test_combined = pd.concat([X_test_spec, X_test_metab], axis=1)
    logger.info(f"Combined feature shape for baselines: Train={X_train_combined.shape}, Test={X_test_combined.shape}")

    # --- Random Forest ---
    try:
        logger.info("Training Random Forest...")
        from sklearn.ensemble import RandomForestClassifier
        rf_model = RandomForestClassifier(n_estimators=200, random_state=random_seed, n_jobs=-1, max_depth=20, min_samples_leaf=5)
        # Multi-output wrapper not strictly needed if fitting per task, but can be used
        # from sklearn.multioutput import MultiOutputClassifier
        # multi_rf = MultiOutputClassifier(rf_model, n_jobs=-1)
        # multi_rf.fit(X_train_combined, y_train)
        # rf_preds = multi_rf.predict(X_test_combined)
        # rf_preds_df = pd.DataFrame(rf_preds, index=y_test.index, columns=target_cols)

        # Fit per task for clearer metrics
        rf_preds_dict = {}
        for i, task_name in enumerate(target_cols):
             rf_task = RandomForestClassifier(n_estimators=200, random_state=random_seed, n_jobs=-1, max_depth=20, min_samples_leaf=5)
             rf_task.fit(X_train_combined, y_train[task_name])
             rf_preds_dict[task_name] = rf_task.predict(X_test_combined)

        logger.info("Evaluating Random Forest...")
        for task_name in target_cols:
            task_target = y_test[task_name]
            task_preds = rf_preds_dict[task_name]
            accuracy = accuracy_score(task_target, task_preds)
            f1 = f1_score(task_target, task_preds, average='macro', zero_division=0)
            baseline_results.append({'Model': 'RandomForest', 'Task': task_name, 'Metric': 'Accuracy', 'Score': accuracy})
            baseline_results.append({'Model': 'RandomForest', 'Task': task_name, 'Metric': 'F1_Macro', 'Score': f1})

    except Exception as e:
        logger.error(f"Error running RandomForest baseline: {e}")
        traceback.print_exc()


    # --- K-Nearest Neighbors (KNN) --- # Replaced Logistic Regression
    try:
        logger.info("Training K-Nearest Neighbors (KNN)...")
        from sklearn.neighbors import KNeighborsClassifier
        # Need to fit per task
        knn_preds_dict = {}
        for i, task_name in enumerate(target_cols):
            # Initialize KNN (using default k=5 for simplicity, could be tuned)
            knn_task = KNeighborsClassifier(n_neighbors=5, n_jobs=-1) # Use default n_neighbors=5
            knn_task.fit(X_train_combined, y_train[task_name])
            knn_preds_dict[task_name] = knn_task.predict(X_test_combined)

        logger.info("Evaluating K-Nearest Neighbors (KNN)...")
        for task_name in target_cols:
            task_target = y_test[task_name]
            task_preds = knn_preds_dict[task_name]
            accuracy = accuracy_score(task_target, task_preds)
            f1 = f1_score(task_target, task_preds, average='macro', zero_division=0)
            baseline_results.append({'Model': 'KNN', 'Task': task_name, 'Metric': 'Accuracy', 'Score': accuracy})
            baseline_results.append({'Model': 'KNN', 'Task': task_name, 'Metric': 'F1_Macro', 'Score': f1})

    except Exception as e:
        logger.error(f"Error running KNN baseline: {e}")
        traceback.print_exc()

    # --- Save Baseline Results ---
    if baseline_results:
        baseline_df = pd.DataFrame(baseline_results)
        outfile = os.path.join(output_dir, f"transformer_baseline_comparison_{pairing}.csv")
        baseline_df.to_csv(outfile, index=False)
        logger.info(f"Baseline comparison results saved to {outfile}")
    else:
        logger.warning("No baseline results generated.")

    logger.info("--- Baseline Models Finished ---")
    return baseline_df


# ===== MAIN EXECUTION =====
def main():
    """Main execution function that handles the training and evaluation pipeline."""
    start_time = time.time()
    logger.info("--- Starting Main Execution ---")

    # --- Set Seed ---
    np.random.seed(RANDOM_SEED)
    torch.manual_seed(RANDOM_SEED)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(RANDOM_SEED)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

    # --- Load Data ---
    try:
        (train_loader, val_loader, test_loader,
         spectral_feature_names, metabolite_feature_names,
         train_ids, val_ids, test_ids,
         scalers, label_encoders,
         X_train_spec_scaled_df, X_val_spec_scaled_df, X_test_spec_scaled_df,
         X_train_metab_scaled_df, X_val_metab_scaled_df, X_test_metab_scaled_df,
         y_train_df, y_val_df, y_test_df,
         full_metadata_df
         ) = load_and_preprocess_data(config=globals())
    except Exception as e:
        logger.error(f"Failed during data loading. Exiting. Error: {e}", exc_info=True)
        return

    # --- Initialize Model, Loss, Optimizer ---
    logger.info("Initializing model, loss, and optimizer...")
    spectral_dim = len(spectral_feature_names)
    metabolite_dim = len(metabolite_feature_names)

    model = SimplifiedTransformer(
        spectral_dim=spectral_dim,
        metabolite_dim=metabolite_dim,
        hidden_dim=HIDDEN_DIM,
        num_heads=NUM_HEADS,
        num_layers=NUM_LAYERS,
        num_classes=NUM_CLASSES,
        dropout=DROPOUT
    ).to(DEVICE)

    logger.info(f"Model initialized:\n{model}")
    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f"Total trainable parameters: {total_params:,}")

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)

    # --- Training Loop ---
    logger.info("--- Starting Training ---")
    best_val_loss = float('inf')
    epochs_no_improve = 0
    training_start_time = time.time()

    # Create directory for model checkpoints
    checkpoint_dir = os.path.join(OUTPUT_DIR, "checkpoints", ANALYSIS_PAIRING)
    os.makedirs(checkpoint_dir, exist_ok=True)
    best_model_path = os.path.join(checkpoint_dir, f"best_model_{SCRIPT_NAME}_{ANALYSIS_PAIRING}.pth")

    for epoch in range(1, EPOCHS + 1):
        logger.info(f"Epoch {epoch}/{EPOCHS}")

        train_loss, train_metrics = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE, TARGET_COLS)
        val_loss, val_metrics, _, _, _ = evaluate(model, val_loader, criterion, DEVICE, TARGET_COLS)

        logger.info(f"Epoch {epoch}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}")
        # Log metrics per task
        for task in TARGET_COLS:
            logger.info(f"  {task}: Train Acc={train_metrics[task]['accuracy']:.4f}, "
                       f"Val Acc={val_metrics[task]['accuracy']:.4f} | "
                       f"Train F1={train_metrics[task]['f1']:.4f}, "
                       f"Val F1={val_metrics[task]['f1']:.4f}")

        # --- Early Stopping & Checkpointing ---
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            epochs_no_improve = 0
            # Save the best model state
            torch.save(model.state_dict(), best_model_path)
            logger.info(f"Validation loss improved. Saved best model to {best_model_path}")
        else:
            epochs_no_improve += 1
            logger.info(f"Validation loss did not improve for {epochs_no_improve} epochs.")

        if epochs_no_improve >= EARLY_STOPPING_PATIENCE:
            logger.info(f"Early stopping triggered after {epoch} epochs.")
            break

    training_duration = time.time() - training_start_time
    logger.info(f"--- Training Finished --- Duration: {training_duration:.2f} seconds")

    # --- Evaluation on Test Set ---
    logger.info("--- Evaluating on Test Set using Best Model ---")
    if os.path.exists(best_model_path):
        model.load_state_dict(torch.load(best_model_path))
        logger.info(f"Loaded best model from {best_model_path}")
    else:
        logger.warning("Best model checkpoint not found. Evaluating with the last state.")

    test_loss, test_metrics, test_preds_df, test_targets_df, test_attention = evaluate(
        model, test_loader, criterion, DEVICE, TARGET_COLS
    )

    logger.info(f"Test Loss: {test_loss:.4f}")
    performance_data = []
    for task in TARGET_COLS:
        logger.info(f"  Test Metrics for {task}:")
        logger.info(f"    Accuracy: {test_metrics[task]['accuracy']:.4f}")
        logger.info(f"    F1 Macro: {test_metrics[task]['f1']:.4f}")
        logger.info(f"    Precision Macro: {test_metrics[task]['precision']:.4f}")
        logger.info(f"    Recall Macro: {test_metrics[task]['recall']:.4f}")
        performance_data.append({'Task': task, 'Metric': 'Accuracy', 'Score': test_metrics[task]['accuracy']})
        performance_data.append({'Task': task, 'Metric': 'F1_Macro', 'Score': test_metrics[task]['f1']})
        performance_data.append({'Task': task, 'Metric': 'Precision_Macro', 'Score': test_metrics[task]['precision']})
        performance_data.append({'Task': task, 'Metric': 'Recall_Macro', 'Score': test_metrics[task]['recall']})

    # Save test performance metrics
    performance_df = pd.DataFrame(performance_data)
    perf_outfile = os.path.join(OUTPUT_DIR, f"transformer_class_performance_{ANALYSIS_PAIRING}.csv")
    performance_df.to_csv(perf_outfile, index=False)
    logger.info(f"Transformer test performance metrics saved to {perf_outfile}")

    # --- Prepare Metadata for Analysis ---
    logger.info("Preparing test metadata subset...")
    test_metadata_subset = None
    test_results_df = None

    # Ensure the full metadata index is set correctly
    if 'full_metadata_df' not in locals() or full_metadata_df is None:
        logger.error("full_metadata_df not available. Cannot prepare test metadata.")
    else:
        if full_metadata_df.index.name != 'Row_names':
            if 'Row_names' in full_metadata_df.columns:
                logger.warning(f"Full metadata index was '{full_metadata_df.index.name}', "
                              f"setting 'Row_names' column as index.")
                try:
                    full_metadata_df = full_metadata_df.set_index('Row_names')
                except Exception as e_idx:
                    logger.error(f"Failed to set 'Row_names' as index: {e_idx}.")
                    full_metadata_df = None
            else:
                logger.error(f"Cannot find 'Row_names' column or index.")
                full_metadata_df = None

        # Select metadata only for the test samples using the test_ids (Row_names)
        if full_metadata_df is not None:
            if not test_ids:
                logger.error("test_ids (Row_names) not available for selecting test metadata.")
            else:
                try:
                    # Check if all test_ids are actually in the metadata index
                    missing_ids_in_meta = [idx for idx in test_ids if idx not in full_metadata_df.index]
                    valid_test_ids = [idx for idx in test_ids if idx in full_metadata_df.index]

                    if missing_ids_in_meta:
                        logger.warning(f"{len(missing_ids_in_meta)} out of {len(test_ids)} "
                                      f"test_ids not found in full_metadata_df index.")

                    if not valid_test_ids:
                        logger.error("No common test IDs found between test set and full metadata index.")
                    else:
                        test_metadata_subset = full_metadata_df.loc[valid_test_ids].copy()
                        logger.info(f"Created test metadata subset for {len(valid_test_ids)} common test samples.")

                        # --- Optional: Create the full results file ---
                        if test_metadata_subset is not None and not test_metadata_subset.empty:
                            # Align predictions and targets to the metadata subset index
                            common_pred_idx = test_metadata_subset.index.intersection(test_preds_df.index)
                            common_target_idx = test_metadata_subset.index.intersection(test_targets_df.index)

                            if len(common_pred_idx) < len(test_metadata_subset.index):
                                logger.warning(f"Prediction index missing "
                                              f"{len(test_metadata_subset.index) - len(common_pred_idx)} "
                                              f"samples from test metadata subset.")
                            if len(common_target_idx) < len(test_metadata_subset.index):
                                logger.warning(f"Target index missing "
                                              f"{len(test_metadata_subset.index) - len(common_target_idx)} "
                                              f"samples from test metadata subset.")

                            # Align DFs before joining
                            test_preds_df_aligned = test_preds_df.loc[common_pred_idx]
                            test_targets_df_aligned = test_targets_df.loc[common_target_idx]
                            test_metadata_subset_aligned = test_metadata_subset.loc[common_pred_idx]

                            test_preds_df_renamed = test_preds_df_aligned.rename(
                                columns={col: f"{col}_pred_encoded" for col in TARGET_COLS})
                            test_targets_df_renamed = test_targets_df_aligned.rename(
                                columns={col: f"{col}_true_encoded" for col in TARGET_COLS})

                            test_results_df = test_metadata_subset_aligned.join(test_preds_df_renamed, how='left')
                            test_results_df = test_results_df.join(test_targets_df_renamed, how='left')

                            # Add decoded labels
                            logger.info("Decoding labels for the full results file...")
                            for task_name in TARGET_COLS:
                                if task_name in label_encoders:
                                    encoder = label_encoders[task_name]
                                    pred_col_encoded = f"{task_name}_pred_encoded"
                                    true_col_encoded = f"{task_name}_true_encoded"
                                    pred_col_decoded = f"{task_name}_pred"
                                    true_col_decoded = f"{task_name}_true"

                                    # Decode predictions
                                    if pred_col_encoded in test_results_df.columns:
                                        valid_preds = test_results_df[pred_col_encoded].dropna()
                                        if not valid_preds.empty:
                                            try:
                                                decoded_preds = encoder.inverse_transform(valid_preds.astype(int))
                                                test_results_df[pred_col_decoded] = pd.Series(
                                                    decoded_preds, index=valid_preds.index)
                                            except Exception as e_dec_pred:
                                                logger.error(f"Error decoding predictions for '{task_name}': "
                                                           f"{e_dec_pred}", exc_info=True)
                                    # Get true decoded labels
                                    if true_col_decoded not in test_results_df.columns:
                                        if task_name in test_metadata_subset.columns:
                                            test_results_df[true_col_decoded] = test_metadata_subset[task_name]
                                        else:
                                            logger.warning(f"Original metadata column '{task_name}' "
                                                         f"not found for true decoded label.")
                                else:
                                    logger.warning(f"Label encoder not found for task '{task_name}'. Cannot decode.")

                            results_outfile = os.path.join(
                                OUTPUT_DIR, f"transformer_test_predictions_metadata_{ANALYSIS_PAIRING}.csv")
                            try:
                                test_results_df.to_csv(results_outfile, index=True, index_label='Row_names')
                                logger.info(f"Full test results with metadata saved to {results_outfile}")
                            except Exception as e_save:
                                logger.error(f"Failed to save full test results: {e_save}")

                except KeyError as e:
                    logger.error(f"Some test_ids (Row_names) not found in full_metadata_df index: {e}.")
                except Exception as e:
                    logger.error(f"Unexpected error selecting test metadata subset: {e}", exc_info=True)

    # --- Attention Analysis ---
    logger.info("Starting view-level attention analysis using test metadata...")

    # Check if test_metadata_subset is valid before passing
    if test_metadata_subset is None or test_metadata_subset.empty:
        logger.warning("Test metadata subset is missing or empty. Skipping view-level attention analysis.")
        view_level_attn_df, grouped_attn, stat_df = None, None, None
    else:
        if 'analyze_attention_view_level' in globals():
            view_level_attn_df, grouped_attn, stat_df = analyze_attention_view_level(
                test_attention,
                test_metadata_subset,
                OUTPUT_DIR,
                ANALYSIS_PAIRING
            )
        else:
            logger.error("Function 'analyze_attention_view_level' not found.")
            view_level_attn_df, grouped_attn, stat_df = None, None, None

    # --- Baseline Model Comparison ---
    logger.info("Running baseline model comparison...")
    baseline_comparison_df = run_baseline_models(
        X_train_spec=X_train_spec_scaled_df, X_train_metab=X_train_metab_scaled_df, y_train=y_train_df,
        X_test_spec=X_test_spec_scaled_df, X_test_metab=X_test_metab_scaled_df, y_test=y_test_df,
        target_cols=TARGET_COLS, output_dir=OUTPUT_DIR, pairing=ANALYSIS_PAIRING, random_seed=RANDOM_SEED
    )

    # --- Visualization ---
    plot_attention_heatmap(test_attention, OUTPUT_DIR, ANALYSIS_PAIRING)

    # Update final_results dictionary
    final_results = {
        'view_level_attention': view_level_attn_df,
        'grouped_view_attention': grouped_attn,
        'view_attention_stats': stat_df,
        'baseline_comparison': baseline_comparison_df,
        'test_results_with_metadata': test_results_df
    }
    plot_final_visualizations(final_results, OUTPUT_DIR, ANALYSIS_PAIRING)

    # --- End ---
    total_duration = time.time() - start_time
    logger.info(f"--- Main Execution Finished --- Total Duration: {total_duration / 60:.2f} minutes ---")
    logger.info("="*60)


# --- Entry Point ---
if __name__ == '__main__':
    try:
        main()
    except Exception as e:
        logger.error(f"An error occurred during main execution: {e}", exc_info=True)
        sys.exit(1)

--------------------------------------------------

transformer_model.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Multi-Omic Transformer Implementation

This code implements a transformer-based deep learning model for integrating
spectral and metabolite data from plant samples. The model employs a cross-attention
mechanism to discover relationships between different omics data types and
analyze plant stress responses across different genotypes and treatments.

Features:
- Integration of spectral and metabolite data
- Cross-attention for multi-omic data fusion
- Multi-task classification of genotype, treatment, and time points
- Attention-based interpretability for feature importance
- Comparison with traditional machine learning baselines

Usage:
    python transformer_implementation.py --tissue leaf
    or
    python transformer_implementation.py --tissue root

"""

import os
import time
import json
import argparse
import logging
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Union, Optional, Any

# For scaling and encoding
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, 
    confusion_matrix, classification_report
)

# Baseline models
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression

# PyTorch imports
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, TensorDataset
import torch.nn.functional as F


# ===== CONFIGURATION =====
class TransformerConfig:
    """Configuration for the Multi-Omic Transformer"""
    
    def __init__(self, tissue: str = "leaf"):
        # Paths
        self.output_dir = "C:/Users/ms/Desktop/hyper/output/transformer"
        self.figure_dir = os.path.join(self.output_dir, "figures")
        self.logs_dir = os.path.join(self.output_dir, "logs")
        self.checkpoints_dir = os.path.join(self.output_dir, "checkpoints")
        
        # Ensure all directories exist
        for dir_path in [self.output_dir, self.figure_dir, self.logs_dir, self.checkpoints_dir]:
            os.makedirs(dir_path, exist_ok=True)
            
        # Tissue type (leaf or root)
        self.tissue = tissue.lower()
        if self.tissue not in ["leaf", "root"]:
            raise ValueError("Tissue must be either 'leaf' or 'root'")
            
        # Data paths
        self.mofa_dir = "C:/Users/ms/Desktop/hyper/output/mofa"
        self.spectral_path = os.path.join(self.mofa_dir, f"transformer_input_{self.tissue}_spectral.csv")
        self.metabolite_path = os.path.join(self.mofa_dir, f"transformer_input_{self.tissue}_metabolite.csv")
        
        # Metadata columns
        self.metadata_columns = [
            "Row_names", "Vac_id", "Genotype", "Entry", "Tissue.type", 
            "Batch", "Treatment", "Replication", "Day"
        ]
        
        # Target columns for multi-task classification
        self.target_columns = ["Genotype", "Treatment", "Day"]
        
        # Target encoding maps
        self.target_encoders = {
            "Genotype": {"G1": 0, "G2": 1},
            "Treatment": {"T0": 0, "T1": 1}
            # Day is numeric and will be encoded as 0, 1, 2
        }
        
        # Train/val/test split ratios
        self.train_ratio = 0.7
        self.val_ratio = 0.15
        self.test_ratio = 0.15
        
        # Cross-validation
        self.use_cross_validation = False  # Start with False, implement CV later
        self.n_splits = 5
        
        # Model hyperparameters
        self.hidden_dim = 64
        self.num_heads = 4
        self.num_layers = 2
        self.dropout = 0.1
        self.learning_rate = 0.001
        self.weight_decay = 1e-5
        
        # Training parameters
        self.batch_size = 32
        self.num_epochs = 100
        self.early_stopping_patience = 10
        self.num_workers = 8
        
        # Analysis parameters
        self.attention_threshold = 0.1  # Threshold for strong attention pairs
        self.top_n_pairs = 100  # Number of top pairs to report
        
        # Random seed for reproducibility
        self.seed = 42


# ===== DATA LOADING =====
def load_and_preprocess_data(config: TransformerConfig) -> Dict[str, Any]:
    """
    Load and preprocess spectral and metabolite data for the specified tissue.
    
    Args:
        config: Configuration object with data paths and parameters
    
    Returns:
        Dictionary containing processed data and metadata
    """
    logging.info(f"Loading {config.tissue} data...")
    
    # Load spectral and metabolite data
    spectral_df = pd.read_csv(config.spectral_path)
    metabolite_df = pd.read_csv(config.metabolite_path)
    
    logging.info(f"Spectral data shape: {spectral_df.shape}")
    logging.info(f"Metabolite data shape: {metabolite_df.shape}")
    
    # Check that both dataframes have the same shape
    if spectral_df.shape[0] != metabolite_df.shape[0]:
        raise ValueError(f"Different number of samples: Spectral ({spectral_df.shape[0]}) vs Metabolite ({metabolite_df.shape[0]})")
    
    # Create a mapping between different naming conventions for later reference
    sample_mapping = pd.DataFrame({
        'spectral_id': spectral_df['Row_names'].values,
        'metabolite_id': metabolite_df['Row_names'].values,
        'index': range(len(spectral_df))
    })
    
    logging.info(f"Created sample mapping between spectral and metabolite IDs")
    
    # Extract metadata - use spectral dataframe's metadata since we're matching positions
    metadata = spectral_df[config.metadata_columns].copy()
    
    # Extract feature columns
    spectral_features = spectral_df.drop(columns=config.metadata_columns)
    metabolite_features = metabolite_df.drop(columns=config.metadata_columns)
    
    # Get feature names for later analysis
    spectral_feature_names = spectral_features.columns.tolist()
    metabolite_feature_names = metabolite_features.columns.tolist()
    
    # Get sample IDs for reference
    sample_ids = metadata['Row_names'].values
    
    # Encode target variables
    target_data = {}
    for col in config.target_columns:
        if col == 'Day':
            # Subtract 1 to make Days 1,2,3 into 0,1,2 (will use as classifier targets)
            target_data[col] = metadata[col].astype(int) - 1
        elif col in config.target_encoders:
            encoder = config.target_encoders[col]
            target_data[col] = metadata[col].map(encoder)
        else:
            # Fallback - shouldn't reach here given our configuration
            encoder = LabelEncoder()
            target_data[col] = encoder.fit_transform(metadata[col])
    
    # Convert to numpy arrays for PyTorch
    spectral_data = spectral_features.values.astype(np.float32)
    metabolite_data = metabolite_features.values.astype(np.float32)
    
    # Create target array for multi-task learning
    target_arrays = [target_data[col].values for col in config.target_columns]
    target_array = np.column_stack(target_arrays).astype(np.int64)
    
    # Standard scale features (will fit on train, transform on val/test later)
    spectral_scaler = StandardScaler()
    metabolite_scaler = StandardScaler()
    
    # Create train/val/test splits with stratification
    # Use Day as stratification target (most balanced classes)
    train_idx, temp_idx = train_test_split(
        np.arange(len(sample_ids)), 
        test_size=(config.val_ratio + config.test_ratio),
        random_state=config.seed,
        stratify=target_data['Day']
    )
    
    # Further split temp into validation and test
    relative_test_ratio = config.test_ratio / (config.val_ratio + config.test_ratio)
    val_idx, test_idx = train_test_split(
        temp_idx,
        test_size=relative_test_ratio,
        random_state=config.seed,
        stratify=target_data['Day'].iloc[temp_idx]
    )
    
    # Log split sizes
    logging.info(f"Train set: {len(train_idx)} samples")
    logging.info(f"Validation set: {len(val_idx)} samples")
    logging.info(f"Test set: {len(test_idx)} samples")
    
    # Fit scalers on training data
    spectral_scaler.fit(spectral_data[train_idx])
    metabolite_scaler.fit(metabolite_data[train_idx])
    
    # Transform all data
    spectral_data_scaled = spectral_scaler.transform(spectral_data)
    metabolite_data_scaled = metabolite_scaler.transform(metabolite_data)
    
    # Pack everything into a dictionary
    data_dict = {
        'spectral_data': spectral_data_scaled,
        'metabolite_data': metabolite_data_scaled,
        'target_data': target_array,
        'sample_ids': sample_ids,
        'sample_mapping': sample_mapping,
        'metadata': metadata,
        'spectral_feature_names': spectral_feature_names,
        'metabolite_feature_names': metabolite_feature_names,
        'train_idx': train_idx,
        'val_idx': val_idx,
        'test_idx': test_idx,
        'spectral_scaler': spectral_scaler,
        'metabolite_scaler': metabolite_scaler
    }
    
    return data_dict


class MultiOmicDataset(Dataset):
    """Dataset for paired spectral and metabolite data"""
    
    def __init__(self, 
                 spectral_data: np.ndarray, 
                 metabolite_data: np.ndarray, 
                 targets: np.ndarray,
                 indices: Optional[np.ndarray] = None):
        """
        Initialize the dataset.
        
        Args:
            spectral_data: Spectral features (scaled)
            metabolite_data: Metabolite features (scaled)
            targets: Target variables (encoded)
            indices: Indices to use (for train/val/test splitting)
        """
        if indices is not None:
            self.spectral_data = torch.FloatTensor(spectral_data[indices])
            self.metabolite_data = torch.FloatTensor(metabolite_data[indices])
            self.targets = torch.LongTensor(targets[indices])
        else:
            self.spectral_data = torch.FloatTensor(spectral_data)
            self.metabolite_data = torch.FloatTensor(metabolite_data)
            self.targets = torch.LongTensor(targets)
    
    def __len__(self):
        return len(self.spectral_data)
    
    def __getitem__(self, idx):
        return {
            'spectral': self.spectral_data[idx],
            'metabolite': self.metabolite_data[idx],
            'targets': self.targets[idx]
        }


def create_dataloaders(data_dict: Dict[str, Any], config: TransformerConfig) -> Dict[str, DataLoader]:
    """
    Create PyTorch DataLoaders for training, validation, and testing.
    
    Args:
        data_dict: Dictionary containing processed data
        config: Configuration object with parameters
    
    Returns:
        Dictionary containing train, validation, and test DataLoaders
    """
    # Create datasets
    train_dataset = MultiOmicDataset(
        data_dict['spectral_data'],
        data_dict['metabolite_data'],
        data_dict['target_data'],
        data_dict['train_idx']
    )
    
    val_dataset = MultiOmicDataset(
        data_dict['spectral_data'],
        data_dict['metabolite_data'],
        data_dict['target_data'],
        data_dict['val_idx']
    )
    
    test_dataset = MultiOmicDataset(
        data_dict['spectral_data'],
        data_dict['metabolite_data'],
        data_dict['target_data'],
        data_dict['test_idx']
    )
    
    # Create dataloaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=config.num_workers,
        pin_memory=True if torch.cuda.is_available() else False
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=config.num_workers,
        pin_memory=True if torch.cuda.is_available() else False
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=config.num_workers,
        pin_memory=True if torch.cuda.is_available() else False
    )
    
    return {
        'train': train_loader,
        'val': val_loader,
        'test': test_loader
    }


# ===== MODEL DEFINITION =====
class CrossAttentionLayer(nn.Module):
    """Cross-attention layer for spectral and metabolite features"""
    
    def __init__(self, hidden_dim: int, num_heads: int, dropout: float = 0.1):
        """
        Initialize the cross-attention layer.
        
        Args:
            hidden_dim: Hidden dimension size
            num_heads: Number of attention heads
            dropout: Dropout probability
        """
        super().__init__()
        
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        
        # Self-attention for spectral
        self.spectral_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )
        
        # Self-attention for metabolite
        self.metabolite_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )
        
        # Cross-attention: spectral queries, metabolite keys/values
        self.cross_attention_s2m = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )
        
        # Cross-attention: metabolite queries, spectral keys/values
        self.cross_attention_m2s = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )
        
        # Feed-forward networks
        self.spectral_ffn = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 4),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 4, hidden_dim)
        )
        
        self.metabolite_ffn = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 4),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 4, hidden_dim)
        )
        
        # Layer normalization
        self.norm1_spectral = nn.LayerNorm(hidden_dim)
        self.norm2_spectral = nn.LayerNorm(hidden_dim)
        self.norm3_spectral = nn.LayerNorm(hidden_dim)
        
        self.norm1_metabolite = nn.LayerNorm(hidden_dim)
        self.norm2_metabolite = nn.LayerNorm(hidden_dim)
        self.norm3_metabolite = nn.LayerNorm(hidden_dim)
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, spectral_emb, metabolite_emb):
        """
        Forward pass through the cross-attention layer.
        
        Args:
            spectral_emb: Embedded spectral features
            metabolite_emb: Embedded metabolite features
            
        Returns:
            Updated embeddings and attention weights
        """
        # Self-attention for spectral
        spectral_attn, _ = self.spectral_attention(
            spectral_emb, spectral_emb, spectral_emb
        )
        spectral_emb = self.norm1_spectral(spectral_emb + self.dropout(spectral_attn))
        
        # Self-attention for metabolite
        metabolite_attn, _ = self.metabolite_attention(
            metabolite_emb, metabolite_emb, metabolite_emb
        )
        metabolite_emb = self.norm1_metabolite(metabolite_emb + self.dropout(metabolite_attn))
        
        # Cross-attention: spectral -> metabolite
        cross_s2m_output, cross_s2m_weights = self.cross_attention_s2m(
            spectral_emb, metabolite_emb, metabolite_emb
        )
        spectral_emb = self.norm2_spectral(spectral_emb + self.dropout(cross_s2m_output))
        
        # Cross-attention: metabolite -> spectral
        cross_m2s_output, cross_m2s_weights = self.cross_attention_m2s(
            metabolite_emb, spectral_emb, spectral_emb
        )
        metabolite_emb = self.norm2_metabolite(metabolite_emb + self.dropout(cross_m2s_output))
        
        # Feed-forward for spectral
        spectral_ffn_output = self.spectral_ffn(spectral_emb)
        spectral_emb = self.norm3_spectral(spectral_emb + self.dropout(spectral_ffn_output))
        
        # Feed-forward for metabolite
        metabolite_ffn_output = self.metabolite_ffn(metabolite_emb)
        metabolite_emb = self.norm3_metabolite(metabolite_emb + self.dropout(metabolite_ffn_output))
        
        # Return updated embeddings and cross-attention weights
        return spectral_emb, metabolite_emb, {
            's2m': cross_s2m_weights,
            'm2s': cross_m2s_weights
        }


class MultiOmicTransformer(nn.Module):
    """Transformer model for multi-omic integration with cross-attention"""
    
    def __init__(self, 
                 spectral_dim: int,
                 metabolite_dim: int,
                 hidden_dim: int = 64,
                 num_heads: int = 4,
                 num_layers: int = 2,
                 dropout: float = 0.1,
                 num_tasks: int = 3,
                 num_classes: List[int] = [2, 2, 3]):
        """
        Initialize the transformer model.
        
        Args:
            spectral_dim: Number of spectral features
            metabolite_dim: Number of metabolite features
            hidden_dim: Hidden dimension size
            num_heads: Number of attention heads
            num_layers: Number of transformer layers
            dropout: Dropout probability
            num_tasks: Number of classification tasks
            num_classes: Number of classes for each task
        """
        super().__init__()
        
        self.spectral_dim = spectral_dim
        self.metabolite_dim = metabolite_dim
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.num_tasks = num_tasks
        self.num_classes = num_classes
        
        # Input embedding layers
        self.spectral_embedding = nn.Linear(spectral_dim, hidden_dim)
        self.metabolite_embedding = nn.Linear(metabolite_dim, hidden_dim)
        
        # Positional encoding (simplified, learnable)
        self.spectral_pos_encoding = nn.Parameter(torch.zeros(1, 1, hidden_dim))
        self.metabolite_pos_encoding = nn.Parameter(torch.zeros(1, 1, hidden_dim))
        
        # Cross-attention layers
        self.cross_attention_layers = nn.ModuleList([
            CrossAttentionLayer(hidden_dim, num_heads, dropout)
            for _ in range(num_layers)
        ])
        
        # Classification heads - one per task
        self.classifiers = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_dim * 2, hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(hidden_dim, num_classes[i])
            )
            for i in range(num_tasks)
        ])
        
        # Initialize parameters
        self._init_parameters()
        
        # Storage for attention weights
        self.attention_weights = []
    
    def _init_parameters(self):
        """Initialize model parameters"""
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
    
    def forward(self, spectral, metabolite):
        """
        Forward pass through the transformer.
        
        Args:
            spectral: Spectral features
            metabolite: Metabolite features
            
        Returns:
            Dictionary containing task outputs and attention weights
        """
        batch_size = spectral.size(0)
        
        # Reshape inputs for attention: [batch_size, 1, features]
        spectral = spectral.unsqueeze(1)
        metabolite = metabolite.unsqueeze(1)
        
        # Embed inputs
        spectral_emb = self.spectral_embedding(spectral) + self.spectral_pos_encoding
        metabolite_emb = self.metabolite_embedding(metabolite) + self.metabolite_pos_encoding
        
        # Clear stored attention weights
        self.attention_weights = []
        
        # Apply cross-attention layers
        for layer in self.cross_attention_layers:
            spectral_emb, metabolite_emb, attn_weights = layer(spectral_emb, metabolite_emb)
            self.attention_weights.append(attn_weights)
        
        # Create combined representation
        # Squeeze out the sequence dimension (which is 1)
        spectral_repr = spectral_emb.squeeze(1)
        metabolite_repr = metabolite_emb.squeeze(1)
        combined = torch.cat([spectral_repr, metabolite_repr], dim=1)
        
        # Apply classification heads
        outputs = [classifier(combined) for classifier in self.classifiers]
        
        return {
            'task_outputs': outputs,
            'attention_weights': self.attention_weights
        }
    
    def get_attention_weights(self):
        """
        Get stored attention weights from the last forward pass.
        
        Returns:
            List of attention weight dictionaries
        """
        return self.attention_weights


# ===== TRAINING FUNCTIONS =====
def compute_loss(outputs, targets, task_weights=None):
    """
    Compute the multi-task loss.
    
    Args:
        outputs: Model outputs (list of logits for each task)
        targets: Target values (batch_size, num_tasks)
        task_weights: Optional weights for each task's loss
        
    Returns:
        Total loss and individual task losses
    """
    # Default equal weights if not provided
    if task_weights is None:
        task_weights = [1.0] * len(outputs)
    
    criterion = nn.CrossEntropyLoss()
    task_losses = []
    
    for i, task_output in enumerate(outputs):
        task_targets = targets[:, i]
        task_loss = criterion(task_output, task_targets)
        task_losses.append(task_loss)
    
    # Weighted sum of task losses
    total_loss = sum(w * loss for w, loss in zip(task_weights, task_losses))
    
    return total_loss, task_losses


def train_epoch(model, dataloader, optimizer, device):
    """
    Train the model for one epoch.
    
    Args:
        model: The transformer model
        dataloader: Training data loader
        optimizer: Optimizer for parameter updates
        device: Device to train on (cpu/cuda)
        
    Returns:
        Average loss and accuracy metrics
    """
    model.train()
    total_loss = 0.0
    task_losses = [0.0] * 3  # Assuming 3 tasks (Genotype, Treatment, Day)
    correct_predictions = [0] * 3
    total_predictions = 0
    
    for batch in dataloader:
        # Get batch data
        spectral = batch['spectral'].to(device)
        metabolite = batch['metabolite'].to(device)
        targets = batch['targets'].to(device)
        
        # Forward pass
        optimizer.zero_grad()
        outputs = model(spectral, metabolite)
        task_outputs = outputs['task_outputs']
        
        # Compute loss
        loss, individual_losses = compute_loss(task_outputs, targets)
        
        # Backward pass
        loss.backward()
        optimizer.step()
        
        # Update metrics
        total_loss += loss.item() * spectral.size(0)
        for i, task_loss in enumerate(individual_losses):
            task_losses[i] += task_loss.item() * spectral.size(0)
        
        # Calculate accuracy
        for i, task_output in enumerate(task_outputs):
            _, predicted = torch.max(task_output, 1)
            correct_predictions[i] += (predicted == targets[:, i]).sum().item()
        
        total_predictions += spectral.size(0)
    
    # Calculate averages
    avg_loss = total_loss / total_predictions
    avg_task_losses = [loss / total_predictions for loss in task_losses]
    task_accuracies = [correct / total_predictions for correct in correct_predictions]
    
    return avg_loss, avg_task_losses, task_accuracies


def evaluate(model, dataloader, device):
    """
    Evaluate the model.
    
    Args:
        model: The transformer model
        dataloader: Validation/test data loader
        device: Device to evaluate on (cpu/cuda)
        
    Returns:
        Average loss, accuracy metrics, and predictions
    """
    model.eval()
    total_loss = 0.0
    task_losses = [0.0] * 3  # Assuming 3 tasks
    correct_predictions = [0] * 3
    total_predictions = 0
    
    all_predictions = [[] for _ in range(3)]
    all_targets = [[] for _ in range(3)]
    
    with torch.no_grad():
        for batch in dataloader:
            # Get batch data
            spectral = batch['spectral'].to(device)
            metabolite = batch['metabolite'].to(device)
            targets = batch['targets'].to(device)
            
            # Forward pass
            outputs = model(spectral, metabolite)
            task_outputs = outputs['task_outputs']
            
            # Compute loss
            loss, individual_losses = compute_loss(task_outputs, targets)
            
            # Update metrics
            total_loss += loss.item() * spectral.size(0)
            for i, task_loss in enumerate(individual_losses):
                task_losses[i] += task_loss.item() * spectral.size(0)
            
            # Calculate accuracy and store predictions
            for i, task_output in enumerate(task_outputs):
                _, predicted = torch.max(task_output, 1)
                correct_predictions[i] += (predicted == targets[:, i]).sum().item()
                
                # Store predictions and targets for metrics
                all_predictions[i].extend(predicted.cpu().numpy())
                all_targets[i].extend(targets[:, i].cpu().numpy())
            
            total_predictions += spectral.size(0)
    
    # Calculate averages
    avg_loss = total_loss / total_predictions
    avg_task_losses = [loss / total_predictions for loss in task_losses]
    task_accuracies = [correct / total_predictions for correct in correct_predictions]
    
    return avg_loss, avg_task_losses, task_accuracies, all_predictions, all_targets


def train_model(model, dataloaders, config, device):
    """
    Train the transformer model with early stopping.
    
    Args:
        model: The transformer model
        dataloaders: Dictionary of train/val/test dataloaders
        config: Configuration object
        device: Device to train on (cpu/cuda)
        
    Returns:
        Trained model and training history
    """
    # Initialize optimizer
    optimizer = optim.Adam(
        model.parameters(), 
        lr=config.learning_rate,
        weight_decay=config.weight_decay
    )
    
    # Initialize history
    history = {
        'train_loss': [],
        'val_loss': [],
        'train_acc': [[] for _ in range(3)],
        'val_acc': [[] for _ in range(3)]
    }
    
    # Early stopping variables
    best_val_loss = float('inf')
    patience_counter = 0
    best_model_state = None
    
    # Training loop
    for epoch in range(config.num_epochs):
        epoch_start_time = time.time()
        
        # Train for one epoch
        train_loss, train_task_losses, train_accuracies = train_epoch(
            model, dataloaders['train'], optimizer, device
        )
        
        # Evaluate on validation set
        val_loss, val_task_losses, val_accuracies, _, _ = evaluate(
            model, dataloaders['val'], device
        )
        
        # Update history
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        for i in range(3):
            history['train_acc'][i].append(train_accuracies[i])
            history['val_acc'][i].append(val_accuracies[i])
        
        # Log progress
        epoch_time = time.time() - epoch_start_time
        logging.info(f"Epoch {epoch+1}/{config.num_epochs} - {epoch_time:.2f}s - "
                    f"Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}")
        for i, task in enumerate(['Genotype', 'Treatment', 'Day']):
            logging.info(f"  {task} - Train Acc: {train_accuracies[i]:.4f} - "
                        f"Val Acc: {val_accuracies[i]:.4f}")
        
        # Check for improvement
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            best_model_state = model.state_dict().copy()
            
            # Save checkpoint
            checkpoint_path = os.path.join(
                config.checkpoints_dir, 
                f"{config.tissue}_transformer_best.pt"
            )
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_loss,
                'val_accuracies': val_accuracies,
                'config': vars(config)
            }, checkpoint_path)
            logging.info(f"Saved best model checkpoint (Val Loss: {val_loss:.4f})")
        else:
            patience_counter += 1
            logging.info(f"No improvement for {patience_counter} epochs")
        
        # Early stopping
        if patience_counter >= config.early_stopping_patience:
            logging.info(f"Early stopping triggered after {epoch+1} epochs")
            break
    
    # Load best model
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        logging.info("Loaded best model weights")
    
    return model, history


# ===== ANALYSIS FUNCTIONS =====
def extract_attention_weights(model, dataloader, data_dict, config, device):
    """
    Extract attention weights from the model for analysis.
    
    Args:
        model: Trained transformer model
        dataloader: Test data loader
        data_dict: Dictionary containing feature names and metadata
        config: Configuration object
        device: Device to run on (cpu/cuda)
        
    Returns:
        Dictionary containing processed attention data
    """
    model.eval()
    
    # Get feature names
    spectral_features = data_dict['spectral_feature_names']
    metabolite_features = data_dict['metabolite_feature_names']
    
    # Storage for attention scores
    all_s2m_attn = []  # spectral -> metabolite
    all_m2s_attn = []  # metabolite -> spectral
    
    # Sample metadata
    sample_idx = []
    metadata = []
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(dataloader):
            # Get batch data
            spectral = batch['spectral'].to(device)
            metabolite = batch['metabolite'].to(device)
            targets = batch['targets'].cpu().numpy()
            
            # Forward pass
            outputs = model(spectral, metabolite)
            attention_weights = model.get_attention_weights()
            
            # We'll focus on the last layer's attention weights (most refined)
            last_layer_weights = attention_weights[-1]
            
            # Extract weights
            s2m_weights = last_layer_weights['s2m'].cpu().numpy()
            m2s_weights = last_layer_weights['m2s'].cpu().numpy()
            
            # Store weights
            all_s2m_attn.append(s2m_weights)
            all_m2s_attn.append(m2s_weights)
            
            # Store sample indices for this batch
            batch_indices = data_dict['test_idx'][batch_idx * config.batch_size:
                                               min((batch_idx + 1) * config.batch_size,
                                                   len(data_dict['test_idx']))]
            sample_idx.extend(batch_indices)
            
            # Store metadata for this batch
            for i in range(len(targets)):
                meta_idx = batch_indices[i] if i < len(batch_indices) else None
                if meta_idx is not None:
                    metadata.append({
                        'Genotype': data_dict['metadata']['Genotype'].iloc[meta_idx],
                        'Treatment': data_dict['metadata']['Treatment'].iloc[meta_idx],
                        'Day': data_dict['metadata']['Day'].iloc[meta_idx],
                        'Batch': data_dict['metadata']['Batch'].iloc[meta_idx]
                    })
    
    # Concatenate all attention weights
    all_s2m_attn = np.concatenate(all_s2m_attn, axis=0)
    all_m2s_attn = np.concatenate(all_m2s_attn, axis=0)
    
    # Average across attention heads
    s2m_avg = np.mean(all_s2m_attn, axis=1)  # shape: [n_samples, n_spectral, n_metabolite]
    m2s_avg = np.mean(all_m2s_attn, axis=1)  # shape: [n_samples, n_metabolite, n_spectral]
    
    # Average across samples (for global importance)
    s2m_global = np.mean(s2m_avg, axis=0)  # shape: [n_spectral, n_metabolite]
    m2s_global = np.mean(m2s_avg, axis=0)  # shape: [n_metabolite, n_spectral]
    
    # Package attention data
    attention_data = {
        'spectral_features': spectral_features,
        'metabolite_features': metabolite_features,
        's2m_attention': s2m_avg,
        'm2s_attention': m2s_avg,
        's2m_global': s2m_global,
        'm2s_global': m2s_global,
        'sample_metadata': metadata,
        'sample_indices': sample_idx
    }
    
    return attention_data


def analyze_cross_modal_pairs(attention_data, config):
    """
    Analyze cross-modal pairs from attention weights.
    
    Args:
        attention_data: Dictionary containing attention data
        config: Configuration object
        
    Returns:
        DataFrame containing top spectral-metabolite pairs
    """
    # Extract global attention scores
    s2m_global = attention_data['s2m_global']
    
    # Get feature names
    spectral_features = attention_data['spectral_features']
    metabolite_features = attention_data['metabolite_features']
    
    # Create DataFrame for all pairs
    pairs_data = []
    
    for i, spectral_feat in enumerate(spectral_features):
        for j, metabolite_feat in enumerate(metabolite_features):
            attention_score = s2m_global[i, j]
            pairs_data.append({
                'Spectral_Feature': spectral_feat,
                'Metabolite_Feature': metabolite_feat,
                'Attention_Score': attention_score,
                'Spectral_Index': i,
                'Metabolite_Index': j
            })
    
    # Create DataFrame
    pairs_df = pd.DataFrame(pairs_data)
    
    # Sort by attention score
    pairs_df = pairs_df.sort_values('Attention_Score', ascending=False)
    
    # Filter top pairs
    top_pairs = pairs_df.head(config.top_n_pairs)
    
    # Save to CSV
    output_path = os.path.join(
        config.output_dir,
        f"transformer_cross_modal_pairs_{config.tissue}.csv"
    )
    top_pairs.to_csv(output_path, index=False)
    
    # Also save all pairs for potential later analysis
    all_pairs_path = os.path.join(
        config.output_dir,
        f"transformer_all_cross_modal_pairs_{config.tissue}.csv"
    )
    pairs_df.to_csv(all_pairs_path, index=False)
    
    logging.info(f"Saved top {config.top_n_pairs} cross-modal pairs to {output_path}")
    logging.info(f"Saved all cross-modal pairs to {all_pairs_path}")
    
    return top_pairs


def analyze_temporal_attention(attention_data, config):
    """
    Analyze temporal patterns in attention weights.
    
    Args:
        attention_data: Dictionary containing attention data
        config: Configuration object
        
    Returns:
        DataFrame containing attention patterns by day
    """
    # Extract attention scores and metadata
    s2m_attention = attention_data['s2m_attention']
    sample_metadata = attention_data['sample_metadata']
    
    # Get feature names
    spectral_features = attention_data['spectral_features']
    metabolite_features = attention_data['metabolite_features']
    
    # Group samples by day
    day_groups = {}
    for i, meta in enumerate(sample_metadata):
        day = meta['Day']
        if day not in day_groups:
            day_groups[day] = []
        day_groups[day].append(i)
    
    # Calculate average attention per day
    day_attention = {}
    for day, indices in day_groups.items():
        day_samples = s2m_attention[indices]
        day_avg = np.mean(day_samples, axis=0)
        day_attention[day] = day_avg
    
    # Create DataFrame for temporal analysis
    temporal_data = []
    
    # For each day, get top pairs
    for day in sorted(day_groups.keys()):
        day_avg = day_attention[day]
        
        # Find top pairs for this day
        for i, spectral_feat in enumerate(spectral_features):
            for j, metabolite_feat in enumerate(metabolite_features):
                attention_score = day_avg[i, j]
                
                # Only keep scores above threshold
                if attention_score > config.attention_threshold:
                    temporal_data.append({
                        'Day': day,
                        'Spectral_Feature': spectral_feat,
                        'Metabolite_Feature': metabolite_feat,
                        'Attention_Score': attention_score,
                        'Spectral_Index': i,
                        'Metabolite_Index': j
                    })
    
    # Create DataFrame
    temporal_df = pd.DataFrame(temporal_data)
    
    # Sort by day and attention score
    temporal_df = temporal_df.sort_values(['Day', 'Attention_Score'], ascending=[True, False])
    
    # Save to CSV
    output_path = os.path.join(
        config.output_dir,
        f"transformer_temporal_attention_{config.tissue}.csv"
    )
    temporal_df.to_csv(output_path, index=False)
    logging.info(f"Saved temporal attention patterns to {output_path}")
    
    return temporal_df


def analyze_feature_importance(attention_data, config):
    """
    Calculate feature importance based on attention weights.
    
    Args:
        attention_data: Dictionary containing attention data
        config: Configuration object
        
    Returns:
        DataFrame containing feature importance scores
    """
    # Extract global attention scores
    s2m_global = attention_data['s2m_global']
    m2s_global = attention_data['m2s_global']
    
    # Get feature names
    spectral_features = attention_data['spectral_features']
    metabolite_features = attention_data['metabolite_features']
    
    # Calculate spectral feature importance
    # For each spectral feature, sum its attention to all metabolite features
    spectral_importance = np.sum(s2m_global, axis=1)
    
    # Calculate metabolite feature importance
    # For each metabolite feature, sum its attention from all spectral features
    metabolite_importance = np.sum(s2m_global, axis=0)
    
    # Create DataFrames
    spectral_df = pd.DataFrame({
        'Feature': spectral_features,
        'Importance': spectral_importance,
        'Feature_Type': 'Spectral'
    })
    
    metabolite_df = pd.DataFrame({
        'Feature': metabolite_features,
        'Importance': metabolite_importance,
        'Feature_Type': 'Metabolite'
    })
    
    # Combine and normalize within feature types
    combined_df = pd.concat([spectral_df, metabolite_df])
    
    # Normalize within feature types
    for feat_type in ['Spectral', 'Metabolite']:
        mask = combined_df['Feature_Type'] == feat_type
        min_val = combined_df.loc[mask, 'Importance'].min()
        max_val = combined_df.loc[mask, 'Importance'].max()
        combined_df.loc[mask, 'Importance_Scaled'] = (
            (combined_df.loc[mask, 'Importance'] - min_val) / 
            (max_val - min_val) if max_val > min_val else 0.0
        )
    
    # Sort by importance
    combined_df = combined_df.sort_values(['Feature_Type', 'Importance'], ascending=[True, False])
    
    # Save to CSV
    output_path = os.path.join(
        config.output_dir,
        f"transformer_feature_importance_{config.tissue}.csv"
    )
    combined_df.to_csv(output_path, index=False)
    logging.info(f"Saved feature importance scores to {output_path}")
    
    return combined_df


def analyze_genotype_differences(attention_data, config):
    """
    Analyze genotype-specific attention patterns.
    
    Args:
        attention_data: Dictionary containing attention data
        config: Configuration object
        
    Returns:
        Dictionary containing genotype-specific attention data
    """
    # Extract attention scores and metadata
    s2m_attention = attention_data['s2m_attention']
    sample_metadata = attention_data['sample_metadata']
    
    # Get feature names
    spectral_features = attention_data['spectral_features']
    metabolite_features = attention_data['metabolite_features']
    
    # Group samples by genotype
    g1_indices = [i for i, meta in enumerate(sample_metadata) if meta['Genotype'] == 'G1']
    g2_indices = [i for i, meta in enumerate(sample_metadata) if meta['Genotype'] == 'G2']
    
    # Calculate average attention per genotype
    g1_avg = np.mean(s2m_attention[g1_indices], axis=0) if g1_indices else None
    g2_avg = np.mean(s2m_attention[g2_indices], axis=0) if g2_indices else None
    
    # Create DataFrame for genotype differences
    genotype_data = []
    
    if g1_avg is not None and g2_avg is not None:
        # Calculate absolute difference
        diff = np.abs(g1_avg - g2_avg)
        
        # Find pairs with significant differences
        for i, spectral_feat in enumerate(spectral_features):
            for j, metabolite_feat in enumerate(metabolite_features):
                g1_score = g1_avg[i, j]
                g2_score = g2_avg[i, j]
                difference = diff[i, j]
                
                # Only keep significant differences
                if difference > config.attention_threshold:
                    genotype_data.append({
                        'Spectral_Feature': spectral_feat,
                        'Metabolite_Feature': metabolite_feat,
                        'G1_Attention': g1_score,
                        'G2_Attention': g2_score,
                        'Abs_Difference': difference,
                        'Stronger_In': 'G1' if g1_score > g2_score else 'G2',
                        'Spectral_Index': i,
                        'Metabolite_Index': j
                    })
    
    # Create DataFrame
    genotype_df = pd.DataFrame(genotype_data)
    
    # Sort by difference
    if not genotype_df.empty:
        genotype_df = genotype_df.sort_values('Abs_Difference', ascending=False)
        
        # Save to CSV
        output_path = os.path.join(
            config.output_dir,
            f"transformer_genotype_diff_{config.tissue}.csv"
        )
        genotype_df.to_csv(output_path, index=False)
        logging.info(f"Saved genotype difference analysis to {output_path}")
    else:
        logging.warning("No significant genotype differences found in attention patterns")
    
    # Return genotype-specific attention for visualizations
    return {
        'g1_avg': g1_avg,
        'g2_avg': g2_avg,
        'genotype_df': genotype_df
    }


def evaluate_model_performance(model, dataloaders, config, device):
    """
    Evaluate model performance with detailed metrics.
    
    Args:
        model: Trained transformer model
        dataloaders: Dictionary of train/val/test dataloaders
        config: Configuration object
        device: Device to evaluate on (cpu/cuda)
        
    Returns:
        DataFrame containing performance metrics
    """
    # Evaluate on test set
    test_loss, test_task_losses, test_accuracies, predictions, targets = evaluate(
        model, dataloaders['test'], device
    )
    
    # Calculate detailed metrics for each task
    performance_data = []
    task_names = ['Genotype', 'Treatment', 'Day']
    
    for i, task in enumerate(task_names):
        y_true = targets[i]
        y_pred = predictions[i]
        
        # Calculate metrics
        accuracy = accuracy_score(y_true, y_pred)
        
        # Handle binary vs. multi-class
        if len(np.unique(y_true)) == 2:
            precision = precision_score(y_true, y_pred)
            recall = recall_score(y_true, y_pred)
            f1 = f1_score(y_true, y_pred)
        else:
            precision = precision_score(y_true, y_pred, average='weighted')
            recall = recall_score(y_true, y_pred, average='weighted')
            f1 = f1_score(y_true, y_pred, average='weighted')
        
        # Store metrics
        performance_data.append({
            'Task': task,
            'Accuracy': accuracy,
            'Precision': precision,
            'Recall': recall,
            'F1_Score': f1,
            'Loss': test_task_losses[i]
        })
    
    # Create DataFrame
    performance_df = pd.DataFrame(performance_data)
    
    # Add average row
    avg_row = {
        'Task': 'Average',
        'Accuracy': np.mean(performance_df['Accuracy']),
        'Precision': np.mean(performance_df['Precision']),
        'Recall': np.mean(performance_df['Recall']),
        'F1_Score': np.mean(performance_df['F1_Score']),
        'Loss': np.mean(performance_df['Loss'])
    }
    performance_df = pd.concat([performance_df, pd.DataFrame([avg_row])])
    
    # Save to CSV
    output_path = os.path.join(
        config.output_dir,
        f"transformer_class_performance_{config.tissue}.csv"
    )
    performance_df.to_csv(output_path, index=False)
    logging.info(f"Saved model performance metrics to {output_path}")
    
    # Generate and save confusion matrices
    for i, task in enumerate(task_names):
        cm = confusion_matrix(targets[i], predictions[i])
        
        # Plot confusion matrix
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.title(f'Confusion Matrix - {task}')
        plt.xlabel('Predicted')
        plt.ylabel('True')
        
        # Save figure
        cm_path = os.path.join(
            config.figure_dir,
            f"confusion_matrix_{task.lower()}_{config.tissue}.png"
        )
        plt.savefig(cm_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    return performance_df


def run_baseline_models(data_dict, config):
    """
    Run baseline models for comparison.
    
    Args:
        data_dict: Dictionary containing processed data
        config: Configuration object
        
    Returns:
        DataFrame comparing baseline models with transformer
    """
    # Extract data
    spectral_data = data_dict['spectral_data']
    metabolite_data = data_dict['metabolite_data']
    target_data = data_dict['target_data']
    
    # Combine features
    X = np.concatenate([spectral_data, metabolite_data], axis=1)
    
    # Extract indices
    train_idx = data_dict['train_idx']
    val_idx = data_dict['val_idx']
    test_idx = data_dict['test_idx']
    
    # Combine train and validation for final evaluation
    train_val_idx = np.concatenate([train_idx, val_idx])
    
    # Task names
    task_names = ['Genotype', 'Treatment', 'Day']
    
    # Initialize results
    baseline_results = []
    
    # Models to evaluate
    models = {
        'RandomForest': RandomForestClassifier(
            n_estimators=100, max_depth=10, random_state=config.seed, n_jobs=-1
        ),
        'SVM': SVC(probability=True, random_state=config.seed),
        'LogisticRegression': LogisticRegression(
            max_iter=1000, random_state=config.seed, n_jobs=-1
        ),
    }
    
    # Train and evaluate models for each task
    for task_idx, task in enumerate(task_names):
        y = target_data[:, task_idx]
        
        for model_name, model in models.items():
            # Train on combined train+val
            model.fit(X[train_val_idx], y[train_val_idx])
            
            # Evaluate on test
            y_pred = model.predict(X[test_idx])
            
            # Calculate metrics
            accuracy = accuracy_score(y[test_idx], y_pred)
            
            if len(np.unique(y)) == 2:
                precision = precision_score(y[test_idx], y_pred)
                recall = recall_score(y[test_idx], y_pred)
                f1 = f1_score(y[test_idx], y_pred)
            else:
                precision = precision_score(y[test_idx], y_pred, average='weighted')
                recall = recall_score(y[test_idx], y_pred, average='weighted')
                f1 = f1_score(y[test_idx], y_pred, average='weighted')
            
            # Store results
            baseline_results.append({
                'Model': model_name,
                'Task': task,
                'Accuracy': accuracy,
                'Precision': precision,
                'Recall': recall,
                'F1_Score': f1
            })
    
    # Create DataFrame
    baseline_df = pd.DataFrame(baseline_results)
    
    # Save to CSV
    output_path = os.path.join(
        config.output_dir,
        f"transformer_baseline_comparison_{config.tissue}.csv"
    )
    baseline_df.to_csv(output_path, index=False)
    logging.info(f"Saved baseline model comparison to {output_path}")
    
    return baseline_df


# ===== VISUALIZATION FUNCTIONS =====
def visualize_cross_modal_network(cross_modal_pairs, attention_data, config):
    """
    Visualize cross-modal network of spectral-metabolite connections.
    
    Args:
        cross_modal_pairs: DataFrame of top spectral-metabolite pairs
        attention_data: Dictionary containing attention data
        config: Configuration object
    """
    # Extract top pairs
    top_n = min(50, len(cross_modal_pairs))  # Limit to 50 for readability
    top_pairs = cross_modal_pairs.head(top_n)
    
    # Create graph
    G = nx.Graph()
    
    # Add nodes
    spectral_nodes = set(top_pairs['Spectral_Feature'])
    metabolite_nodes = set(top_pairs['Metabolite_Feature'])
    
    for node in spectral_nodes:
        G.add_node(node, type='spectral')
    
    for node in metabolite_nodes:
        G.add_node(node, type='metabolite')
    
    # Add edges with weights
    for _, row in top_pairs.iterrows():
        G.add_edge(
            row['Spectral_Feature'],
            row['Metabolite_Feature'],
            weight=row['Attention_Score']
        )
    
    # Create positions (spectral on left, metabolite on right)
    pos = {}
    
    # Position spectral nodes
    for i, node in enumerate(spectral_nodes):
        pos[node] = (-2, (i - len(spectral_nodes)/2) * 1.0)
    
    # Position metabolite nodes
    for i, node in enumerate(metabolite_nodes):
        pos[node] = (2, (i - len(metabolite_nodes)/2) * 1.0)
    
    # Create figure
    plt.figure(figsize=(12, 12))
    
    # Draw nodes
    nx.draw_networkx_nodes(
        G, pos,
        nodelist=[n for n in G.nodes if G.nodes[n]['type'] == 'spectral'],
        node_color='skyblue',
        node_size=300,
        label='Spectral'
    )
    
    nx.draw_networkx_nodes(
        G, pos,
        nodelist=[n for n in G.nodes if G.nodes[n]['type'] == 'metabolite'],
        node_color='lightgreen',
        node_size=300,
        label='Metabolite'
    )
    
    # Draw edges with width based on attention score
    edge_weights = [G[u][v]['weight'] * 5 for u, v in G.edges]
    nx.draw_networkx_edges(
        G, pos,
        width=edge_weights,
        alpha=0.7,
        edge_color='gray'
    )
    
    # Draw node labels
    nx.draw_networkx_labels(
        G, pos,
        font_size=8,
        font_family='sans-serif'
    )
    
    plt.title(f"Top {top_n} Cross-Modal Connections ({config.tissue.capitalize()} Tissue)")
    plt.legend()
    plt.axis('off')
    
    # Save figure
    output_path = os.path.join(
        config.figure_dir,
        f"network_cross_modal_{config.tissue}.png"
    )
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    logging.info(f"Saved cross-modal network visualization to {output_path}")


def visualize_attention_heatmap(attention_data, cross_modal_pairs, config):
    """
    Visualize attention heatmap of strongest spectral-metabolite connections.
    
    Args:
        attention_data: Dictionary containing attention data
        cross_modal_pairs: DataFrame of top spectral-metabolite pairs
        config: Configuration object
    """
    # Extract global attention matrix
    s2m_global = attention_data['s2m_global']
    
    # Extract top features
    top_n = min(25, len(cross_modal_pairs))  # Limit for readability
    top_pairs = cross_modal_pairs.head(top_n)
    
    # Get unique features
    unique_spectral = top_pairs['Spectral_Feature'].unique()
    unique_metabolite = top_pairs['Metabolite_Feature'].unique()
    
    # Get indices
    spectral_indices = [attention_data['spectral_features'].index(feat) for feat in unique_spectral]
    metabolite_indices = [attention_data['metabolite_features'].index(feat) for feat in unique_metabolite]
    
    # Extract submatrix
    submatrix = s2m_global[np.ix_(spectral_indices, metabolite_indices)]
    
    # Create heatmap
    plt.figure(figsize=(12, 10))
    sns.heatmap(
        submatrix,
        xticklabels=unique_metabolite,
        yticklabels=unique_spectral,
        cmap='Blues',
        annot=True,
        fmt='.2f',
        linewidths=0.5
    )
    
    plt.title(f"Attention Heatmap ({config.tissue.capitalize()} Tissue)")
    plt.xlabel('Metabolite Features')
    plt.ylabel('Spectral Features')
    
    # Rotate x-axis labels
    plt.xticks(rotation=45, ha='right')
    
    # Save figure
    output_path = os.path.join(
        config.figure_dir,
        f"heatmap_attention_{config.tissue}.png"
    )
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    logging.info(f"Saved attention heatmap to {output_path}")


def visualize_temporal_evolution(temporal_df, config):
    """
    Visualize temporal evolution of attention patterns across days.
    
    Args:
        temporal_df: DataFrame containing temporal attention patterns
        config: Configuration object
    """
    if temporal_df.empty:
        logging.warning("Cannot create temporal visualization (empty data)")
        return
    
    # Get unique days
    days = sorted(temporal_df['Day'].unique())
    
    # Create subplots for each day
    fig, axes = plt.subplots(1, len(days), figsize=(16, 6), sharey=True)
    
    # If only one day, wrap axes in list
    if len(days) == 1:
        axes = [axes]
    
    for i, day in enumerate(days):
        # Get data for this day
        day_data = temporal_df[temporal_df['Day'] == day]
        
        # Get top pairs for visualization
        top_n = min(20, len(day_data))
        top_pairs = day_data.head(top_n)
        
        # Create bar plot
        sns.barplot(
            x='Attention_Score',
            y='Metabolite_Feature',
            hue='Spectral_Feature',
            data=top_pairs,
            ax=axes[i],
            palette='viridis'
        )
        
        axes[i].set_title(f"Day {day}")
        
        # Only show legend for the last subplot
        if i < len(days) - 1:
            axes[i].legend([])
        else:
            axes[i].legend(title='Spectral Feature', loc='center left', bbox_to_anchor=(1, 0.5))
    
    plt.tight_layout()
    plt.suptitle(f"Temporal Evolution of Attention Patterns ({config.tissue.capitalize()} Tissue)", y=1.05)
    
    # Save figure
    output_path = os.path.join(
        config.figure_dir,
        f"temporal_attention_evolution_{config.tissue}.png"
    )
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    logging.info(f"Saved temporal evolution visualization to {output_path}")


def visualize_genotype_differences(genotype_diff, attention_data, config):
    """
    Visualize genotype-specific attention patterns.
    
    Args:
        genotype_diff: Dictionary containing genotype attention data
        attention_data: Dictionary containing attention data
        config: Configuration object
    """
    genotype_df = genotype_diff['genotype_df']
    
    if genotype_df is None or genotype_df.empty:
        logging.warning("Cannot create genotype visualization (empty data)")
        return
    
    # Extract top differential pairs
    top_n = min(20, len(genotype_df))
    top_diff = genotype_df.head(top_n)
    
    # Create plot
    plt.figure(figsize=(12, 8))
    
    # Create a paired bar plot
    x = np.arange(len(top_diff))
    width = 0.35
    
    plt.bar(x - width/2, top_diff['G1_Attention'], width, label='G1 (Tolerant)')
    plt.bar(x + width/2, top_diff['G2_Attention'], width, label='G2 (Susceptible)')
    
    # Add labels and title
    plt.xlabel('Spectral-Metabolite Pair')
    plt.ylabel('Attention Score')
    plt.title(f"Genotype-Specific Attention Patterns ({config.tissue.capitalize()} Tissue)")
    
    # Create pair labels
    pair_labels = [f"{s[:10]}...{m[:10]}..." for s, m in 
                   zip(top_diff['Spectral_Feature'], top_diff['Metabolite_Feature'])]
    
    plt.xticks(x, pair_labels, rotation=45, ha='right')
    plt.legend()
    
    plt.tight_layout()
    
    # Save figure
    output_path = os.path.join(
        config.figure_dir,
        f"genotype_differential_network_{config.tissue}.png"
    )
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    logging.info(f"Saved genotype difference visualization to {output_path}")


def visualize_spectral_regions(feature_importance, attention_data, config):
    """
    Visualize important spectral wavelength regions.
    
    Args:
        feature_importance: DataFrame containing feature importance scores
        attention_data: Dictionary containing attention data
        config: Configuration object
    """
    # Extract spectral feature importance
    spectral_df = feature_importance[feature_importance['Feature_Type'] == 'Spectral'].copy()
    
    if spectral_df.empty:
        logging.warning("Cannot create spectral regions visualization (empty data)")
        return
    
    # Extract wavelength numbers and sort
    spectral_df['Wavelength'] = spectral_df['Feature'].apply(
        lambda x: int(x.split('_')[0][1:]) if x.startswith('W_') and x.split('_')[0][1:].isdigit() else 0
    )
    spectral_df = spectral_df.sort_values('Wavelength')
    
    # Filter for non-zero wavelengths
    spectral_df = spectral_df[spectral_df['Wavelength'] > 0]
    
    # Create plot
    plt.figure(figsize=(12, 6))
    
    # Plot wavelength importance
    plt.scatter(
        spectral_df['Wavelength'],
        spectral_df['Importance_Scaled'],
        c=spectral_df['Importance_Scaled'],
        cmap='viridis',
        s=50,
        alpha=0.7
    )
    
    # Add smoothed trend line
    if len(spectral_df) > 5:  # Only if we have enough points
        from scipy.signal import savgol_filter
        x = spectral_df['Wavelength'].values
        y = spectral_df['Importance_Scaled'].values
        
        # Sort by x for smoothing
        sorted_indices = np.argsort(x)
        x_sorted = x[sorted_indices]
        y_sorted = y[sorted_indices]
        
        # Apply Savitzky-Golay filter with appropriate window length
        window_length = min(15, len(x) - (len(x) % 2) - 1)  # Must be odd and less than len(x)
        if window_length > 3:  # Minimum required window length
            y_smooth = savgol_filter(y_sorted, window_length, 3)
            plt.plot(x_sorted, y_smooth, 'r-', linewidth=2)
    
    # Add labels and title
    plt.xlabel('Wavelength (nm)')
    plt.ylabel('Importance Score (Scaled)')
    plt.title(f"Key Spectral Regions ({config.tissue.capitalize()} Tissue)")
    
    # Add colorbar
    plt.colorbar(label='Importance Score')
    
    plt.tight_layout()
    
    # Save figure
    output_path = os.path.join(
        config.figure_dir,
        f"key_spectral_regions_{config.tissue}.png"
    )
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    logging.info(f"Saved spectral regions visualization to {output_path}")


# ===== MAIN EXECUTION =====
def setup_logging(config):
    """Set up logging configuration"""
    log_file = os.path.join(
        config.logs_dir,
        f"transformer_{config.tissue}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    )
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )


def main():
    """Main execution function"""
    # Parse arguments
    parser = argparse.ArgumentParser(description='Multi-Omic Transformer Implementation')
    parser.add_argument('--tissue', type=str, choices=['leaf', 'root'], default='leaf',
                        help='Tissue type to analyze (leaf or root)')
    args = parser.parse_args()
    
    # Initialize configuration
    config = TransformerConfig(tissue=args.tissue)
    
    # Setup logging
    setup_logging(config)
    
    # Record start time
    start_time = time.time()
    logging.info(f"Starting Multi-Omic Transformer Analysis for {config.tissue.capitalize()} Tissue")
    
    # Set random seeds for reproducibility
    np.random.seed(config.seed)
    torch.manual_seed(config.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(config.seed)
    
    # Determine device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logging.info(f"Using device: {device}")
    
    try:
        # Load and preprocess data
        data_dict = load_and_preprocess_data(config)
        
        # Create dataloaders
        dataloaders = create_dataloaders(data_dict, config)
        
        # Initialize model
        spectral_dim = data_dict['spectral_data'].shape[1]
        metabolite_dim = data_dict['metabolite_data'].shape[1]
        
        model = MultiOmicTransformer(
            spectral_dim=spectral_dim,
            metabolite_dim=metabolite_dim,
            hidden_dim=config.hidden_dim,
            num_heads=config.num_heads,
            num_layers=config.num_layers,
            dropout=config.dropout,
            num_tasks=3,
            num_classes=[2, 2, 3]  # Genotype, Treatment, Day
        ).to(device)
        
        logging.info(f"Initialized model with {spectral_dim} spectral features and {metabolite_dim} metabolite features")
        
        # Train model
        trained_model, history = train_model(model, dataloaders, config, device)
        
        # Evaluate model performance
        performance_df = evaluate_model_performance(trained_model, dataloaders, config, device)
        
        # Run baseline models
        baseline_df = run_baseline_models(data_dict, config)
        
        # Extract and analyze attention weights
        attention_data = extract_attention_weights(trained_model, dataloaders['test'], data_dict, config, device)
        
        # Analyze cross-modal pairs
        cross_modal_pairs = analyze_cross_modal_pairs(attention_data, config)
        
        # Analyze temporal attention
        temporal_df = analyze_temporal_attention(attention_data, config)
        
        # Calculate feature importance
        feature_importance = analyze_feature_importance(attention_data, config)
        
        # Analyze genotype differences
        genotype_diff = analyze_genotype_differences(attention_data, config)
        
        # Create visualizations
        visualize_cross_modal_network(cross_modal_pairs, attention_data, config)
        visualize_attention_heatmap(attention_data, cross_modal_pairs, config)
        visualize_temporal_evolution(temporal_df, config)
        visualize_genotype_differences(genotype_diff, attention_data, config)
        visualize_spectral_regions(feature_importance, attention_data, config)
        
        # Log completion
        end_time = time.time()
        runtime = end_time - start_time
        logging.info(f"Analysis completed in {runtime:.2f} seconds ({runtime/60:.2f} minutes)")
        
    except Exception as e:
        logging.error(f"Error in main execution: {str(e)}", exc_info=True)
        raise
    
    return 0


if __name__ == "__main__":
    main()

--------------------------------------------------

